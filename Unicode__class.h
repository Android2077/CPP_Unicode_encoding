#pragma once

#include <iostream>
#include <string>
#include <vector>
#include <unordered_set>



class Unicode__class
{

public:

	enum class result_flag: int
	{
		OK = 1,
		
		UTF8_incorrect   = 2,
		UTF16_incorrect  = 3,
		Unicode_incorrect = 4,
		size_not_enough_for_analysis = 5,
		NumPos_NotExist = 6,
		IncludeSurrogatePairValue = 7,
		Greater_than_MaxUnicodeValue = 8,
	};

	struct UTF8_SymvInfo_struct
	{
		int_least32_t UnicodePointCode;             //Номер кодовой точки Юникода.
		size_t NumPos;                              //Номер байта в переданной строке с которого начинается данный UTF-8 символ.
		const char* pointer_to_UTF8_Symv;           //Указатель на байт в строке  с которого начинается данный UTF-8 символ.
		int ByteSize;                               //Размер в байтах данного UTF8 символа.

		UTF8_SymvInfo_struct() = default;

		UTF8_SymvInfo_struct(int_least32_t UnicodePointCode_, size_t NumPos_, const char* pointer_to_UTF8_Symv_, int ByteSize_): UnicodePointCode(UnicodePointCode_), NumPos(NumPos_), pointer_to_UTF8_Symv(pointer_to_UTF8_Symv_), ByteSize(ByteSize_) {}
	};

	struct UTF16_SymvInfo_struct
	{
		int_least32_t UnicodePointCode;          //Номер кодовой точки Юникода.
		size_t NumPos;                           //Номер байта в переданной строке с которого начинается данный UTF-16 символ.
		const wchar_t* pointer_to_UTF16_Symv;          //Указатель на байт в строке  с которого начинается данный UTF-16 символ.
		int PairSize;                            //Размер в парах данного UTF16 символа.

		UTF16_SymvInfo_struct() = default;

		UTF16_SymvInfo_struct(int_least32_t UnicodePointCode_, size_t NumPos_, const wchar_t* pointer_to_UTF16_Symv_, int PairSize_) : UnicodePointCode(UnicodePointCode_), NumPos(NumPos_), pointer_to_UTF16_Symv(pointer_to_UTF16_Symv_), PairSize(PairSize_) {}
	};


public:

 
	//****************************************************************о***********************************************************************************
	inline const result_flag check__UnicodePointCode(const int_least32_t UnicodePointCode) noexcept
	{

		if (UnicodePointCode >= 55296 && UnicodePointCode <= 57343)
		{
			error = "Unicode_incorrect";
			return result_flag::Unicode_incorrect;
		}
		else
		{
			if (UnicodePointCode > 1114111)
			{
				error = "Unicode_incorrect";
				return result_flag::Unicode_incorrect;
			}
			else
			{
				return result_flag::OK;
			}
		}
	}

	inline const result_flag check__UnicodePointCode(const std::vector<int_least32_t>& vec__UnicodePointCode) noexcept
	{

		for (size_t i = 0; i < vec__UnicodePointCode.size(); i++)
		{
			if (check__UnicodePointCode(vec__UnicodePointCode[i]) == result_flag::Unicode_incorrect)
			{
				error = "Unicode_incorrect";
				return result_flag::Unicode_incorrect;
			}
		}

		return result_flag::OK;
	}

	inline const result_flag check__UnicodePointCode(const std::vector<int_least32_t>& vec__UnicodePointCode, size_t& incorrect_num) noexcept
	{

		for (size_t i = 0; i < vec__UnicodePointCode.size(); i++)
		{
			if (check__UnicodePointCode(vec__UnicodePointCode[i]) == result_flag::Unicode_incorrect)
			{
				incorrect_num = i;
				error = "Unicode_incorrect";
				return result_flag::Unicode_incorrect;
			}
		}

		return result_flag::OK;
	}
	//****************************************************************о***********************************************************************************



	//****************************************************************UTF-8:Начало***********************************************************************************
	inline const result_flag check__UTF8symv(const char* pointer_to_begin_UTF8_symv, const size_t pointer_SizeByte) noexcept
	{


		//-------------------------------------------------------------------------------------------------------
		//Описание, как кодируется символ UTF8: в трех примерах:
		//Кодировка UTF8 - это кодировка переменной длинны от 1 до 4 байт:
		//Условимся, что мы рассматриваем байты и биты слева направо, то есть первый бит или байт всегда будет слева, осталные правее.

		//Пример1: Закодируем символ ASCII к примеру "Q" кодировкой UTF8:
		//Первый БИТ начального байта будет содержать 0 - он будет указывать на то, что это первый БАЙТ - это единсвенный БАЙТ, в котором содержится кодировка символа "Q".
		//Остальные БИТы БАЙТа - будут содержать само закодированное значение и это значение 81 из таблицы ASII закодированое в бинарное - 1010001 и с учетом первого нуля - это будет: 01010001 - то есть значение кодировки UTF8 для символов ASCII будет ПОЛНОСТЬЮ совпадать.



		//Пример2: Закодируем символ к примеру "Ǎ" "Латинская заглавная буква «A» с гачеком Ǎ" кодировкой UTF8:
		//В таблице Юникода буква "Ǎ" - имеет десятичное значение - 461.
		//Переведем десятичное значения 461 - в двоичное число: 111001101 - то есть для кодирования значения 461 или буквы "Ǎ" нужно 9 БИТ. 9 рабочих БИТ, не учитывая служебные БИТы, которые говрят о кол-ве БАЙТ, которым закодирован символ.
		//Теперь закодируем значение 111001101 по правилам кодировки UTF8:
		//То есть в первом БАЙТЕ в первых двух БИТАХ указывается - 11, что будет говорит о том, что кодирование значения "111001101" будет занимать два БАЙТА.
		//В первом байте после указания "11" ДОЛЖЕН ВСЕГДА ОБЯЗАТЕЛЬНО ИДТИ БИТ с числом "0": 110 ... ТАК ПРЕДВАРИТЕЛЬНО Первый БАЙТ Готов. ТЕПЕРЬ ВАЖНО: теперь заполнение эти Двух БАЙТ нужно ввести с конца, то есть в данном случае со второго БАЙТА:
		//Теперь закодируем Второй БАЙТ : ЛЮБЫЕ ПОСЛЕДУЮЩИЕ БАЙТЫ закодированного символа после Первого байта ДОЛЖНЫ начинатся с служебных двух БИТОВ : 10 ..и после них уже должны идти следующие рабочие БИТЫ.
		//То есть во Втором БАЙТЕ у нас с учетом первых двух служебных БИТОВ 10 - остается 6 БИТ: заполним их 6-Ю БИТАМИ из нашего символа "Ǎ" справа: "111_001101" - то есть мы берем правую часть БИТОВ, который помещаются во второй БАЙТ и помещаем их во Второй БАЙТ: 10_001101
		//Теперь переходим к Первому БАЙТУ и оставшимся БИТАМ из "Ǎ"  "111_001101" - то есть остались "111" - их нужно добавить в Первый БАЙТ...
		//Напоминаю, что Первый БАЙТ у нас заполнен на данный момент следующими служебными БИТАМИ: 110... - то есть у нас в Первом БАЙТЕ свободно еще 5 БИТ, но ДОБАВИТЬ нужно всего 3 БИТА "111"...
		//ДОБАВЛЯТЬ эти три БИТА "111" в Первый БАЙТ нужно также справа и те БИТЫ в первом БАЙТЕ, который не "заполнились" - заполняем нулем: итого: "110_00_111"


		//ВСЕ остальные символы кодируются по этим двум правилам: итого:
		//Кодировка UTF8 может занимать от 1 до 4 байт.
		//Символ закодированный 1 байтом - всегда начинается с 0 бита слева. И все остальные 7 бит - это самое значение символа:
		//0.....

		//Символ закодированный от 2 до 4 байт ВСЕГДА начинается в Первом байте с:
		//110...     - закодирован двумя байтами
		//1110...    - закодирован тремя байтами
		//11110...   - закодирован четрьмя байтами

		//Вторые и поcледующие байты символ закодированных более, чем одним байтом ВСЕГДА начинаются с двух первых битов "10" и остальные 6 бит - заполянются уже рабочми битами самого символа.
		//Заполнение рабочих бит символа - идет с последнего байта к первому. И там где в Первом байте остались "незаполенные биты" - они заполняются нулями.
		//-------------------------------------------------------------------------------------------------------


		//-------------------------------------------------------------------------------------------------
		//pointer_to_begin_UTF8_symv - должен быть указатель на первый байт UTF-8 символа.
		//pointer_SizeByte      - размер байт, в пределах которого нужно проверить Один символ на который указывает "pointer_to_begin_UTF8_symv", может быть по сути любым, ТО ЕСТЬ, функция проверит является ли Указатель "pointer_to_begin_UTF8_symv" указатель на начальый Байт UTF-8 символа, и если является, то првоерит корректность остальных байт этого UTF-8 символа, если размер указанынй в "pointer_SizeByte" будет больше, чем общий размер в байтах UTF-8 символа, то он просто не будет ни как испльзоватся, а, если размер указанынй в "pointer_SizeByte" будет меньше, чем размер в байтах UTF-8 символа на который указывает указатель "pointer_to_begin_UTF8_symv" - то такой символ будет признан Некорректным, так как функции просто тупо не хватило данных для окончания анализа.
		//-------------------------------------------------------------------------------------------------



		//А теперь применем побитовое "И" для того, чтобы определить есть ли в проверяемом байте сооветствие 4-ем видам битов UTF-8 говорящее о кол-ве задействоаных байт для кодировки символа:

		if (0b11110000 == (pointer_to_begin_UTF8_symv[0] & 0b11110000))
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 4 байтами. Теперь нужно проверить остальын три байта на корректность, то есть наличие первых двух Бит "10" в начале каждого последующего байта символа.

			//-------------------------------------------
			if (pointer_SizeByte >= 4)
			{
				if ((0b10000000 == (pointer_to_begin_UTF8_symv[1] & 0b11000000)) && (0b10000000 == (pointer_to_begin_UTF8_symv[2] & 0b11000000)) && (0b10000000 == (pointer_to_begin_UTF8_symv[3] & 0b11000000)))
				{
					return result_flag::OK;  					//Значит служебные биты остальных байтов соответсвуют служебным битам данного UTF-8 символа, значит это коректный UTF-8 символ.
				}
				else
				{
					error = "UTF8_incorrect";
					return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else if (0b11100000 == (pointer_to_begin_UTF8_symv[0] & 0b11100000))
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 3 байтами. Теперь нужно проверить остальын три байта на корректность, то есть наличие первых двух Бит "10" в начале каждого последующего байта символа.


			//-------------------------------------------
			if (pointer_SizeByte >= 3)
			{
				if ((0b10000000 == (pointer_to_begin_UTF8_symv[1] & 0b11000000)) && (0b10000000 == (pointer_to_begin_UTF8_symv[2] & 0b11000000)))
				{
					return result_flag::OK;  	//Значит служебные биты остальных байтов соответсвуют служебным битам данного UTF-8 символа, значит это коректный UTF-8 символ.
				}
				else
				{
					error = "UTF8_incorrect";
					return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else if (0b11000000 == (pointer_to_begin_UTF8_symv[0] & 0b11000000))
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 2 байтами. Теперь нужно проверить остальын три байта на корректность, то есть наличие первых двух Бит "10" в начале каждого последующего байта символа.


			//-------------------------------------------
			if (pointer_SizeByte >= 2)
			{
				if ((0b10000000 == (pointer_to_begin_UTF8_symv[1] & 0b11000000)))
				{
					return result_flag::OK; 	//Значит служебные биты остальных байтов соответсвуют служебным битам данного UTF-8 символа, значит это коректный UTF-8 символ.
				}
				else
				{
					error = "UTF8_incorrect";
					return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else if (0b00000000 == (pointer_to_begin_UTF8_symv[0] & 0b10000000))     //То есть если проверяемый байт имеет первый Бит 1_1010101 - то после применения "И" - будет 1_0010101 - что не соответвует одному из 4-ех видов начальных бит. Если же первый бит 0_0010101 - то после "И" соответсвенно будет 0_0010101 - что показывает, что первый Бит нулевой, что соответвует одному их 4 видов.
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 1 байтами. Проверка окончена.

			//-------------------------------------------
			if (pointer_SizeByte >= 1)
			{
				return result_flag::OK;
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else
		{
			error = "UTF8_incorrect";
			return result_flag::UTF8_incorrect;  //То есть это точно НЕ ПЕРВЫЙ байт любого символа UTF-8.
		}
	}

	inline const result_flag check__UTF8symv(const char* pointer_to_begin_UTF8_symv, const size_t pointer_SizeByte, const char** pointer_to_IncorrectByte) noexcept
	{

		//-------------------------------------------------------------------------------------------------------
		//Описание, как кодируется символ UTF8: в трех примерах:
		//Кодировка UTF8 - это кодировка переменной длинны от 1 до 4 байт:
		//Условимся, что мы рассматриваем байты и биты слева направо, то есть первый бит или байт всегда будет слева, осталные правее.

		//Пример1: Закодируем символ ASCII к примеру "Q" кодировкой UTF8:
		//Первый БИТ начального байта будет содержать 0 - он будет указывать на то, что это первый БАЙТ - это единсвенный БАЙТ, в котором содержится кодировка символа "Q".
		//Остальные БИТы БАЙТа - будут содержать само закодированное значение и это значение 81 из таблицы ASII закодированое в бинарное - 1010001 и с учетом первого нуля - это будет: 01010001 - то есть значение кодировки UTF8 для символов ASCII будет ПОЛНОСТЬЮ совпадать.



		//Пример2: Закодируем символ к примеру "Ǎ" "Латинская заглавная буква «A» с гачеком Ǎ" кодировкой UTF8:
		//В таблице Юникода буква "Ǎ" - имеет десятичное значение - 461.
		//Переведем десятичное значения 461 - в двоичное число: 111001101 - то есть для кодирования значения 461 или буквы "Ǎ" нужно 9 БИТ. 9 рабочих БИТ, не учитывая служебные БИТы, которые говрят о кол-ве БАЙТ, которым закодирован символ.
		//Теперь закодируем значение 111001101 по правилам кодировки UTF8:
		//То есть в первом БАЙТЕ в первых двух БИТАХ указывается - 11, что будет говорит о том, что кодирование значения "111001101" будет занимать два БАЙТА.
		//В первом байте после указания "11" ДОЛЖЕН ВСЕГДА ОБЯЗАТЕЛЬНО ИДТИ БИТ с числом "0": 110 ... ТАК ПРЕДВАРИТЕЛЬНО Первый БАЙТ Готов. ТЕПЕРЬ ВАЖНО: теперь заполнение эти Двух БАЙТ нужно ввести с конца, то есть в данном случае со второго БАЙТА:
		//Теперь закодируем Второй БАЙТ : ЛЮБЫЕ ПОСЛЕДУЮЩИЕ БАЙТЫ закодированного символа после Первого байта ДОЛЖНЫ начинатся с служебных двух БИТОВ : 10 ..и после них уже должны идти следующие рабочие БИТЫ.
		//То есть во Втором БАЙТЕ у нас с учетом первых двух служебных БИТОВ 10 - остается 6 БИТ: заполним их 6-Ю БИТАМИ из нашего символа "Ǎ" справа: "111_001101" - то есть мы берем правую часть БИТОВ, который помещаются во второй БАЙТ и помещаем их во Второй БАЙТ: 10_001101
		//Теперь переходим к Первому БАЙТУ и оставшимся БИТАМ из "Ǎ"  "111_001101" - то есть остались "111" - их нужно добавить в Первый БАЙТ...
		//Напоминаю, что Первый БАЙТ у нас заполнен на данный момент следующими служебными БИТАМИ: 110... - то есть у нас в Первом БАЙТЕ свободно еще 5 БИТ, но ДОБАВИТЬ нужно всего 3 БИТА "111"...
		//ДОБАВЛЯТЬ эти три БИТА "111" в Первый БАЙТ нужно также справа и те БИТЫ в первом БАЙТЕ, который не "заполнились" - заполняем нулем: итого: "110_00_111"


		//ВСЕ остальные символы кодируются по этим двум правилам: итого:
		//Кодировка UTF8 может занимать от 1 до 4 байт.
		//Символ закодированный 1 байтом - всегда начинается с 0 бита слева. И все остальные 7 бит - это самое значение символа:
		//0.....

		//Символ закодированный от 2 до 4 байт ВСЕГДА начинается в Первом байте с:
		//110...     - закодирован двумя байтами
		//1110...    - закодирован тремя байтами
		//11110...   - закодирован четрьмя байтами

		//Вторые и поcледующие байты символ закодированных более, чем одним байтом ВСЕГДА начинаются с двух первых битов "10" и остальные 6 бит - заполянются уже рабочми битами самого символа.
		//Заполнение рабочих бит символа - идет с последнего байта к первому. И там где в Первом байте остались "незаполенные биты" - они заполняются нулями.
		//-------------------------------------------------------------------------------------------------------


		//-------------------------------------------------------------------------------------------------
		//pointer_to_begin_UTF8_symv - должен быть указатель на первый байт UTF-8 символа.
		//pointer_SizeByte      - размер байт, в пределах которого нужно проверить Один символ на который указывает "pointer_to_begin_UTF8_symv", может быть по сути любым, ТО ЕСТЬ, функция проверит является ли Указатель "pointer_to_begin_UTF8_symv" указатель на начальый Байт UTF-8 символа, и если является, то првоерит корректность остальных байт этого UTF-8 символа, если размер указанынй в "pointer_SizeByte" будет больше, чем общий размер в байтах UTF-8 символа, то он просто не будет ни как испльзоватся, а, если размер указанынй в "pointer_SizeByte" будет меньше, чем размер в байтах UTF-8 символа на который указывает указатель "pointer_to_begin_UTF8_symv" - то такой символ будет признан Некорректным, так как функции просто тупо не хватило данных для окончания анализа.
		//-------------------------------------------------------------------------------------------------



		//А теперь применем побитовое "И" для того, чтобы определить есть ли в проверяемом байте сооветствие 4-ем видам битов UTF-8 говорящее о кол-ве задействоаных байт для кодировки символа:

		if (0b11110000 == (pointer_to_begin_UTF8_symv[0] & 0b11110000))
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 4 байтами. Теперь нужно проверить остальын три байта на корректность, то есть наличие первых двух Бит "10" в начале каждого последующего байта символа.

			//-------------------------------------------
			if (pointer_SizeByte >= 4)
			{
				if (0b10000000 == (pointer_to_begin_UTF8_symv[1] & 0b11000000))
				{
					if (0b10000000 == (pointer_to_begin_UTF8_symv[2] & 0b11000000))
					{
						if (0b10000000 == (pointer_to_begin_UTF8_symv[3] & 0b11000000))
						{
							return result_flag::OK;  					//Значит служебные биты остальных байтов соответсвуют служебным битам данного UTF-8 символа, значит это коректный UTF-8 символ.
						}
						else
						{
							*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[3];
							error = "UTF8_incorrect";
							return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
						}
					}
					else
					{
						*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[2];
						error = "UTF8_incorrect";
						return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
					}
				}
				else
				{
					*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[1];
					error = "UTF8_incorrect";
					return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else if (0b11100000 == (pointer_to_begin_UTF8_symv[0] & 0b11100000))
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 3 байтами. Теперь нужно проверить остальын три байта на корректность, то есть наличие первых двух Бит "10" в начале каждого последующего байта символа.


			//-------------------------------------------
			if (pointer_SizeByte >= 3)
			{
				if (0b10000000 == (pointer_to_begin_UTF8_symv[1] & 0b11000000))
				{
					if (0b10000000 == (pointer_to_begin_UTF8_symv[2] & 0b11000000))
					{
						return result_flag::OK;  					//Значит служебные биты остальных байтов соответсвуют служебным битам данного UTF-8 символа, значит это коректный UTF-8 символ.
					}
					else
					{
						*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[2];
						error = "UTF8_incorrect";
						return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
					}
				}
				else
				{
					*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[1];
					error = "UTF8_incorrect";
					return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else if (0b11000000 == (pointer_to_begin_UTF8_symv[0] & 0b11000000))
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 2 байтами. Теперь нужно проверить остальын три байта на корректность, то есть наличие первых двух Бит "10" в начале каждого последующего байта символа.


			//-------------------------------------------
			if (pointer_SizeByte >= 2)
			{
				if (0b10000000 == (pointer_to_begin_UTF8_symv[1] & 0b11000000))
				{
					return result_flag::OK;  					//Значит служебные биты остальных байтов соответсвуют служебным битам данного UTF-8 символа, значит это коректный UTF-8 символ.
				}
				else
				{
					*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[1];
					error = "UTF8_incorrect";
					return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
				}
			}
			else
			{
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else if (0b00000000 == (pointer_to_begin_UTF8_symv[0] & 0b10000000))     //То есть если проверяемый байт имеет первый Бит 1_1010101 - то после применения "И" - будет 1_0010101 - что не соответвует одному из 4-ех видов начальных бит. Если же первый бит 0_0010101 - то после "И" соответсвенно будет 0_0010101 - что показывает, что первый Бит нулевой, что соответвует одному их 4 видов.
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 1 байтами. Проверка окончена.

			//-------------------------------------------
			if (pointer_SizeByte >= 1)
			{
				return result_flag::OK;
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------


		}
		else
		{
			*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[0];
			error = "UTF8_incorrect";
			return result_flag::UTF8_incorrect;   //То есть это точно НЕ ПЕРВЫЙ байт любого символа UTF-8.
		}
	}

	inline const result_flag check__UTF8symv_SurrogatePair(const char* pointer_to_begin_UTF8_symv, const size_t pointer_SizeByte) noexcept
	{

		//-------------------------------------------------------------------------------------------------------
		//Описание, как кодируется символ UTF8: в трех примерах:
		//Кодировка UTF8 - это кодировка переменной длинны от 1 до 4 байт:
		//Условимся, что мы рассматриваем байты и биты слева направо, то есть первый бит или байт всегда будет слева, осталные правее.

		//Пример1: Закодируем символ ASCII к примеру "Q" кодировкой UTF8:
		//Первый БИТ начального байта будет содержать 0 - он будет указывать на то, что это первый БАЙТ - это единсвенный БАЙТ, в котором содержится кодировка символа "Q".
		//Остальные БИТы БАЙТа - будут содержать само закодированное значение и это значение 81 из таблицы ASII закодированое в бинарное - 1010001 и с учетом первого нуля - это будет: 01010001 - то есть значение кодировки UTF8 для символов ASCII будет ПОЛНОСТЬЮ совпадать.



		//Пример2: Закодируем символ к примеру "Ǎ" "Латинская заглавная буква «A» с гачеком Ǎ" кодировкой UTF8:
		//В таблице Юникода буква "Ǎ" - имеет десятичное значение - 461.
		//Переведем десятичное значения 461 - в двоичное число: 111001101 - то есть для кодирования значения 461 или буквы "Ǎ" нужно 9 БИТ. 9 рабочих БИТ, не учитывая служебные БИТы, которые говрят о кол-ве БАЙТ, которым закодирован символ.
		//Теперь закодируем значение 111001101 по правилам кодировки UTF8:
		//То есть в первом БАЙТЕ в первых двух БИТАХ указывается - 11, что будет говорит о том, что кодирование значения "111001101" будет занимать два БАЙТА.
		//В первом байте после указания "11" ДОЛЖЕН ВСЕГДА ОБЯЗАТЕЛЬНО ИДТИ БИТ с числом "0": 110 ... ТАК ПРЕДВАРИТЕЛЬНО Первый БАЙТ Готов. ТЕПЕРЬ ВАЖНО: теперь заполнение эти Двух БАЙТ нужно ввести с конца, то есть в данном случае со второго БАЙТА:
		//Теперь закодируем Второй БАЙТ : ЛЮБЫЕ ПОСЛЕДУЮЩИЕ БАЙТЫ закодированного символа после Первого байта ДОЛЖНЫ начинатся с служебных двух БИТОВ : 10 ..и после них уже должны идти следующие рабочие БИТЫ.
		//То есть во Втором БАЙТЕ у нас с учетом первых двух служебных БИТОВ 10 - остается 6 БИТ: заполним их 6-Ю БИТАМИ из нашего символа "Ǎ" справа: "111_001101" - то есть мы берем правую часть БИТОВ, который помещаются во второй БАЙТ и помещаем их во Второй БАЙТ: 10_001101
		//Теперь переходим к Первому БАЙТУ и оставшимся БИТАМ из "Ǎ"  "111_001101" - то есть остались "111" - их нужно добавить в Первый БАЙТ...
		//Напоминаю, что Первый БАЙТ у нас заполнен на данный момент следующими служебными БИТАМИ: 110... - то есть у нас в Первом БАЙТЕ свободно еще 5 БИТ, но ДОБАВИТЬ нужно всего 3 БИТА "111"...
		//ДОБАВЛЯТЬ эти три БИТА "111" в Первый БАЙТ нужно также справа и те БИТЫ в первом БАЙТЕ, который не "заполнились" - заполняем нулем: итого: "110_00_111"


		//ВСЕ остальные символы кодируются по этим двум правилам: итого:
		//Кодировка UTF8 может занимать от 1 до 4 байт.
		//Символ закодированный 1 байтом - всегда начинается с 0 бита слева. И все остальные 7 бит - это самое значение символа:
		//0.....

		//Символ закодированный от 2 до 4 байт ВСЕГДА начинается в Первом байте с:
		//110...     - закодирован двумя байтами
		//1110...    - закодирован тремя байтами
		//11110...   - закодирован четрьмя байтами

		//Вторые и поcледующие байты символ закодированных более, чем одним байтом ВСЕГДА начинаются с двух первых битов "10" и остальные 6 бит - заполянются уже рабочми битами самого символа.
		//Заполнение рабочих бит символа - идет с последнего байта к первому. И там где в Первом байте остались "незаполенные биты" - они заполняются нулями.
		//-------------------------------------------------------------------------------------------------------


		//-------------------------------------------------------------------------------------------------
		//pointer_to_begin_UTF8_symv - должен быть указатель на первый байт UTF-8 символа.
		//pointer_SizeByte           - размер байт, в пределах которого нужно проверить Один символ на который указывает "pointer_to_begin_UTF8_symv", может быть по сути любым, ТО ЕСТЬ, функция проверит является ли Указатель "pointer_to_begin_UTF8_symv" указатель на начальый Байт UTF-8 символа, и если является, то првоерит корректность остальных байт этого UTF-8 символа, если размер указанынй в "pointer_SizeByte" будет больше, чем общий размер в байтах UTF-8 символа, то он просто не будет ни как испльзоватся, а, если размер указанынй в "pointer_SizeByte" будет меньше, чем размер в байтах UTF-8 символа на который указывает указатель "pointer_to_begin_UTF8_symv" - то такой символ будет признан Некорректным, так как функции просто тупо не хватило данных для окончания анализа.
		//-------------------------------------------------------------------------------------------------







		//А теперь применем побитовое "И" для того, чтобы определить есть ли в проверяемом байте сооветствие 4-ем видам битов UTF-8 говорящее о кол-ве задействоаных байт для кодировки символа:

		if (0b11110000 == (pointer_to_begin_UTF8_symv[0] & 0b11110000))
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 4 байтами. Теперь нужно проверить остальын три байта на корректность, то есть наличие первых двух Бит "10" в начале каждого последующего байта символа.

			//-------------------------------------------
			if (pointer_SizeByte >= 4)
			{
				if ((0b10000000 == (pointer_to_begin_UTF8_symv[1] & 0b11000000)) && (0b10000000 == (pointer_to_begin_UTF8_symv[2] & 0b11000000)) && (0b10000000 == (pointer_to_begin_UTF8_symv[3] & 0b11000000)))
				{

					//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
					const int_least32_t UnicodePointCode = convert__UTF8Symv_to_UnicodePointCode(pointer_to_begin_UTF8_symv, 4);

					if (UnicodePointCode > 1114111)     //Если больше максимального значения для Юникодного символа
					{
						error = "Greater_than_MaxUnicodeValue";
						return result_flag::Greater_than_MaxUnicodeValue;
					}
					//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


					return result_flag::OK;  					//Значит служебные биты остальных байтов соответсвуют служебным битам данного UTF-8 символа, значит это коректный UTF-8 символ.
				}
				else
				{
					error = "UTF8_incorrect";
					return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else if (0b11100000 == (pointer_to_begin_UTF8_symv[0] & 0b11100000))
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 3 байтами. Теперь нужно проверить остальын три байта на корректность, то есть наличие первых двух Бит "10" в начале каждого последующего байта символа.


			//-------------------------------------------
			if (pointer_SizeByte >= 3)
			{
				if ((0b10000000 == (pointer_to_begin_UTF8_symv[1] & 0b11000000)) && (0b10000000 == (pointer_to_begin_UTF8_symv[2] & 0b11000000)))
				{

					//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
					const int_least32_t UnicodePointCode = convert__UTF8Symv_to_UnicodePointCode(pointer_to_begin_UTF8_symv, 3);

					if (UnicodePointCode >= 55296 && UnicodePointCode <= 57343)     //(0xD800 – 0xDFFF)
					{
						error = "IncludeSurrogatePairValue";
						return result_flag::IncludeSurrogatePairValue;    //Данный условно корреткный UTF-8 символ входит в Юникодное значение "суррогатной пары" - для которого в Юникодной таблицы - нет никаких символов. А значит такого Юникодного символа закодированного в UTF-8 или в какой либо другой кодировке быть не должно.
					}
					//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


					return result_flag::OK;  	//Значит служебные биты остальных байтов соответсвуют служебным битам данного UTF-8 символа, значит это коректный UTF-8 символ.
				}
				else
				{
					error = "UTF8_incorrect";
					return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else if (0b10000000 == (pointer_to_begin_UTF8_symv[0] & 0b11000000))
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 2 байтами. Теперь нужно проверить остальын три байта на корректность, то есть наличие первых двух Бит "10" в начале каждого последующего байта символа.


			//-------------------------------------------
			if (pointer_SizeByte >= 2)
			{
				if ((0b11000000 == (pointer_to_begin_UTF8_symv[1] & 0b11000000)))
				{
					return result_flag::OK;  	//Значит служебные биты остальных байтов соответсвуют служебным битам данного UTF-8 символа, значит это коректный UTF-8 символ.
				}
				else
				{
					error = "UTF8_incorrect";
					return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else if (0b00000000 == (pointer_to_begin_UTF8_symv[0] & 0b10000000))     //То есть если проверяемый байт имеет первый Бит 1_1010101 - то после применения "И" - будет 1_0010101 - что не соответвует одному из 4-ех видов начальных бит. Если же первый бит 0_0010101 - то после "И" соответсвенно будет 0_0010101 - что показывает, что первый Бит нулевой, что соответвует одному их 4 видов.
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 1 байтами. Проверка окончена.

			//-------------------------------------------
			if (pointer_SizeByte >= 1)
			{
				return result_flag::OK;
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else
		{
			error = "UTF8_incorrect";
			return result_flag::UTF8_incorrect;   //То есть это точно НЕ ПЕРВЫЙ байт любого символа UTF-8.
		}
	}

	inline const result_flag check__UTF8symv_SurrogatePair(const char* pointer_to_begin_UTF8_symv, const size_t pointer_SizeByte, const char** pointer_to_IncorrectByte) noexcept
	{

		//-------------------------------------------------------------------------------------------------------
		//Описание, как кодируется символ UTF8: в трех примерах:
		//Кодировка UTF8 - это кодировка переменной длинны от 1 до 4 байт:
		//Условимся, что мы рассматриваем байты и биты слева направо, то есть первый бит или байт всегда будет слева, осталные правее.

		//Пример1: Закодируем символ ASCII к примеру "Q" кодировкой UTF8:
		//Первый БИТ начального байта будет содержать 0 - он будет указывать на то, что это первый БАЙТ - это единсвенный БАЙТ, в котором содержится кодировка символа "Q".
		//Остальные БИТы БАЙТа - будут содержать само закодированное значение и это значение 81 из таблицы ASII закодированое в бинарное - 1010001 и с учетом первого нуля - это будет: 01010001 - то есть значение кодировки UTF8 для символов ASCII будет ПОЛНОСТЬЮ совпадать.



		//Пример2: Закодируем символ к примеру "Ǎ" "Латинская заглавная буква «A» с гачеком Ǎ" кодировкой UTF8:
		//В таблице Юникода буква "Ǎ" - имеет десятичное значение - 461.
		//Переведем десятичное значения 461 - в двоичное число: 111001101 - то есть для кодирования значения 461 или буквы "Ǎ" нужно 9 БИТ. 9 рабочих БИТ, не учитывая служебные БИТы, которые говрят о кол-ве БАЙТ, которым закодирован символ.
		//Теперь закодируем значение 111001101 по правилам кодировки UTF8:
		//То есть в первом БАЙТЕ в первых двух БИТАХ указывается - 11, что будет говорит о том, что кодирование значения "111001101" будет занимать два БАЙТА.
		//В первом байте после указания "11" ДОЛЖЕН ВСЕГДА ОБЯЗАТЕЛЬНО ИДТИ БИТ с числом "0": 110 ... ТАК ПРЕДВАРИТЕЛЬНО Первый БАЙТ Готов. ТЕПЕРЬ ВАЖНО: теперь заполнение эти Двух БАЙТ нужно ввести с конца, то есть в данном случае со второго БАЙТА:
		//Теперь закодируем Второй БАЙТ : ЛЮБЫЕ ПОСЛЕДУЮЩИЕ БАЙТЫ закодированного символа после Первого байта ДОЛЖНЫ начинатся с служебных двух БИТОВ : 10 ..и после них уже должны идти следующие рабочие БИТЫ.
		//То есть во Втором БАЙТЕ у нас с учетом первых двух служебных БИТОВ 10 - остается 6 БИТ: заполним их 6-Ю БИТАМИ из нашего символа "Ǎ" справа: "111_001101" - то есть мы берем правую часть БИТОВ, который помещаются во второй БАЙТ и помещаем их во Второй БАЙТ: 10_001101
		//Теперь переходим к Первому БАЙТУ и оставшимся БИТАМ из "Ǎ"  "111_001101" - то есть остались "111" - их нужно добавить в Первый БАЙТ...
		//Напоминаю, что Первый БАЙТ у нас заполнен на данный момент следующими служебными БИТАМИ: 110... - то есть у нас в Первом БАЙТЕ свободно еще 5 БИТ, но ДОБАВИТЬ нужно всего 3 БИТА "111"...
		//ДОБАВЛЯТЬ эти три БИТА "111" в Первый БАЙТ нужно также справа и те БИТЫ в первом БАЙТЕ, который не "заполнились" - заполняем нулем: итого: "110_00_111"


		//ВСЕ остальные символы кодируются по этим двум правилам: итого:
		//Кодировка UTF8 может занимать от 1 до 4 байт.
		//Символ закодированный 1 байтом - всегда начинается с 0 бита слева. И все остальные 7 бит - это самое значение символа:
		//0.....

		//Символ закодированный от 2 до 4 байт ВСЕГДА начинается в Первом байте с:
		//110...     - закодирован двумя байтами
		//1110...    - закодирован тремя байтами
		//11110...   - закодирован четрьмя байтами

		//Вторые и поcледующие байты символ закодированных более, чем одним байтом ВСЕГДА начинаются с двух первых битов "10" и остальные 6 бит - заполянются уже рабочми битами самого символа.
		//Заполнение рабочих бит символа - идет с последнего байта к первому. И там где в Первом байте остались "незаполенные биты" - они заполняются нулями.
		//-------------------------------------------------------------------------------------------------------


		//-------------------------------------------------------------------------------------------------
		//pointer_to_begin_UTF8_symv - должен быть указатель на первый байт UTF-8 символа.
		//pointer_SizeByte           - размер байт, в пределах которого нужно проверить Один символ на который указывает "pointer_to_begin_UTF8_symv", может быть по сути любым, ТО ЕСТЬ, функция проверит является ли Указатель "pointer_to_begin_UTF8_symv" указатель на начальый Байт UTF-8 символа, и если является, то првоерит корректность остальных байт этого UTF-8 символа, если размер указанынй в "pointer_SizeByte" будет больше, чем общий размер в байтах UTF-8 символа, то он просто не будет ни как испльзоватся, а, если размер указанынй в "pointer_SizeByte" будет меньше, чем размер в байтах UTF-8 символа на который указывает указатель "pointer_to_begin_UTF8_symv" - то такой символ будет признан Некорректным, так как функции просто тупо не хватило данных для окончания анализа.
		//-------------------------------------------------------------------------------------------------



		//А теперь применем побитовое "И" для того, чтобы определить есть ли в проверяемом байте сооветствие 4-ем видам битов UTF-8 говорящее о кол-ве задействоаных байт для кодировки символа:

		if (0b11110000 == (pointer_to_begin_UTF8_symv[0] & 0b11110000))
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 4 байтами. Теперь нужно проверить остальын три байта на корректность, то есть наличие первых двух Бит "10" в начале каждого последующего байта символа.

			//-------------------------------------------
			if (pointer_SizeByte >= 4)
			{
				if (0b10000000 == (pointer_to_begin_UTF8_symv[1] & 0b11000000))
				{
					if (0b10000000 == (pointer_to_begin_UTF8_symv[2] & 0b11000000))
					{
						if (0b10000000 == (pointer_to_begin_UTF8_symv[3] & 0b11000000))
						{

							//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
							const int_least32_t UnicodePointCode = convert__UTF8Symv_to_UnicodePointCode(pointer_to_begin_UTF8_symv, 4);

							if (UnicodePointCode > 1114111)     //Если больше максимального значения для Юникодного символа
							{
								error = "Greater_than_MaxUnicodeValue";
								return result_flag::Greater_than_MaxUnicodeValue;
							}
							//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

							return result_flag::OK;  					//Значит служебные биты остальных байтов соответсвуют служебным битам данного UTF-8 символа, значит это коректный UTF-8 символ.
						}
						else
						{
							*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[3];
							error = "UTF8_incorrect";
							return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
						}
					}
					else
					{
						*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[2];
						error = "UTF8_incorrect";
						return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
					}
				}
				else
				{
					*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[1];
					error = "UTF8_incorrect";
					return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else if (0b11100000 == (pointer_to_begin_UTF8_symv[0] & 0b11100000))
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 3 байтами. Теперь нужно проверить остальын три байта на корректность, то есть наличие первых двух Бит "10" в начале каждого последующего байта символа.


			//-------------------------------------------
			if (pointer_SizeByte >= 3)
			{
				if (0b10000000 == (pointer_to_begin_UTF8_symv[1] & 0b11000000))
				{
					if (0b10000000 == (pointer_to_begin_UTF8_symv[2] & 0b11000000))
					{

						//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
						const int_least32_t UnicodePointCode = convert__UTF8Symv_to_UnicodePointCode(pointer_to_begin_UTF8_symv, 3);

						if (UnicodePointCode >= 55296 && UnicodePointCode <= 57343)     //(0xD800 – 0xDFFF)
						{
							error = "IncludeSurrogatePairValue";
							return result_flag::IncludeSurrogatePairValue;    //Данный условно корреткный UTF-8 символ входит в Юникодное значение "суррогатной пары" - для которого в Юникодной таблицы - нет никаких символов. А значит такого Юникодного символа закодированного в UTF-8 или в какой либо другой кодировке быть не должно.
						}
						//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


						return result_flag::OK;  					//Значит служебные биты остальных байтов соответсвуют служебным битам данного UTF-8 символа, значит это коректный UTF-8 символ.
					}
					else
					{
						*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[2];
						error = "UTF8_incorrect";
						return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
					}
				}
				else
				{
					*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[1];
					error = "UTF8_incorrect";
					return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else if (0b11000000 == (pointer_to_begin_UTF8_symv[0] & 0b11000000))
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 2 байтами. Теперь нужно проверить остальын три байта на корректность, то есть наличие первых двух Бит "10" в начале каждого последующего байта символа.


			//-------------------------------------------
			if (pointer_SizeByte >= 2)
			{
				if (0b10000000 == (pointer_to_begin_UTF8_symv[1] & 0b11000000))
				{
					return result_flag::OK;  					//Значит служебные биты остальных байтов соответсвуют служебным битам данного UTF-8 символа, значит это коректный UTF-8 символ.
				}
				else
				{
					*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[1];
					error = "UTF8_incorrect";
					return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else if (0b00000000 == (pointer_to_begin_UTF8_symv[0] & 0b10000000))     //То есть если проверяемый байт имеет первый Бит 1_1010101 - то после применения "И" - будет 1_0010101 - что не соответвует одному из 4-ех видов начальных бит. Если же первый бит 0_0010101 - то после "И" соответсвенно будет 0_0010101 - что показывает, что первый Бит нулевой, что соответвует одному их 4 видов.
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 1 байтами. Проверка окончена.

			//-------------------------------------------
			if (pointer_SizeByte >= 1)
			{
				return result_flag::OK;
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else
		{
			*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[0];
			error = "UTF8_incorrect";
			return result_flag::UTF8_incorrect;   //То есть это точно НЕ ПЕРВЫЙ байт любого символа UTF-8.
		}
	}


	inline const result_flag check__UTF8symv_and_return_ByteSize_(const char* pointer_to_begin_UTF8_symv, const size_t pointer_SizeByte, int& UTF8_symv_ByteSize) noexcept
	{


		//-------------------------------------------------------------------------------------------------------
		//Описание, как кодируется символ UTF8: в трех примерах:
		//Кодировка UTF8 - это кодировка переменной длинны от 1 до 4 байт:
		//Условимся, что мы рассматриваем байты и биты слева направо, то есть первый бит или байт всегда будет слева, осталные правее.

		//Пример1: Закодируем символ ASCII к примеру "Q" кодировкой UTF8:
		//Первый БИТ начального байта будет содержать 0 - он будет указывать на то, что это первый БАЙТ - это единсвенный БАЙТ, в котором содержится кодировка символа "Q".
		//Остальные БИТы БАЙТа - будут содержать само закодированное значение и это значение 81 из таблицы ASII закодированое в бинарное - 1010001 и с учетом первого нуля - это будет: 01010001 - то есть значение кодировки UTF8 для символов ASCII будет ПОЛНОСТЬЮ совпадать.



		//Пример2: Закодируем символ к примеру "Ǎ" "Латинская заглавная буква «A» с гачеком Ǎ" кодировкой UTF8:
		//В таблице Юникода буква "Ǎ" - имеет десятичное значение - 461.
		//Переведем десятичное значения 461 - в двоичное число: 111001101 - то есть для кодирования значения 461 или буквы "Ǎ" нужно 9 БИТ. 9 рабочих БИТ, не учитывая служебные БИТы, которые говрят о кол-ве БАЙТ, которым закодирован символ.
		//Теперь закодируем значение 111001101 по правилам кодировки UTF8:
		//То есть в первом БАЙТЕ в первых двух БИТАХ указывается - 11, что будет говорит о том, что кодирование значения "111001101" будет занимать два БАЙТА.
		//В первом байте после указания "11" ДОЛЖЕН ВСЕГДА ОБЯЗАТЕЛЬНО ИДТИ БИТ с числом "0": 110 ... ТАК ПРЕДВАРИТЕЛЬНО Первый БАЙТ Готов. ТЕПЕРЬ ВАЖНО: теперь заполнение эти Двух БАЙТ нужно ввести с конца, то есть в данном случае со второго БАЙТА:
		//Теперь закодируем Второй БАЙТ : ЛЮБЫЕ ПОСЛЕДУЮЩИЕ БАЙТЫ закодированного символа после Первого байта ДОЛЖНЫ начинатся с служебных двух БИТОВ : 10 ..и после них уже должны идти следующие рабочие БИТЫ.
		//То есть во Втором БАЙТЕ у нас с учетом первых двух служебных БИТОВ 10 - остается 6 БИТ: заполним их 6-Ю БИТАМИ из нашего символа "Ǎ" справа: "111_001101" - то есть мы берем правую часть БИТОВ, который помещаются во второй БАЙТ и помещаем их во Второй БАЙТ: 10_001101
		//Теперь переходим к Первому БАЙТУ и оставшимся БИТАМ из "Ǎ"  "111_001101" - то есть остались "111" - их нужно добавить в Первый БАЙТ...
		//Напоминаю, что Первый БАЙТ у нас заполнен на данный момент следующими служебными БИТАМИ: 110... - то есть у нас в Первом БАЙТЕ свободно еще 5 БИТ, но ДОБАВИТЬ нужно всего 3 БИТА "111"...
		//ДОБАВЛЯТЬ эти три БИТА "111" в Первый БАЙТ нужно также справа и те БИТЫ в первом БАЙТЕ, который не "заполнились" - заполняем нулем: итого: "110_00_111"


		//ВСЕ остальные символы кодируются по этим двум правилам: итого:
		//Кодировка UTF8 может занимать от 1 до 4 байт.
		//Символ закодированный 1 байтом - всегда начинается с 0 бита слева. И все остальные 7 бит - это самое значение символа:
		//0.....

		//Символ закодированный от 2 до 4 байт ВСЕГДА начинается в Первом байте с:
		//110...     - закодирован двумя байтами
		//1110...    - закодирован тремя байтами
		//11110...   - закодирован четрьмя байтами

		//Вторые и поcледующие байты символ закодированных более, чем одним байтом ВСЕГДА начинаются с двух первых битов "10" и остальные 6 бит - заполянются уже рабочми битами самого символа.
		//Заполнение рабочих бит символа - идет с последнего байта к первому. И там где в Первом байте остались "незаполенные биты" - они заполняются нулями.
		//-------------------------------------------------------------------------------------------------------


		//-------------------------------------------------------------------------------------------------
		//pointer_to_begin_UTF8_symv - должен быть указатель на первый байт UTF-8 символа.
		//pointer_SizeByte      - размер байт, в пределах которого нужно проверить Один символ на который указывает "pointer_to_begin_UTF8_symv", может быть по сути любым, ТО ЕСТЬ, функция проверит является ли Указатель "pointer_to_begin_UTF8_symv" указатель на начальый Байт UTF-8 символа, и если является, то првоерит корректность остальных байт этого UTF-8 символа, если размер указанынй в "pointer_SizeByte" будет больше, чем общий размер в байтах UTF-8 символа, то он просто не будет ни как испльзоватся, а, если размер указанынй в "pointer_SizeByte" будет меньше, чем размер в байтах UTF-8 символа на который указывает указатель "pointer_to_begin_UTF8_symv" - то такой символ будет признан Некорректным, так как функции просто тупо не хватило данных для окончания анализа.
		//-------------------------------------------------------------------------------------------------



		//А теперь применем побитовое "И" для того, чтобы определить есть ли в проверяемом байте сооветствие 4-ем видам битов UTF-8 говорящее о кол-ве задействоаных байт для кодировки символа:

		if (0b11110000 == (pointer_to_begin_UTF8_symv[0] & 0b11110000))
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 4 байтами. Теперь нужно проверить остальын три байта на корректность, то есть наличие первых двух Бит "10" в начале каждого последующего байта символа.

			//-------------------------------------------
			if (pointer_SizeByte >= 4)
			{
				if ((0b10000000 == (pointer_to_begin_UTF8_symv[1] & 0b11000000)) && (0b10000000 == (pointer_to_begin_UTF8_symv[2] & 0b11000000)) && (0b10000000 == (pointer_to_begin_UTF8_symv[3] & 0b11000000)))
				{
					UTF8_symv_ByteSize = 4;
					return result_flag::OK;  					//Значит служебные биты остальных байтов соответсвуют служебным битам данного UTF-8 символа, значит это коректный UTF-8 символ.
				}
				else
				{
					error = "UTF8_incorrect";
					return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else if (0b11100000 == (pointer_to_begin_UTF8_symv[0] & 0b11100000))
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 3 байтами. Теперь нужно проверить остальын три байта на корректность, то есть наличие первых двух Бит "10" в начале каждого последующего байта символа.


			//-------------------------------------------
			if (pointer_SizeByte >= 3)
			{
				if ((0b10000000 == (pointer_to_begin_UTF8_symv[1] & 0b11000000)) && (0b10000000 == (pointer_to_begin_UTF8_symv[2] & 0b11000000)))
				{
					UTF8_symv_ByteSize = 3;
					return result_flag::OK;  	//Значит служебные биты остальных байтов соответсвуют служебным битам данного UTF-8 символа, значит это коректный UTF-8 символ.
				}
				else
				{
					error = "UTF8_incorrect";
					return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else if (0b11000000 == (pointer_to_begin_UTF8_symv[0] & 0b11000000))
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 2 байтами. Теперь нужно проверить остальын три байта на корректность, то есть наличие первых двух Бит "10" в начале каждого последующего байта символа.


			//-------------------------------------------
			if (pointer_SizeByte >= 2)
			{
				if ((0b10000000 == (pointer_to_begin_UTF8_symv[1] & 0b11000000)))
				{
					UTF8_symv_ByteSize = 2;
					return result_flag::OK; 	//Значит служебные биты остальных байтов соответсвуют служебным битам данного UTF-8 символа, значит это коректный UTF-8 символ.
				}
				else
				{
					error = "UTF8_incorrect";
					return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else if (0b00000000 == (pointer_to_begin_UTF8_symv[0] & 0b10000000))     //То есть если проверяемый байт имеет первый Бит 1_1010101 - то после применения "И" - будет 1_0010101 - что не соответвует одному из 4-ех видов начальных бит. Если же первый бит 0_0010101 - то после "И" соответсвенно будет 0_0010101 - что показывает, что первый Бит нулевой, что соответвует одному их 4 видов.
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 1 байтами. Проверка окончена.

			//-------------------------------------------
			if (pointer_SizeByte >= 1)
			{
				UTF8_symv_ByteSize = 1;
				return result_flag::OK;
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else
		{
			error = "UTF8_incorrect";
			return result_flag::UTF8_incorrect;  //То есть это точно НЕ ПЕРВЫЙ байт любого символа UTF-8.
		}
	}

	inline const result_flag check__UTF8symv_and_return_ByteSize_(const char* pointer_to_begin_UTF8_symv, const size_t pointer_SizeByte, const char** pointer_to_IncorrectByte, int& UTF8_symv_ByteSize) noexcept
	{

		//-------------------------------------------------------------------------------------------------------
		//Описание, как кодируется символ UTF8: в трех примерах:
		//Кодировка UTF8 - это кодировка переменной длинны от 1 до 4 байт:
		//Условимся, что мы рассматриваем байты и биты слева направо, то есть первый бит или байт всегда будет слева, осталные правее.

		//Пример1: Закодируем символ ASCII к примеру "Q" кодировкой UTF8:
		//Первый БИТ начального байта будет содержать 0 - он будет указывать на то, что это первый БАЙТ - это единсвенный БАЙТ, в котором содержится кодировка символа "Q".
		//Остальные БИТы БАЙТа - будут содержать само закодированное значение и это значение 81 из таблицы ASII закодированое в бинарное - 1010001 и с учетом первого нуля - это будет: 01010001 - то есть значение кодировки UTF8 для символов ASCII будет ПОЛНОСТЬЮ совпадать.



		//Пример2: Закодируем символ к примеру "Ǎ" "Латинская заглавная буква «A» с гачеком Ǎ" кодировкой UTF8:
		//В таблице Юникода буква "Ǎ" - имеет десятичное значение - 461.
		//Переведем десятичное значения 461 - в двоичное число: 111001101 - то есть для кодирования значения 461 или буквы "Ǎ" нужно 9 БИТ. 9 рабочих БИТ, не учитывая служебные БИТы, которые говрят о кол-ве БАЙТ, которым закодирован символ.
		//Теперь закодируем значение 111001101 по правилам кодировки UTF8:
		//То есть в первом БАЙТЕ в первых двух БИТАХ указывается - 11, что будет говорит о том, что кодирование значения "111001101" будет занимать два БАЙТА.
		//В первом байте после указания "11" ДОЛЖЕН ВСЕГДА ОБЯЗАТЕЛЬНО ИДТИ БИТ с числом "0": 110 ... ТАК ПРЕДВАРИТЕЛЬНО Первый БАЙТ Готов. ТЕПЕРЬ ВАЖНО: теперь заполнение эти Двух БАЙТ нужно ввести с конца, то есть в данном случае со второго БАЙТА:
		//Теперь закодируем Второй БАЙТ : ЛЮБЫЕ ПОСЛЕДУЮЩИЕ БАЙТЫ закодированного символа после Первого байта ДОЛЖНЫ начинатся с служебных двух БИТОВ : 10 ..и после них уже должны идти следующие рабочие БИТЫ.
		//То есть во Втором БАЙТЕ у нас с учетом первых двух служебных БИТОВ 10 - остается 6 БИТ: заполним их 6-Ю БИТАМИ из нашего символа "Ǎ" справа: "111_001101" - то есть мы берем правую часть БИТОВ, который помещаются во второй БАЙТ и помещаем их во Второй БАЙТ: 10_001101
		//Теперь переходим к Первому БАЙТУ и оставшимся БИТАМ из "Ǎ"  "111_001101" - то есть остались "111" - их нужно добавить в Первый БАЙТ...
		//Напоминаю, что Первый БАЙТ у нас заполнен на данный момент следующими служебными БИТАМИ: 110... - то есть у нас в Первом БАЙТЕ свободно еще 5 БИТ, но ДОБАВИТЬ нужно всего 3 БИТА "111"...
		//ДОБАВЛЯТЬ эти три БИТА "111" в Первый БАЙТ нужно также справа и те БИТЫ в первом БАЙТЕ, который не "заполнились" - заполняем нулем: итого: "110_00_111"


		//ВСЕ остальные символы кодируются по этим двум правилам: итого:
		//Кодировка UTF8 может занимать от 1 до 4 байт.
		//Символ закодированный 1 байтом - всегда начинается с 0 бита слева. И все остальные 7 бит - это самое значение символа:
		//0.....

		//Символ закодированный от 2 до 4 байт ВСЕГДА начинается в Первом байте с:
		//110...     - закодирован двумя байтами
		//1110...    - закодирован тремя байтами
		//11110...   - закодирован четрьмя байтами

		//Вторые и поcледующие байты символ закодированных более, чем одним байтом ВСЕГДА начинаются с двух первых битов "10" и остальные 6 бит - заполянются уже рабочми битами самого символа.
		//Заполнение рабочих бит символа - идет с последнего байта к первому. И там где в Первом байте остались "незаполенные биты" - они заполняются нулями.
		//-------------------------------------------------------------------------------------------------------


		//-------------------------------------------------------------------------------------------------
		//pointer_to_begin_UTF8_symv - должен быть указатель на первый байт UTF-8 символа.
		//pointer_SizeByte      - размер байт, в пределах которого нужно проверить Один символ на который указывает "pointer_to_begin_UTF8_symv", может быть по сути любым, ТО ЕСТЬ, функция проверит является ли Указатель "pointer_to_begin_UTF8_symv" указатель на начальый Байт UTF-8 символа, и если является, то првоерит корректность остальных байт этого UTF-8 символа, если размер указанынй в "pointer_SizeByte" будет больше, чем общий размер в байтах UTF-8 символа, то он просто не будет ни как испльзоватся, а, если размер указанынй в "pointer_SizeByte" будет меньше, чем размер в байтах UTF-8 символа на который указывает указатель "pointer_to_begin_UTF8_symv" - то такой символ будет признан Некорректным, так как функции просто тупо не хватило данных для окончания анализа.
		//-------------------------------------------------------------------------------------------------



		//А теперь применем побитовое "И" для того, чтобы определить есть ли в проверяемом байте сооветствие 4-ем видам битов UTF-8 говорящее о кол-ве задействоаных байт для кодировки символа:

		if (0b11110000 == (pointer_to_begin_UTF8_symv[0] & 0b11110000))
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 4 байтами. Теперь нужно проверить остальын три байта на корректность, то есть наличие первых двух Бит "10" в начале каждого последующего байта символа.

			//-------------------------------------------
			if (pointer_SizeByte >= 4)
			{
				if (0b10000000 == (pointer_to_begin_UTF8_symv[1] & 0b11000000))
				{
					if (0b10000000 == (pointer_to_begin_UTF8_symv[2] & 0b11000000))
					{
						if (0b10000000 == (pointer_to_begin_UTF8_symv[3] & 0b11000000))
						{
							UTF8_symv_ByteSize = 4;
							return result_flag::OK;  					//Значит служебные биты остальных байтов соответсвуют служебным битам данного UTF-8 символа, значит это коректный UTF-8 символ.
						}
						else
						{
							*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[3];
							error = "UTF8_incorrect";
							return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
						}
					}
					else
					{
						*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[2];
						error = "UTF8_incorrect";
						return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
					}
				}
				else
				{
					*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[1];
					error = "UTF8_incorrect";
					return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else if (0b11100000 == (pointer_to_begin_UTF8_symv[0] & 0b11100000))
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 3 байтами. Теперь нужно проверить остальын три байта на корректность, то есть наличие первых двух Бит "10" в начале каждого последующего байта символа.


			//-------------------------------------------
			if (pointer_SizeByte >= 3)
			{
				if (0b10000000 == (pointer_to_begin_UTF8_symv[1] & 0b11000000))
				{
					if (0b10000000 == (pointer_to_begin_UTF8_symv[2] & 0b11000000))
					{
						UTF8_symv_ByteSize = 3;
						return result_flag::OK;  					//Значит служебные биты остальных байтов соответсвуют служебным битам данного UTF-8 символа, значит это коректный UTF-8 символ.
					}
					else
					{
						*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[2];
						error = "UTF8_incorrect";
						return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
					}
				}
				else
				{
					*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[1];
					error = "UTF8_incorrect";
					return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else if (0b11000000 == (pointer_to_begin_UTF8_symv[0] & 0b11000000))
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 2 байтами. Теперь нужно проверить остальын три байта на корректность, то есть наличие первых двух Бит "10" в начале каждого последующего байта символа.


			//-------------------------------------------
			if (pointer_SizeByte >= 2)
			{
				if (0b10000000 == (pointer_to_begin_UTF8_symv[1] & 0b11000000))
				{
					UTF8_symv_ByteSize = 2;
					return result_flag::OK;  					//Значит служебные биты остальных байтов соответсвуют служебным битам данного UTF-8 символа, значит это коректный UTF-8 символ.
				}
				else
				{
					*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[1];
					error = "UTF8_incorrect";
					return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else if (0b00000000 == (pointer_to_begin_UTF8_symv[0] & 0b10000000))     //То есть если проверяемый байт имеет первый Бит 1_1010101 - то после применения "И" - будет 1_0010101 - что не соответвует одному из 4-ех видов начальных бит. Если же первый бит 0_0010101 - то после "И" соответсвенно будет 0_0010101 - что показывает, что первый Бит нулевой, что соответвует одному их 4 видов.
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 1 байтами. Проверка окончена.

			//-------------------------------------------
			if (pointer_SizeByte >= 1)
			{
				UTF8_symv_ByteSize = 1;
				return result_flag::OK;
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------


		}
		else
		{
			*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[0];
			error = "UTF8_incorrect";
			return result_flag::UTF8_incorrect;   //То есть это точно НЕ ПЕРВЫЙ байт любого символа UTF-8.
		}
	}

	inline const result_flag check__UTF8symv_SurrogatePair_and_return_ByteSize_(const char* pointer_to_begin_UTF8_symv, const size_t pointer_SizeByte, int& UTF8_symv_ByteSize) noexcept
	{

		//-------------------------------------------------------------------------------------------------------
		//Описание, как кодируется символ UTF8: в трех примерах:
		//Кодировка UTF8 - это кодировка переменной длинны от 1 до 4 байт:
		//Условимся, что мы рассматриваем байты и биты слева направо, то есть первый бит или байт всегда будет слева, осталные правее.

		//Пример1: Закодируем символ ASCII к примеру "Q" кодировкой UTF8:
		//Первый БИТ начального байта будет содержать 0 - он будет указывать на то, что это первый БАЙТ - это единсвенный БАЙТ, в котором содержится кодировка символа "Q".
		//Остальные БИТы БАЙТа - будут содержать само закодированное значение и это значение 81 из таблицы ASII закодированое в бинарное - 1010001 и с учетом первого нуля - это будет: 01010001 - то есть значение кодировки UTF8 для символов ASCII будет ПОЛНОСТЬЮ совпадать.



		//Пример2: Закодируем символ к примеру "Ǎ" "Латинская заглавная буква «A» с гачеком Ǎ" кодировкой UTF8:
		//В таблице Юникода буква "Ǎ" - имеет десятичное значение - 461.
		//Переведем десятичное значения 461 - в двоичное число: 111001101 - то есть для кодирования значения 461 или буквы "Ǎ" нужно 9 БИТ. 9 рабочих БИТ, не учитывая служебные БИТы, которые говрят о кол-ве БАЙТ, которым закодирован символ.
		//Теперь закодируем значение 111001101 по правилам кодировки UTF8:
		//То есть в первом БАЙТЕ в первых двух БИТАХ указывается - 11, что будет говорит о том, что кодирование значения "111001101" будет занимать два БАЙТА.
		//В первом байте после указания "11" ДОЛЖЕН ВСЕГДА ОБЯЗАТЕЛЬНО ИДТИ БИТ с числом "0": 110 ... ТАК ПРЕДВАРИТЕЛЬНО Первый БАЙТ Готов. ТЕПЕРЬ ВАЖНО: теперь заполнение эти Двух БАЙТ нужно ввести с конца, то есть в данном случае со второго БАЙТА:
		//Теперь закодируем Второй БАЙТ : ЛЮБЫЕ ПОСЛЕДУЮЩИЕ БАЙТЫ закодированного символа после Первого байта ДОЛЖНЫ начинатся с служебных двух БИТОВ : 10 ..и после них уже должны идти следующие рабочие БИТЫ.
		//То есть во Втором БАЙТЕ у нас с учетом первых двух служебных БИТОВ 10 - остается 6 БИТ: заполним их 6-Ю БИТАМИ из нашего символа "Ǎ" справа: "111_001101" - то есть мы берем правую часть БИТОВ, который помещаются во второй БАЙТ и помещаем их во Второй БАЙТ: 10_001101
		//Теперь переходим к Первому БАЙТУ и оставшимся БИТАМ из "Ǎ"  "111_001101" - то есть остались "111" - их нужно добавить в Первый БАЙТ...
		//Напоминаю, что Первый БАЙТ у нас заполнен на данный момент следующими служебными БИТАМИ: 110... - то есть у нас в Первом БАЙТЕ свободно еще 5 БИТ, но ДОБАВИТЬ нужно всего 3 БИТА "111"...
		//ДОБАВЛЯТЬ эти три БИТА "111" в Первый БАЙТ нужно также справа и те БИТЫ в первом БАЙТЕ, который не "заполнились" - заполняем нулем: итого: "110_00_111"


		//ВСЕ остальные символы кодируются по этим двум правилам: итого:
		//Кодировка UTF8 может занимать от 1 до 4 байт.
		//Символ закодированный 1 байтом - всегда начинается с 0 бита слева. И все остальные 7 бит - это самое значение символа:
		//0.....

		//Символ закодированный от 2 до 4 байт ВСЕГДА начинается в Первом байте с:
		//110...     - закодирован двумя байтами
		//1110...    - закодирован тремя байтами
		//11110...   - закодирован четрьмя байтами

		//Вторые и поcледующие байты символ закодированных более, чем одним байтом ВСЕГДА начинаются с двух первых битов "10" и остальные 6 бит - заполянются уже рабочми битами самого символа.
		//Заполнение рабочих бит символа - идет с последнего байта к первому. И там где в Первом байте остались "незаполенные биты" - они заполняются нулями.
		//-------------------------------------------------------------------------------------------------------


		//-------------------------------------------------------------------------------------------------
		//pointer_to_begin_UTF8_symv - должен быть указатель на первый байт UTF-8 символа.
		//pointer_SizeByte           - размер байт, в пределах которого нужно проверить Один символ на который указывает "pointer_to_begin_UTF8_symv", может быть по сути любым, ТО ЕСТЬ, функция проверит является ли Указатель "pointer_to_begin_UTF8_symv" указатель на начальый Байт UTF-8 символа, и если является, то првоерит корректность остальных байт этого UTF-8 символа, если размер указанынй в "pointer_SizeByte" будет больше, чем общий размер в байтах UTF-8 символа, то он просто не будет ни как испльзоватся, а, если размер указанынй в "pointer_SizeByte" будет меньше, чем размер в байтах UTF-8 символа на который указывает указатель "pointer_to_begin_UTF8_symv" - то такой символ будет признан Некорректным, так как функции просто тупо не хватило данных для окончания анализа.
		//-------------------------------------------------------------------------------------------------







		//А теперь применем побитовое "И" для того, чтобы определить есть ли в проверяемом байте сооветствие 4-ем видам битов UTF-8 говорящее о кол-ве задействоаных байт для кодировки символа:

		if (0b11110000 == (pointer_to_begin_UTF8_symv[0] & 0b11110000))
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 4 байтами. Теперь нужно проверить остальын три байта на корректность, то есть наличие первых двух Бит "10" в начале каждого последующего байта символа.

			//-------------------------------------------
			if (pointer_SizeByte >= 4)
			{
				if ((0b10000000 == (pointer_to_begin_UTF8_symv[1] & 0b11000000)) && (0b10000000 == (pointer_to_begin_UTF8_symv[2] & 0b11000000)) && (0b10000000 == (pointer_to_begin_UTF8_symv[3] & 0b11000000)))
				{

					//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
					const int_least32_t UnicodePointCode = convert__UTF8Symv_to_UnicodePointCode(pointer_to_begin_UTF8_symv, 4);

					if (UnicodePointCode > 1114111)     //Если больше максимального значения для Юникодного символа
					{
						error = "Greater_than_MaxUnicodeValue";
						return result_flag::Greater_than_MaxUnicodeValue;
					}
					//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


					UTF8_symv_ByteSize = 4;
					return result_flag::OK;  					//Значит служебные биты остальных байтов соответсвуют служебным битам данного UTF-8 символа, значит это коректный UTF-8 символ.
				}
				else
				{
					error = "UTF8_incorrect";
					return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else if (0b11100000 == (pointer_to_begin_UTF8_symv[0] & 0b11100000))
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 3 байтами. Теперь нужно проверить остальын три байта на корректность, то есть наличие первых двух Бит "10" в начале каждого последующего байта символа.


			//-------------------------------------------
			if (pointer_SizeByte >= 3)
			{
				if ((0b10000000 == (pointer_to_begin_UTF8_symv[1] & 0b11000000)) && (0b10000000 == (pointer_to_begin_UTF8_symv[2] & 0b11000000)))
				{

					//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
					const int_least32_t UnicodePointCode = convert__UTF8Symv_to_UnicodePointCode(pointer_to_begin_UTF8_symv, 3);

					if (UnicodePointCode >= 55296 && UnicodePointCode <= 57343)     //(0xD800 – 0xDFFF)
					{
						error = "IncludeSurrogatePairValue";
						return result_flag::IncludeSurrogatePairValue;    //Данный условно корреткный UTF-8 символ входит в Юникодное значение "суррогатной пары" - для которого в Юникодной таблицы - нет никаких символов. А значит такого Юникодного символа закодированного в UTF-8 или в какой либо другой кодировке быть не должно.
					}
					//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


					UTF8_symv_ByteSize = 3;
					return result_flag::OK;  	//Значит служебные биты остальных байтов соответсвуют служебным битам данного UTF-8 символа, значит это коректный UTF-8 символ.
				}
				else
				{
					error = "UTF8_incorrect";
					return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else if (0b11000000 == (pointer_to_begin_UTF8_symv[0] & 0b11000000))
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 2 байтами. Теперь нужно проверить остальын три байта на корректность, то есть наличие первых двух Бит "10" в начале каждого последующего байта символа.


			//-------------------------------------------
			if (pointer_SizeByte >= 2)
			{
				if ((0b10000000 == (pointer_to_begin_UTF8_symv[1] & 0b11000000)))
				{
					UTF8_symv_ByteSize = 2;
					return result_flag::OK;  	//Значит служебные биты остальных байтов соответсвуют служебным битам данного UTF-8 символа, значит это коректный UTF-8 символ.
				}
				else
				{
					error = "UTF8_incorrect";
					return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else if (0b00000000 == (pointer_to_begin_UTF8_symv[0] & 0b10000000))     //То есть если проверяемый байт имеет первый Бит 1_1010101 - то после применения "И" - будет 1_0010101 - что не соответвует одному из 4-ех видов начальных бит. Если же первый бит 0_0010101 - то после "И" соответсвенно будет 0_0010101 - что показывает, что первый Бит нулевой, что соответвует одному их 4 видов.
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 1 байтами. Проверка окончена.

			//-------------------------------------------
			if (pointer_SizeByte >= 1)
			{
				UTF8_symv_ByteSize = 1;
				return result_flag::OK;
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else
		{
			error = "UTF8_incorrect";
			return result_flag::UTF8_incorrect;   //То есть это точно НЕ ПЕРВЫЙ байт любого символа UTF-8.
		}
	}

	inline const result_flag check__UTF8symv_SurrogatePair_and_return_ByteSize_(const char* pointer_to_begin_UTF8_symv, const size_t pointer_SizeByte, const char** pointer_to_IncorrectByte, int& UTF8_symv_ByteSize) noexcept
	{

		//-------------------------------------------------------------------------------------------------------
		//Описание, как кодируется символ UTF8: в трех примерах:
		//Кодировка UTF8 - это кодировка переменной длинны от 1 до 4 байт:
		//Условимся, что мы рассматриваем байты и биты слева направо, то есть первый бит или байт всегда будет слева, осталные правее.

		//Пример1: Закодируем символ ASCII к примеру "Q" кодировкой UTF8:
		//Первый БИТ начального байта будет содержать 0 - он будет указывать на то, что это первый БАЙТ - это единсвенный БАЙТ, в котором содержится кодировка символа "Q".
		//Остальные БИТы БАЙТа - будут содержать само закодированное значение и это значение 81 из таблицы ASII закодированое в бинарное - 1010001 и с учетом первого нуля - это будет: 01010001 - то есть значение кодировки UTF8 для символов ASCII будет ПОЛНОСТЬЮ совпадать.



		//Пример2: Закодируем символ к примеру "Ǎ" "Латинская заглавная буква «A» с гачеком Ǎ" кодировкой UTF8:
		//В таблице Юникода буква "Ǎ" - имеет десятичное значение - 461.
		//Переведем десятичное значения 461 - в двоичное число: 111001101 - то есть для кодирования значения 461 или буквы "Ǎ" нужно 9 БИТ. 9 рабочих БИТ, не учитывая служебные БИТы, которые говрят о кол-ве БАЙТ, которым закодирован символ.
		//Теперь закодируем значение 111001101 по правилам кодировки UTF8:
		//То есть в первом БАЙТЕ в первых двух БИТАХ указывается - 11, что будет говорит о том, что кодирование значения "111001101" будет занимать два БАЙТА.
		//В первом байте после указания "11" ДОЛЖЕН ВСЕГДА ОБЯЗАТЕЛЬНО ИДТИ БИТ с числом "0": 110 ... ТАК ПРЕДВАРИТЕЛЬНО Первый БАЙТ Готов. ТЕПЕРЬ ВАЖНО: теперь заполнение эти Двух БАЙТ нужно ввести с конца, то есть в данном случае со второго БАЙТА:
		//Теперь закодируем Второй БАЙТ : ЛЮБЫЕ ПОСЛЕДУЮЩИЕ БАЙТЫ закодированного символа после Первого байта ДОЛЖНЫ начинатся с служебных двух БИТОВ : 10 ..и после них уже должны идти следующие рабочие БИТЫ.
		//То есть во Втором БАЙТЕ у нас с учетом первых двух служебных БИТОВ 10 - остается 6 БИТ: заполним их 6-Ю БИТАМИ из нашего символа "Ǎ" справа: "111_001101" - то есть мы берем правую часть БИТОВ, который помещаются во второй БАЙТ и помещаем их во Второй БАЙТ: 10_001101
		//Теперь переходим к Первому БАЙТУ и оставшимся БИТАМ из "Ǎ"  "111_001101" - то есть остались "111" - их нужно добавить в Первый БАЙТ...
		//Напоминаю, что Первый БАЙТ у нас заполнен на данный момент следующими служебными БИТАМИ: 110... - то есть у нас в Первом БАЙТЕ свободно еще 5 БИТ, но ДОБАВИТЬ нужно всего 3 БИТА "111"...
		//ДОБАВЛЯТЬ эти три БИТА "111" в Первый БАЙТ нужно также справа и те БИТЫ в первом БАЙТЕ, который не "заполнились" - заполняем нулем: итого: "110_00_111"


		//ВСЕ остальные символы кодируются по этим двум правилам: итого:
		//Кодировка UTF8 может занимать от 1 до 4 байт.
		//Символ закодированный 1 байтом - всегда начинается с 0 бита слева. И все остальные 7 бит - это самое значение символа:
		//0.....

		//Символ закодированный от 2 до 4 байт ВСЕГДА начинается в Первом байте с:
		//110...     - закодирован двумя байтами
		//1110...    - закодирован тремя байтами
		//11110...   - закодирован четрьмя байтами

		//Вторые и поcледующие байты символ закодированных более, чем одним байтом ВСЕГДА начинаются с двух первых битов "10" и остальные 6 бит - заполянются уже рабочми битами самого символа.
		//Заполнение рабочих бит символа - идет с последнего байта к первому. И там где в Первом байте остались "незаполенные биты" - они заполняются нулями.
		//-------------------------------------------------------------------------------------------------------


		//-------------------------------------------------------------------------------------------------
		//pointer_to_begin_UTF8_symv - должен быть указатель на первый байт UTF-8 символа.
		//pointer_SizeByte           - размер байт, в пределах которого нужно проверить Один символ на который указывает "pointer_to_begin_UTF8_symv", может быть по сути любым, ТО ЕСТЬ, функция проверит является ли Указатель "pointer_to_begin_UTF8_symv" указатель на начальый Байт UTF-8 символа, и если является, то првоерит корректность остальных байт этого UTF-8 символа, если размер указанынй в "pointer_SizeByte" будет больше, чем общий размер в байтах UTF-8 символа, то он просто не будет ни как испльзоватся, а, если размер указанынй в "pointer_SizeByte" будет меньше, чем размер в байтах UTF-8 символа на который указывает указатель "pointer_to_begin_UTF8_symv" - то такой символ будет признан Некорректным, так как функции просто тупо не хватило данных для окончания анализа.
		//-------------------------------------------------------------------------------------------------



		//А теперь применем побитовое "И" для того, чтобы определить есть ли в проверяемом байте сооветствие 4-ем видам битов UTF-8 говорящее о кол-ве задействоаных байт для кодировки символа:

		if (0b11110000 == (pointer_to_begin_UTF8_symv[0] & 0b11110000))
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 4 байтами. Теперь нужно проверить остальын три байта на корректность, то есть наличие первых двух Бит "10" в начале каждого последующего байта символа.

			//-------------------------------------------
			if (pointer_SizeByte >= 4)
			{
				if (0b10000000 == (pointer_to_begin_UTF8_symv[1] & 0b11000000))
				{
					if (0b10000000 == (pointer_to_begin_UTF8_symv[2] & 0b11000000))
					{
						if (0b10000000 == (pointer_to_begin_UTF8_symv[3] & 0b11000000))
						{

							//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
							const int_least32_t UnicodePointCode = convert__UTF8Symv_to_UnicodePointCode(pointer_to_begin_UTF8_symv, 4);

							if (UnicodePointCode > 1114111)     //Если больше максимального значения для Юникодного символа
							{
								error = "Greater_than_MaxUnicodeValue";
								return result_flag::Greater_than_MaxUnicodeValue;
							}
							//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


							UTF8_symv_ByteSize = 4;
							return result_flag::OK;  					//Значит служебные биты остальных байтов соответсвуют служебным битам данного UTF-8 символа, значит это коректный UTF-8 символ.
						}
						else
						{
							*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[3];
							error = "UTF8_incorrect";
							return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
						}
					}
					else
					{
						*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[2];
						error = "UTF8_incorrect";
						return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
					}
				}
				else
				{
					*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[1];
					error = "UTF8_incorrect";
					return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else if (0b11100000 == (pointer_to_begin_UTF8_symv[0] & 0b11100000))
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 3 байтами. Теперь нужно проверить остальын три байта на корректность, то есть наличие первых двух Бит "10" в начале каждого последующего байта символа.


			//-------------------------------------------
			if (pointer_SizeByte >= 3)
			{
				if (0b10000000 == (pointer_to_begin_UTF8_symv[1] & 0b11000000))
				{
					if (0b10000000 == (pointer_to_begin_UTF8_symv[2] & 0b11000000))
					{

						//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
						const int_least32_t UnicodePointCode = convert__UTF8Symv_to_UnicodePointCode(pointer_to_begin_UTF8_symv, 3);

						if (UnicodePointCode >= 55296 && UnicodePointCode <= 57343)     //(0xD800 – 0xDFFF)
						{
							error = "IncludeSurrogatePairValue";
							return result_flag::IncludeSurrogatePairValue;    //Данный условно корреткный UTF-8 символ входит в Юникодное значение "суррогатной пары" - для которого в Юникодной таблицы - нет никаких символов. А значит такого Юникодного символа закодированного в UTF-8 или в какой либо другой кодировке быть не должно.
						}
						//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


						UTF8_symv_ByteSize = 3;
						return result_flag::OK;  					//Значит служебные биты остальных байтов соответсвуют служебным битам данного UTF-8 символа, значит это коректный UTF-8 символ.
					}
					else
					{
						*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[2];
						error = "UTF8_incorrect";
						return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
					}
				}
				else
				{
					*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[1];
					error = "UTF8_incorrect";
					return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else if (0b11000000 == (pointer_to_begin_UTF8_symv[0] & 0b11000000))
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 2 байтами. Теперь нужно проверить остальын три байта на корректность, то есть наличие первых двух Бит "10" в начале каждого последующего байта символа.


			//-------------------------------------------
			if (pointer_SizeByte >= 2)
			{
				if (0b10000000 == (pointer_to_begin_UTF8_symv[1] & 0b11000000))
				{
					UTF8_symv_ByteSize = 2;
					return result_flag::OK;  					//Значит служебные биты остальных байтов соответсвуют служебным битам данного UTF-8 символа, значит это коректный UTF-8 символ.
				}
				else
				{
					*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[1];
					error = "UTF8_incorrect";
					return result_flag::UTF8_incorrect; //Значит не все последующие байты имеют корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else if (0b00000000 == (pointer_to_begin_UTF8_symv[0] & 0b10000000))     //То есть если проверяемый байт имеет первый Бит 1_1010101 - то после применения "И" - будет 1_0010101 - что не соответвует одному из 4-ех видов начальных бит. Если же первый бит 0_0010101 - то после "И" соответсвенно будет 0_0010101 - что показывает, что первый Бит нулевой, что соответвует одному их 4 видов.
		{
			//Значит Начальный байт является корректным и говорит о том, что UTF-8 символ закодировн 1 байтами. Проверка окончена.

			//-------------------------------------------
			if (pointer_SizeByte >= 1)
			{
				UTF8_symv_ByteSize = 1;
				return result_flag::OK;
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-8 символ. Надо лучше размеры указывать.
			}
			//-------------------------------------------

		}
		else
		{
			*pointer_to_IncorrectByte = &pointer_to_begin_UTF8_symv[0];
			error = "UTF8_incorrect";
			return result_flag::UTF8_incorrect;   //То есть это точно НЕ ПЕРВЫЙ байт любого символа UTF-8.
		}
	}


	const result_flag check_UTF8_string(const char* pointer_to_UTF8_string, const size_t string_UTF8_size_byte) noexcept
	{
		//pointer_to_UTF8_string - должен быть указатель на первый байт UTF-8 символа.
		//string_UTF8_size_byte  - размер в байтах всего массива на который указывает указатель "pointer_to_UTF8_string"


		result_flag result;
		int UTF8_symv_ByteSize;


		//-----------------------------------------------------------------------------------------------
		for (size_t i = 0; i < string_UTF8_size_byte; i++)
		{
			result = check__UTF8symv_and_return_ByteSize_(&pointer_to_UTF8_string[i], string_UTF8_size_byte - i, UTF8_symv_ByteSize);

			if (result != result_flag::OK)
			{
				//Значит "&pointer_to_UTF8_string[i]" - наткнулся на некорректное начало очередного UTF-8 символа

				return result;
			}

			i = i + UTF8_symv_ByteSize - 1;          //Прибавляем размер байт найденого UTF8-символа, чтобы со следующей итерацией начать с теоертически нового UTF-8 символа. "-1" - нужен для компенсации того, что со следующей итерацией цикла "for" сам еще прибавит "+1". 
		}
		//-----------------------------------------------------------------------------------------------

		return result_flag::OK;
	}

	const result_flag check_UTF8_string(const char* pointer_to_UTF8_string, const size_t string_UTF8_size_byte, const char** pointer_to_IncorrectByte) noexcept
	{
		//pointer_to_UTF8_string - должен быть указатель на первый байт UTF-8 символа.
		//string_UTF8_size_byte  - размер в байтах всего массива на который указывает указатель "pointer_to_UTF8_string"


		result_flag result;
		int UTF8_symv_ByteSize;

		//-----------------------------------------------------------------------------------------------
		for (size_t i = 0; i < string_UTF8_size_byte; i++)
		{
			result = check__UTF8symv_and_return_ByteSize_(&pointer_to_UTF8_string[i], string_UTF8_size_byte - i, pointer_to_IncorrectByte, UTF8_symv_ByteSize);          //Проверяем сооветтвует ли байт на который указывает указатель - началу UTF-8 символа.

			if (result != result_flag::OK)
			{
				//Значит "&pointer_to_UTF8_string[i]" - наткнулся на некорректное начало очередного UTF-8 символа

				return result;
			}

			i = i + UTF8_symv_ByteSize - 1;          //Прибавляем размер байт найденого UTF8-символа, чтобы со следующей итерацией начать с теоертически нового UTF-8 символа. "-1" - нужен для компенсации того, что со следующей итерацией цикла "for" сам еще прибавит "+1". 
		}
		//-----------------------------------------------------------------------------------------------


		return result_flag::OK;
	}

	const result_flag check__UTF8_string_SurrogatePair(const char* pointer_to_UTF8_string, const size_t string_UTF8_size_byte) noexcept
	{
		//pointer_to_UTF8_string - должен быть указатель на первый байт UTF-8 символа.
		//string_UTF8_size_byte  - размер в байтах всего массива на который указывает указатель "pointer_to_UTF8_string"


		result_flag result;
		int UTF8_symv_ByteSize;

		//-----------------------------------------------------------------------------------------------
		for (size_t i = 0; i < string_UTF8_size_byte; i++)
		{
			result = check__UTF8symv_SurrogatePair_and_return_ByteSize_(&pointer_to_UTF8_string[i], string_UTF8_size_byte - i, UTF8_symv_ByteSize);

			if (result != result_flag::OK)
			{
				//Значит "&pointer_to_UTF8_string[i]" - наткнулся на некорректное значение UTF-8 символа.

				return result;
			}

			i = i + UTF8_symv_ByteSize - 1;          //Прибавляем размер байт найденого UTF8-символа, чтобы со следующей итерацией начать с теоертически нового UTF-8 символа. "-1" - нужен для компенсации того, что со следующей итерацией цикла "for" сам еще прибавит "+1". 
		}
		//-----------------------------------------------------------------------------------------------

		return result_flag::OK;
	}

	const result_flag check__UTF8_string_SurrogatePair(const char* pointer_to_UTF8_string, const size_t string_UTF8_size_byte, const char** pointer_to_IncorrectByte) noexcept
	{
		//pointer_to_UTF8_string - должен быть указатель на первый байт UTF-8 символа.
		//string_UTF8_size_byte  - размер в байтах всего массива на который указывает указатель "pointer_to_UTF8_string"


		result_flag result;
		int UTF8_symv_ByteSize;

		//-----------------------------------------------------------------------------------------------
		for (size_t i = 0; i < string_UTF8_size_byte; i++)
		{
			result = check__UTF8symv_SurrogatePair_and_return_ByteSize_(&pointer_to_UTF8_string[i], string_UTF8_size_byte - i, pointer_to_IncorrectByte, UTF8_symv_ByteSize);          //Проверяем сооветтвует ли байт на который указывает указатель - началу UTF-8 символа.

			if (result != result_flag::OK)
			{
				//Значит "&pointer_to_UTF8_string[i]" - наткнулся на некорректное значение UTF-8 символа.

				return result;
			}

			i = i + UTF8_symv_ByteSize - 1;          //Прибавляем размер байт найденого UTF8-символа, чтобы со следующей итерацией начать с теоертически нового UTF-8 символа. "-1" - нужен для компенсации того, что со следующей итерацией цикла "for" сам еще прибавит "+1". 
		}
		//-----------------------------------------------------------------------------------------------


		return result_flag::OK;
	}
	//****************************************************************UTF-8:Конец***********************************************************************************


	//****************************************************************UTF-16:Начало***********************************************************************************
	template<typename T> inline const result_flag check_UTF16symvLE(const T* pointer_to_begin_UTF16_symv, const size_t pointer_Pair_size)   noexcept
	{

		//---------------------------------------------------------------------------------------------------------------------------------------
		//Про кодировку UTF-8 читать в "check__UTF8symv_and_return_ByteSize".
		//Про перевод символа UTF-8 в Юникодное значение читать в "convert__UTF8Symv_to_UnicodePointCode".

		//Кодировка UTF-16 - это кодировка переменной длинны, в которой используется либо одна двухбайтная пара, либо две двухбайтные пары.
		//Все значения Юникод от 0 до 65535 (за исключение значений зарезервированных для "суррогатных пар"(об этом ниже) - это (десятичное: 55296 - 57343, шестнадацтиричное:0xD800 – 0xDFFF)) - кодируются ровно первой двухбайтной парой прямым способом, то есть значение Юникод просто помещается в два байта без всяких служебных битов, в UTF-8 - это сооветтвует кодированию о 1 до 3 байт.
		//Все значения Юникод от 65536 до максимального значения Юникод в 1114111 - кодируются в UTF-16 уже двумя двухбайтными парами, в UTF-8 это соответвует кодровки в 4 байта. И вот тут уже появляются служебные биты для распознования того, что UTF-16 символ закодирван двумя парами:
		//В первой Левой двухбайтовой паре Слева(в первом байте) идут сначала 6 служебных бит:110110_xx xxxxxxxx
		//Во второй Правой двухбайтовой паре Слева(в первом байте) идут сначала 6 служебных бит:110111_xx xxxxxxxx
		//Итого получается так: [110110_xx xxxxxxxx] [110111_xx xxxxxxxx]   - где x - Это "рабочие" биты, которые и составляют самое Юникодно значение, НО БЛЯТЬ ЕСТЬ НЮАНС НАХОЙ!!!! На сцену выходят "Суррогатные мать их пары".

		//"Суррогтаные пары"    - Предположим, что нам нужно закодировать Юникодный символ под значением "55296" в UTF-16, так как значение меньше "65535" - то кодируется одной парой - простым присваиванием значения. В двочином виде получаем: [11011000 00000000]
		//[11011000 00000000]   - как видно данный символ Юникодный символ под значнием "55296" очень странным образом первыми 6-ю своими битами напомниает - служебные биты при двух парной кодировке UTF-16, а если так, то, как тогда отличать, реально закодированный одной парой символ "55296" от двух парной кодировки - если первые 6 бит у них совпадают ?? 
		//А ни как не отличать, для этого в Юникоде есть костыль-геморрой, состоящий в том, что ЮНИКОДНЫЙ СИМВОЛОВ СО ЗНАЧНИЕМ В ДИАПАЗОНЕ "55296 - 57343" НЕ СУЩЕСТВУЕТ!!! НИ ОНДНОМУ ЗНАЧЕНИЮ ИЗ ЭТОГО ДИАПАЗОНА НЕТ СООВТЕТВУЮЩЕГО ЮНИКОДНОГО СИМВОЛА, СПЕЦИАЛЬНО ДЛЯ ТОГО, ЧТОБЫ НЕ БЫЛО ВОТ ТАКОЙ ПРОБЛЕМЫ С ПЕРСЕЧЕНИЕМ РЕАЛНЫХ И СЛУЖЕБНЫХ БИТ ПРИ UTF-16 КОДИРОВКЕ.
		//Возникает вопрос, а НА***УЯ это вообще придумали, почему нельзя было сделать так, как в UTF-8 кодировки, то есть если символ Юникод умещается в первую пару UTF-16, то он бы кодировался первым нулевым служебным битом(как в однобоайтовой кодировке UTF-8), а потом бы уже шли рабочие биты. И если бы символ Юникода не умещался бы в первую пару, то тогда, первая парабы начиналась со служеюных бит "110", а во творой паре было бы продолжение в виде двух служебных бит "10" (как при двухбайтовой кодировании UTF-8) - КОРОЧЕ если я праильно понял, то из за того, что нужно было сохранить совместимость с ранеее сущестовашей еще до UTF-16 кодировки UCS-2, которая так же была строго двух байтной и кодировалась прямым способо без всяких служебных бит.

		//ОДНАКО это не все: то есть еще раз, все значения Юникода от 0 до 65535 (исключая не существующий сурогаттный диапазон) просто заносятся в первую пару без изменений и получатся кодировка UTF-16.
		//Значение выше 65535 (исключая не существующий сурогаттный диапазон):  для кодировки таких символов уже нужно кол-во рабочих бит от 17 бит до 21 бита, ОДНАКО смотрим сколько реально рабочих бит доступно при двухпарном кодировании - [110110_xx xxxxxxxx] [110111_xx xxxxxxxx] = 10 + 10 = 20. Доступно 20 бит!!! А для максимальных значений Юникода нужен 21 бит!!!  Че за фигня ?
		//При кодировании символа Юникод выше значения 65535 - Всегда из значения этого символа вычитается значение "65536": то есть если взять максимальное значние Юникодного символа в "1114111" вычесть из него 65536, то получим "1048575", что в двоичном виде займет ровно 20 бит - "11111111111111111111", которые уже раваномрно распределятся по двум парам UTF-16.
		//РАССМОТРИМ ПРИМЕР: Возьмем к примеру иероглиф "𠀅" со значением в Юникоде "131077", теперь по правилу вычитаем из него "65536": 131077 - 65536 = 65541.  
		//Смотрим как выглядет "65541" в двоичном виде: 10000000000000101  - теперь первые Левые 10 бит [10000000000]  берем и помещаем в первую пару за место X-ов: [110110_10 000000000].
		//После того, как мы взяли Левые 10 бит, у нас остаются 7 бит Справа [0000101] - к ним нужно доавбить Слева нули до 10 бит в этом случае это 3 нуля --> [0000000101] и помещаем их во вторую пару за место X-ов: [110111_00 00000101].
		//Все UTF-16 символ готов.
		//---------------------------------------------------------------------------------------------------------------------------------------


		//-------------------------------------------------------------------------------------------------
		//pointer_to_begin_UTF8_symv - должен быть указатель на первый байт UTF-8 символа.
		//string_UTF8_size_byte      - размер в символах whar_t, в пределах которого нужно проверить Один символ на который указывает "pointer_to_begin_UTF16_symv", может быть по сути любым, ТО ЕСТЬ, функция проверит является ли Указатель "pointer_to_begin_UTF16_symv" указатель на начальый Байт UTF-16 символа, и если является, то првоерит корректность остальных байт этого UTF-8 символа, если размер указанынй в "string_UTF8_size_byte" будет больше, чем общий размер в байтах UTF-8 символа, то он просто не будет ни как испльзоватся, а, если размер указанынй в "string_UTF8_size_byte" будет меньше, чем размер в байтах UTF-8 символа на который указывает указатель "pointer_to_begin_UTF8_symv" - то такой символ будет признан Некорректным, так как функции просто тупо не хватило данных для окончания анализа.
		//-------------------------------------------------------------------------------------------------



		if ((pointer_to_begin_UTF16_symv[0] & 0b1111110000000000) == 0b1101100000000000)
		{
			//Значит первый Левый байт этой Левой пары содержит служебные биты "110110" харакерные для символа UTF-16 закодированного двумя парами.
			//Теперь проверим содержит ли вторая пара, которая Справа от этой служеюные биты "110111":
			//-Если содержит, то это корректный UTF-16 символ состоязий из двух пар.
			//-Если не содержит, то значит это не UTF-16 символ, так как во второй Правой части нет обязательного наличия служебных бит "110111"


			//------------------------------------------------------------------------------------------
			if (pointer_Pair_size >= 2)
			{
				if ((pointer_to_begin_UTF16_symv[1] & 0b1111110000000000) == 0b1101110000000000)
				{
					return result_flag::OK;   //Возвращаем размер в символах wchar_t.
				}
				else
				{
					error = "UTF16_incorrect";
					return result_flag::UTF16_incorrect; //Значит вторая пара имеют Не корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-16 символ. Надо лучше размеры указывать.
			}
			//------------------------------------------------------------------------------------------


		}
		else
		{
			if (pointer_Pair_size >= 1)
			{
				return result_flag::OK;  //Значит UTF16 символ состоит из одной пары wchar_t.
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-16 символ. Надо лучше размеры указывать.
			}

		}

	}
	
	template<typename T> inline const result_flag check_UTF16symvLE(const T* pointer_to_begin_UTF16_symv, const size_t pointer_Pair_size, const T** pointer_to_IncorrectPair)   noexcept
	{

		//---------------------------------------------------------------------------------------------------------------------------------------
		//Про кодировку UTF-8 читать в "check__UTF8symv_and_return_ByteSize".
		//Про перевод символа UTF-8 в Юникодное значение читать в "convert__UTF8Symv_to_UnicodePointCode".

		//Кодировка UTF-16 - это кодировка переменной длинны, в которой используется либо одна двухбайтная пара, либо две двухбайтные пары.
		//Все значения Юникод от 0 до 65535 (за исключение значений зарезервированных для "суррогатных пар"(об этом ниже) - это (десятичное: 55296 - 57343, шестнадацтиричное:0xD800 – 0xDFFF)) - кодируются ровно первой двухбайтной парой прямым способом, то есть значение Юникод просто помещается в два байта без всяких служебных битов, в UTF-8 - это сооветтвует кодированию о 1 до 3 байт.
		//Все значения Юникод от 65536 до максимального значения Юникод в 1114111 - кодируются в UTF-16 уже двумя двухбайтными парами, в UTF-8 это соответвует кодровки в 4 байта. И вот тут уже появляются служебные биты для распознования того, что UTF-16 символ закодирван двумя парами:
		//В первой Левой двухбайтовой паре Слева(в первом байте) идут сначала 6 служебных бит:110110_xx xxxxxxxx
		//Во второй Правой двухбайтовой паре Слева(в первом байте) идут сначала 6 служебных бит:110111_xx xxxxxxxx
		//Итого получается так: [110110_xx xxxxxxxx] [110111_xx xxxxxxxx]   - где x - Это "рабочие" биты, которые и составляют самое Юникодно значение, НО БЛЯТЬ ЕСТЬ НЮАНС НАХОЙ!!!! На сцену выходят "Суррогатные мать их пары".

		//"Суррогтаные пары"    - Предположим, что нам нужно закодировать Юникодный символ под значением "55296" в UTF-16, так как значение меньше "65535" - то кодируется одной парой - простым присваиванием значения. В двочином виде получаем: [11011000 00000000]
		//[11011000 00000000]   - как видно данный символ Юникодный символ под значнием "55296" очень странным образом первыми 6-ю своими битами напомниает - служебные биты при двух парной кодировке UTF-16, а если так, то, как тогда отличать, реально закодированный одной парой символ "55296" от двух парной кодировки - если первые 6 бит у них совпадают ?? 
		//А ни как не отличать, для этого в Юникоде есть костыль-геморрой, состоящий в том, что ЮНИКОДНЫЙ СИМВОЛОВ СО ЗНАЧНИЕМ В ДИАПАЗОНЕ "55296 - 57343" НЕ СУЩЕСТВУЕТ!!! НИ ОНДНОМУ ЗНАЧЕНИЮ ИЗ ЭТОГО ДИАПАЗОНА НЕТ СООВТЕТВУЮЩЕГО ЮНИКОДНОГО СИМВОЛА, СПЕЦИАЛЬНО ДЛЯ ТОГО, ЧТОБЫ НЕ БЫЛО ВОТ ТАКОЙ ПРОБЛЕМЫ С ПЕРСЕЧЕНИЕМ РЕАЛНЫХ И СЛУЖЕБНЫХ БИТ ПРИ UTF-16 КОДИРОВКЕ.
		//Возникает вопрос, а НА***УЯ это вообще придумали, почему нельзя было сделать так, как в UTF-8 кодировки, то есть если символ Юникод умещается в первую пару UTF-16, то он бы кодировался первым нулевым служебным битом(как в однобоайтовой кодировке UTF-8), а потом бы уже шли рабочие биты. И если бы символ Юникода не умещался бы в первую пару, то тогда, первая парабы начиналась со служеюных бит "110", а во творой паре было бы продолжение в виде двух служебных бит "10" (как при двухбайтовой кодировании UTF-8) - КОРОЧЕ если я праильно понял, то из за того, что нужно было сохранить совместимость с ранеее сущестовашей еще до UTF-16 кодировки UCS-2, которая так же была строго двух байтной и кодировалась прямым способо без всяких служебных бит.

		//ОДНАКО это не все: то есть еще раз, все значения Юникода от 0 до 65535 (исключая не существующий сурогаттный диапазон) просто заносятся в первую пару без изменений и получатся кодировка UTF-16.
		//Значение выше 65535 (исключая не существующий сурогаттный диапазон):  для кодировки таких символов уже нужно кол-во рабочих бит от 17 бит до 21 бита, ОДНАКО смотрим сколько реально рабочих бит доступно при двухпарном кодировании - [110110_xx xxxxxxxx] [110111_xx xxxxxxxx] = 10 + 10 = 20. Доступно 20 бит!!! А для максимальных значений Юникода нужен 21 бит!!!  Че за фигня ?
		//При кодировании символа Юникод выше значения 65535 - Всегда из значения этого символа вычитается значение "65536": то есть если взять максимальное значние Юникодного символа в "1114111" вычесть из него 65536, то получим "1048575", что в двоичном виде займет ровно 20 бит - "11111111111111111111", которые уже раваномрно распределятся по двум парам UTF-16.
		//РАССМОТРИМ ПРИМЕР: Возьмем к примеру иероглиф "𠀅" со значением в Юникоде "131077", теперь по правилу вычитаем из него "65536": 131077 - 65536 = 65541.  
		//Смотрим как выглядет "65541" в двоичном виде: 10000000000000101  - теперь первые Левые 10 бит [10000000000]  берем и помещаем в первую пару за место X-ов: [110110_10 000000000].
		//После того, как мы взяли Левые 10 бит, у нас остаются 7 бит Справа [0000101] - к ним нужно доавбить Слева нули до 10 бит в этом случае это 3 нуля --> [0000000101] и помещаем их во вторую пару за место X-ов: [110111_00 00000101].
		//Все UTF-16 символ готов.
		//---------------------------------------------------------------------------------------------------------------------------------------


		//-------------------------------------------------------------------------------------------------
		//pointer_to_begin_UTF8_symv - должен быть указатель на первый байт UTF-8 символа.
		//string_UTF8_size_byte      - размер в символах whar_t, в пределах которого нужно проверить Один символ на который указывает "pointer_to_begin_UTF16_symv", может быть по сути любым, ТО ЕСТЬ, функция проверит является ли Указатель "pointer_to_begin_UTF16_symv" указатель на начальый Байт UTF-16 символа, и если является, то првоерит корректность остальных байт этого UTF-8 символа, если размер указанынй в "string_UTF8_size_byte" будет больше, чем общий размер в байтах UTF-8 символа, то он просто не будет ни как испльзоватся, а, если размер указанынй в "string_UTF8_size_byte" будет меньше, чем размер в байтах UTF-8 символа на который указывает указатель "pointer_to_begin_UTF8_symv" - то такой символ будет признан Некорректным, так как функции просто тупо не хватило данных для окончания анализа.
		//-------------------------------------------------------------------------------------------------



		if ((pointer_to_begin_UTF16_symv[0] & 0b1111110000000000) == 0b1101100000000000)
		{
			//Значит первый Левый байт этой Левой пары содержит служебные биты "110110" харакерные для символа UTF-16 закодированного двумя парами.
			//Теперь проверим содержит ли вторая пара, которая Справа от этой служеюные биты "110111":
			//-Если содержит, то это корректный UTF-16 символ состоязий из двух пар.
			//-Если не содержит, то значит это не UTF-16 символ, так как во второй Правой части нет обязательного наличия служебных бит "110111"


			//------------------------------------------------------------------------------------------
			if (pointer_Pair_size >= 2)
			{
				if ((pointer_to_begin_UTF16_symv[1] & 0b1111110000000000) == 0b1101110000000000)
				{
					return result_flag::OK;   //Возвращаем размер в символах wchar_t.
				}
				else
				{
					*pointer_to_IncorrectPair = &pointer_to_begin_UTF16_symv[1];
					error = "UTF16_incorrect";
					return result_flag::UTF16_incorrect; //Значит вторая пара имеют Не корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-16 символ. Надо лучше размеры указывать.
			}
			//------------------------------------------------------------------------------------------


		}
		else
		{
			if (pointer_Pair_size >= 1)
			{
				return result_flag::OK;  //Значит UTF16 символ состоит из одной пары wchar_t.
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-16 символ. Надо лучше размеры указывать.
			}

		}

	}

	template<typename T> inline const result_flag check_UTF16symvLE__SurrogatePair(const T* pointer_to_begin_UTF16_symv, const size_t pointer_Pair_size)  noexcept
	{

		//---------------------------------------------------------------------------------------------------------------------------------------
		//Про кодировку UTF-8 читать в "check__UTF8symv_and_return_ByteSize".
		//Про перевод символа UTF-8 в Юникодное значение читать в "convert__UTF8Symv_to_UnicodePointCode".

		//Кодировка UTF-16 - это кодировка переменной длинны, в которой используется либо одна двухбайтная пара, либо две двухбайтные пары.
		//Все значения Юникод от 0 до 65535 (за исключение значений зарезервированных для "суррогатных пар"(об этом ниже) - это (десятичное: 55296 - 57343, шестнадацтиричное:0xD800 – 0xDFFF)) - кодируются ровно первой двухбайтной парой прямым способом, то есть значение Юникод просто помещается в два байта без всяких служебных битов, в UTF-8 - это сооветтвует кодированию о 1 до 3 байт.
		//Все значения Юникод от 65536 до максимального значения Юникод в 1114111 - кодируются в UTF-16 уже двумя двухбайтными парами, в UTF-8 это соответвует кодровки в 4 байта. И вот тут уже появляются служебные биты для распознования того, что UTF-16 символ закодирван двумя парами:
		//В первой Левой двухбайтовой паре Слева(в первом байте) идут сначала 6 служебных бит:110110_xx xxxxxxxx
		//Во второй Правой двухбайтовой паре Слева(в первом байте) идут сначала 6 служебных бит:110111_xx xxxxxxxx
		//Итого получается так: [110110_xx xxxxxxxx] [110111_xx xxxxxxxx]   - где x - Это "рабочие" биты, которые и составляют самое Юникодно значение, НО БЛЯТЬ ЕСТЬ НЮАНС НАХОЙ!!!! На сцену выходят "Суррогатные мать их пары".

		//"Суррогтаные пары"    - Предположим, что нам нужно закодировать Юникодный символ под значением "55296" в UTF-16, так как значение меньше "65535" - то кодируется одной парой - простым присваиванием значения. В двочином виде получаем: [11011000 00000000]
		//[11011000 00000000]   - как видно данный символ Юникодный символ под значнием "55296" очень странным образом первыми 6-ю своими битами напомниает - служебные биты при двух парной кодировке UTF-16, а если так, то, как тогда отличать, реально закодированный одной парой символ "55296" от двух парной кодировки - если первые 6 бит у них совпадают ?? 
		//А ни как не отличать, для этого в Юникоде есть костыль-геморрой, состоящий в том, что ЮНИКОДНЫЙ СИМВОЛОВ СО ЗНАЧНИЕМ В ДИАПАЗОНЕ "55296 - 57343" НЕ СУЩЕСТВУЕТ!!! НИ ОНДНОМУ ЗНАЧЕНИЮ ИЗ ЭТОГО ДИАПАЗОНА НЕТ СООВТЕТВУЮЩЕГО ЮНИКОДНОГО СИМВОЛА, СПЕЦИАЛЬНО ДЛЯ ТОГО, ЧТОБЫ НЕ БЫЛО ВОТ ТАКОЙ ПРОБЛЕМЫ С ПЕРСЕЧЕНИЕМ РЕАЛНЫХ И СЛУЖЕБНЫХ БИТ ПРИ UTF-16 КОДИРОВКЕ.
		//Возникает вопрос, а НА***УЯ это вообще придумали, почему нельзя было сделать так, как в UTF-8 кодировки, то есть если символ Юникод умещается в первую пару UTF-16, то он бы кодировался первым нулевым служебным битом(как в однобоайтовой кодировке UTF-8), а потом бы уже шли рабочие биты. И если бы символ Юникода не умещался бы в первую пару, то тогда, первая парабы начиналась со служеюных бит "110", а во творой паре было бы продолжение в виде двух служебных бит "10" (как при двухбайтовой кодировании UTF-8) - КОРОЧЕ если я праильно понял, то из за того, что нужно было сохранить совместимость с ранеее сущестовашей еще до UTF-16 кодировки UCS-2, которая так же была строго двух байтной и кодировалась прямым способо без всяких служебных бит.

		//ОДНАКО это не все: то есть еще раз, все значения Юникода от 0 до 65535 (исключая не существующий сурогаттный диапазон) просто заносятся в первую пару без изменений и получатся кодировка UTF-16.
		//Значение выше 65535 (исключая не существующий сурогаттный диапазон):  для кодировки таких символов уже нужно кол-во рабочих бит от 17 бит до 21 бита, ОДНАКО смотрим сколько реально рабочих бит доступно при двухпарном кодировании - [110110_xx xxxxxxxx] [110111_xx xxxxxxxx] = 10 + 10 = 20. Доступно 20 бит!!! А для максимальных значений Юникода нужен 21 бит!!!  Че за фигня ?
		//При кодировании символа Юникод выше значения 65535 - Всегда из значения этого символа вычитается значение "65536": то есть если взять максимальное значние Юникодного символа в "1114111" вычесть из него 65536, то получим "1048575", что в двоичном виде займет ровно 20 бит - "11111111111111111111", которые уже раваномрно распределятся по двум парам UTF-16.
		//РАССМОТРИМ ПРИМЕР: Возьмем к примеру иероглиф "𠀅" со значением в Юникоде "131077", теперь по правилу вычитаем из него "65536": 131077 - 65536 = 65541.  
		//Смотрим как выглядет "65541" в двоичном виде: 10000000000000101  - теперь первые Левые 10 бит [10000000000]  берем и помещаем в первую пару за место X-ов: [110110_10 000000000].
		//После того, как мы взяли Левые 10 бит, у нас остаются 7 бит Справа [0000101] - к ним нужно доавбить Слева нули до 10 бит в этом случае это 3 нуля --> [0000000101] и помещаем их во вторую пару за место X-ов: [110111_00 00000101].
		//Все UTF-16 символ готов.
		//---------------------------------------------------------------------------------------------------------------------------------------


		//-------------------------------------------------------------------------------------------------
		//pointer_to_begin_UTF8_symv - должен быть указатель на первый байт UTF-8 символа.
		//string_UTF8_size_byte      - размер в символах whar_t, в пределах которого нужно проверить Один символ на который указывает "pointer_to_begin_UTF16_symv", может быть по сути любым, ТО ЕСТЬ, функция проверит является ли Указатель "pointer_to_begin_UTF16_symv" указатель на начальый Байт UTF-16 символа, и если является, то првоерит корректность остальных байт этого UTF-8 символа, если размер указанынй в "string_UTF8_size_byte" будет больше, чем общий размер в байтах UTF-8 символа, то он просто не будет ни как испльзоватся, а, если размер указанынй в "string_UTF8_size_byte" будет меньше, чем размер в байтах UTF-8 символа на который указывает указатель "pointer_to_begin_UTF8_symv" - то такой символ будет признан Некорректным, так как функции просто тупо не хватило данных для окончания анализа.
		//-------------------------------------------------------------------------------------------------



		if ((pointer_to_begin_UTF16_symv[0] & 0b1111110000000000) == 0b1101100000000000)
		{
			//Значит первый Левый байт этой Левой пары содержит служебные биты "110110" харакерные для символа UTF-16 закодированного двумя парами.
			//Теперь проверим содержит ли вторая пара, которая Справа от этой служеюные биты "110111":
			//-Если содержит, то это корректный UTF-16 символ состоязий из двух пар.
			//-Если не содержит, то значит это не UTF-16 символ, так как во второй Правой части нет обязательного наличия служебных бит "110111"


			//------------------------------------------------------------------------------------------
			if (pointer_Pair_size >= 2)
			{
				if ((pointer_to_begin_UTF16_symv[1] & 0b1111110000000000) == 0b1101110000000000)
				{

					//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
					const int_least32_t UnicodePointCode = convert__UTF16SymvLE_to_UnicodePointCode<T>(pointer_to_begin_UTF16_symv, 2);

					if (UnicodePointCode > 1114111)     //Если больше максимального значения для Юникодного символа
					{
						error = "Greater_than_MaxUnicodeValue";
						return result_flag::Greater_than_MaxUnicodeValue;
					}
					//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


					return result_flag::OK;
				}
				else
				{
					error = "UTF16_incorrect";
					return result_flag::UTF16_incorrect; //Значит вторая пара имеют Не корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-16 символ. Надо лучше размеры указывать.
			}
			//------------------------------------------------------------------------------------------


		}
		else
		{
			if (pointer_Pair_size >= 1)
			{

				//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
				const int UnicodePointCode = convert__UTF16SymvLE_to_UnicodePointCode<T>(pointer_to_begin_UTF16_symv, 1);

				if (UnicodePointCode >= 55296 && UnicodePointCode <= 57343)     //(0xD800 – 0xDFFF)
				{
					error = "IncludeSurrogatePairValue";
					return result_flag::IncludeSurrogatePairValue;    //Данный условно корреткный UTF-8 символ входит в Юникодное значение "суррогатной пары" - для которого в Юникодной таблицы - нет никаких символов. А значит такого Юникодного символа закодированного в UTF-8 или в какой либо другой кодировке быть не должно.
				}
				//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


				return result_flag::OK;  //Значит UTF16 символ состоит из одной пары wchar_t.
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-16 символ. Надо лучше размеры указывать.
			}

		}

	}

	template<typename T> inline const result_flag check_UTF16symvLE__SurrogatePair(const T* pointer_to_begin_UTF16_symv, const size_t pointer_Pair_size, const T** pointer_to_IncorrectPair)  noexcept
	{

		//---------------------------------------------------------------------------------------------------------------------------------------
		//Про кодировку UTF-8 читать в "check__UTF8symv_and_return_ByteSize".
		//Про перевод символа UTF-8 в Юникодное значение читать в "convert__UTF8Symv_to_UnicodePointCode".

		//Кодировка UTF-16 - это кодировка переменной длинны, в которой используется либо одна двухбайтная пара, либо две двухбайтные пары.
		//Все значения Юникод от 0 до 65535 (за исключение значений зарезервированных для "суррогатных пар"(об этом ниже) - это (десятичное: 55296 - 57343, шестнадацтиричное:0xD800 – 0xDFFF)) - кодируются ровно первой двухбайтной парой прямым способом, то есть значение Юникод просто помещается в два байта без всяких служебных битов, в UTF-8 - это сооветтвует кодированию о 1 до 3 байт.
		//Все значения Юникод от 65536 до максимального значения Юникод в 1114111 - кодируются в UTF-16 уже двумя двухбайтными парами, в UTF-8 это соответвует кодровки в 4 байта. И вот тут уже появляются служебные биты для распознования того, что UTF-16 символ закодирван двумя парами:
		//В первой Левой двухбайтовой паре Слева(в первом байте) идут сначала 6 служебных бит:110110_xx xxxxxxxx
		//Во второй Правой двухбайтовой паре Слева(в первом байте) идут сначала 6 служебных бит:110111_xx xxxxxxxx
		//Итого получается так: [110110_xx xxxxxxxx] [110111_xx xxxxxxxx]   - где x - Это "рабочие" биты, которые и составляют самое Юникодно значение, НО БЛЯТЬ ЕСТЬ НЮАНС НАХОЙ!!!! На сцену выходят "Суррогатные мать их пары".

		//"Суррогтаные пары"    - Предположим, что нам нужно закодировать Юникодный символ под значением "55296" в UTF-16, так как значение меньше "65535" - то кодируется одной парой - простым присваиванием значения. В двочином виде получаем: [11011000 00000000]
		//[11011000 00000000]   - как видно данный символ Юникодный символ под значнием "55296" очень странным образом первыми 6-ю своими битами напомниает - служебные биты при двух парной кодировке UTF-16, а если так, то, как тогда отличать, реально закодированный одной парой символ "55296" от двух парной кодировки - если первые 6 бит у них совпадают ?? 
		//А ни как не отличать, для этого в Юникоде есть костыль-геморрой, состоящий в том, что ЮНИКОДНЫЙ СИМВОЛОВ СО ЗНАЧНИЕМ В ДИАПАЗОНЕ "55296 - 57343" НЕ СУЩЕСТВУЕТ!!! НИ ОНДНОМУ ЗНАЧЕНИЮ ИЗ ЭТОГО ДИАПАЗОНА НЕТ СООВТЕТВУЮЩЕГО ЮНИКОДНОГО СИМВОЛА, СПЕЦИАЛЬНО ДЛЯ ТОГО, ЧТОБЫ НЕ БЫЛО ВОТ ТАКОЙ ПРОБЛЕМЫ С ПЕРСЕЧЕНИЕМ РЕАЛНЫХ И СЛУЖЕБНЫХ БИТ ПРИ UTF-16 КОДИРОВКЕ.
		//Возникает вопрос, а НА***УЯ это вообще придумали, почему нельзя было сделать так, как в UTF-8 кодировки, то есть если символ Юникод умещается в первую пару UTF-16, то он бы кодировался первым нулевым служебным битом(как в однобоайтовой кодировке UTF-8), а потом бы уже шли рабочие биты. И если бы символ Юникода не умещался бы в первую пару, то тогда, первая парабы начиналась со служеюных бит "110", а во творой паре было бы продолжение в виде двух служебных бит "10" (как при двухбайтовой кодировании UTF-8) - КОРОЧЕ если я праильно понял, то из за того, что нужно было сохранить совместимость с ранеее сущестовашей еще до UTF-16 кодировки UCS-2, которая так же была строго двух байтной и кодировалась прямым способо без всяких служебных бит.

		//ОДНАКО это не все: то есть еще раз, все значения Юникода от 0 до 65535 (исключая не существующий сурогаттный диапазон) просто заносятся в первую пару без изменений и получатся кодировка UTF-16.
		//Значение выше 65535 (исключая не существующий сурогаттный диапазон):  для кодировки таких символов уже нужно кол-во рабочих бит от 17 бит до 21 бита, ОДНАКО смотрим сколько реально рабочих бит доступно при двухпарном кодировании - [110110_xx xxxxxxxx] [110111_xx xxxxxxxx] = 10 + 10 = 20. Доступно 20 бит!!! А для максимальных значений Юникода нужен 21 бит!!!  Че за фигня ?
		//При кодировании символа Юникод выше значения 65535 - Всегда из значения этого символа вычитается значение "65536": то есть если взять максимальное значние Юникодного символа в "1114111" вычесть из него 65536, то получим "1048575", что в двоичном виде займет ровно 20 бит - "11111111111111111111", которые уже раваномрно распределятся по двум парам UTF-16.
		//РАССМОТРИМ ПРИМЕР: Возьмем к примеру иероглиф "𠀅" со значением в Юникоде "131077", теперь по правилу вычитаем из него "65536": 131077 - 65536 = 65541.  
		//Смотрим как выглядет "65541" в двоичном виде: 10000000000000101  - теперь первые Левые 10 бит [10000000000]  берем и помещаем в первую пару за место X-ов: [110110_10 000000000].
		//После того, как мы взяли Левые 10 бит, у нас остаются 7 бит Справа [0000101] - к ним нужно доавбить Слева нули до 10 бит в этом случае это 3 нуля --> [0000000101] и помещаем их во вторую пару за место X-ов: [110111_00 00000101].
		//Все UTF-16 символ готов.
		//---------------------------------------------------------------------------------------------------------------------------------------


		//-------------------------------------------------------------------------------------------------
		//pointer_to_begin_UTF8_symv - должен быть указатель на первый байт UTF-8 символа.
		//string_UTF8_size_byte      - размер в символах whar_t, в пределах которого нужно проверить Один символ на который указывает "pointer_to_begin_UTF16_symv", может быть по сути любым, ТО ЕСТЬ, функция проверит является ли Указатель "pointer_to_begin_UTF16_symv" указатель на начальый Байт UTF-16 символа, и если является, то првоерит корректность остальных байт этого UTF-8 символа, если размер указанынй в "string_UTF8_size_byte" будет больше, чем общий размер в байтах UTF-8 символа, то он просто не будет ни как испльзоватся, а, если размер указанынй в "string_UTF8_size_byte" будет меньше, чем размер в байтах UTF-8 символа на который указывает указатель "pointer_to_begin_UTF8_symv" - то такой символ будет признан Некорректным, так как функции просто тупо не хватило данных для окончания анализа.
		//-------------------------------------------------------------------------------------------------



		if ((pointer_to_begin_UTF16_symv[0] & 0b1111110000000000) == 0b1101100000000000)
		{
			//Значит первый Левый байт этой Левой пары содержит служебные биты "110110" харакерные для символа UTF-16 закодированного двумя парами.
			//Теперь проверим содержит ли вторая пара, которая Справа от этой служеюные биты "110111":
			//-Если содержит, то это корректный UTF-16 символ состоязий из двух пар.
			//-Если не содержит, то значит это не UTF-16 символ, так как во второй Правой части нет обязательного наличия служебных бит "110111"


			//------------------------------------------------------------------------------------------
			if (pointer_Pair_size >= 2)
			{
				if ((pointer_to_begin_UTF16_symv[1] & 0b1111110000000000) == 0b1101110000000000)
				{

					//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
					const int_least32_t UnicodePointCode = convert__UTF16SymvLE_to_UnicodePointCode<T>(pointer_to_begin_UTF16_symv, 2);

					if (UnicodePointCode > 1114111)     //Если больше максимального значения для Юникодного символа
					{
						error = "Greater_than_MaxUnicodeValue";
						return result_flag::Greater_than_MaxUnicodeValue;
					}
					//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


					return result_flag::OK;   //Возвращаем размер в символах wchar_t.
				}
				else
				{
					*pointer_to_IncorrectPair = &pointer_to_begin_UTF16_symv[1];
					error = "UTF16_incorrect";
					return result_flag::UTF16_incorrect; //Значит вторая пара имеют Не корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-16 символ. Надо лучше размеры указывать.
			}
			//------------------------------------------------------------------------------------------


		}
		else
		{
			if (pointer_Pair_size >= 1)
			{

				//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
				const int_least32_t UnicodePointCode = convert__UTF16SymvLE_to_UnicodePointCode<T>(pointer_to_begin_UTF16_symv, 1);

				if (UnicodePointCode >= 55296 && UnicodePointCode <= 57343)     //(0xD800 – 0xDFFF)
				{
					error = "IncludeSurrogatePairValue";
					return result_flag::IncludeSurrogatePairValue;    //Данный условно корреткный UTF-8 символ входит в Юникодное значение "суррогатной пары" - для которого в Юникодной таблицы - нет никаких символов. А значит такого Юникодного символа закодированного в UTF-8 или в какой либо другой кодировке быть не должно.
				}
				//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


				return result_flag::OK;  //Значит UTF16 символ состоит из одной пары wchar_t.
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-16 символ. Надо лучше размеры указывать.
			}

		}

	}


	template<typename T> inline const result_flag check_UTF16symvLE_and_return_PairSize_(const T* pointer_to_begin_UTF16_symv, const size_t pointer_Pair_size, int& UTF16_PairSize)  noexcept
	{

		//---------------------------------------------------------------------------------------------------------------------------------------
		//Про кодировку UTF-8 читать в "check__UTF8symv_and_return_ByteSize".
		//Про перевод символа UTF-8 в Юникодное значение читать в "convert__UTF8Symv_to_UnicodePointCode".

		//Кодировка UTF-16 - это кодировка переменной длинны, в которой используется либо одна двухбайтная пара, либо две двухбайтные пары.
		//Все значения Юникод от 0 до 65535 (за исключение значений зарезервированных для "суррогатных пар"(об этом ниже) - это (десятичное: 55296 - 57343, шестнадацтиричное:0xD800 – 0xDFFF)) - кодируются ровно первой двухбайтной парой прямым способом, то есть значение Юникод просто помещается в два байта без всяких служебных битов, в UTF-8 - это сооветтвует кодированию о 1 до 3 байт.
		//Все значения Юникод от 65536 до максимального значения Юникод в 1114111 - кодируются в UTF-16 уже двумя двухбайтными парами, в UTF-8 это соответвует кодровки в 4 байта. И вот тут уже появляются служебные биты для распознования того, что UTF-16 символ закодирван двумя парами:
		//В первой Левой двухбайтовой паре Слева(в первом байте) идут сначала 6 служебных бит:110110_xx xxxxxxxx
		//Во второй Правой двухбайтовой паре Слева(в первом байте) идут сначала 6 служебных бит:110111_xx xxxxxxxx
		//Итого получается так: [110110_xx xxxxxxxx] [110111_xx xxxxxxxx]   - где x - Это "рабочие" биты, которые и составляют самое Юникодно значение, НО БЛЯТЬ ЕСТЬ НЮАНС НАХОЙ!!!! На сцену выходят "Суррогатные мать их пары".

		//"Суррогтаные пары"    - Предположим, что нам нужно закодировать Юникодный символ под значением "55296" в UTF-16, так как значение меньше "65535" - то кодируется одной парой - простым присваиванием значения. В двочином виде получаем: [11011000 00000000]
		//[11011000 00000000]   - как видно данный символ Юникодный символ под значнием "55296" очень странным образом первыми 6-ю своими битами напомниает - служебные биты при двух парной кодировке UTF-16, а если так, то, как тогда отличать, реально закодированный одной парой символ "55296" от двух парной кодировки - если первые 6 бит у них совпадают ?? 
		//А ни как не отличать, для этого в Юникоде есть костыль-геморрой, состоящий в том, что ЮНИКОДНЫЙ СИМВОЛОВ СО ЗНАЧНИЕМ В ДИАПАЗОНЕ "55296 - 57343" НЕ СУЩЕСТВУЕТ!!! НИ ОНДНОМУ ЗНАЧЕНИЮ ИЗ ЭТОГО ДИАПАЗОНА НЕТ СООВТЕТВУЮЩЕГО ЮНИКОДНОГО СИМВОЛА, СПЕЦИАЛЬНО ДЛЯ ТОГО, ЧТОБЫ НЕ БЫЛО ВОТ ТАКОЙ ПРОБЛЕМЫ С ПЕРСЕЧЕНИЕМ РЕАЛНЫХ И СЛУЖЕБНЫХ БИТ ПРИ UTF-16 КОДИРОВКЕ.
		//Возникает вопрос, а НА***УЯ это вообще придумали, почему нельзя было сделать так, как в UTF-8 кодировки, то есть если символ Юникод умещается в первую пару UTF-16, то он бы кодировался первым нулевым служебным битом(как в однобоайтовой кодировке UTF-8), а потом бы уже шли рабочие биты. И если бы символ Юникода не умещался бы в первую пару, то тогда, первая парабы начиналась со служеюных бит "110", а во творой паре было бы продолжение в виде двух служебных бит "10" (как при двухбайтовой кодировании UTF-8) - КОРОЧЕ если я праильно понял, то из за того, что нужно было сохранить совместимость с ранеее сущестовашей еще до UTF-16 кодировки UCS-2, которая так же была строго двух байтной и кодировалась прямым способо без всяких служебных бит.

		//ОДНАКО это не все: то есть еще раз, все значения Юникода от 0 до 65535 (исключая не существующий сурогаттный диапазон) просто заносятся в первую пару без изменений и получатся кодировка UTF-16.
		//Значение выше 65535 (исключая не существующий сурогаттный диапазон):  для кодировки таких символов уже нужно кол-во рабочих бит от 17 бит до 21 бита, ОДНАКО смотрим сколько реально рабочих бит доступно при двухпарном кодировании - [110110_xx xxxxxxxx] [110111_xx xxxxxxxx] = 10 + 10 = 20. Доступно 20 бит!!! А для максимальных значений Юникода нужен 21 бит!!!  Че за фигня ?
		//При кодировании символа Юникод выше значения 65535 - Всегда из значения этого символа вычитается значение "65536": то есть если взять максимальное значние Юникодного символа в "1114111" вычесть из него 65536, то получим "1048575", что в двоичном виде займет ровно 20 бит - "11111111111111111111", которые уже раваномрно распределятся по двум парам UTF-16.
		//РАССМОТРИМ ПРИМЕР: Возьмем к примеру иероглиф "𠀅" со значением в Юникоде "131077", теперь по правилу вычитаем из него "65536": 131077 - 65536 = 65541.  
		//Смотрим как выглядет "65541" в двоичном виде: 10000000000000101  - теперь первые Левые 10 бит [10000000000]  берем и помещаем в первую пару за место X-ов: [110110_10 000000000].
		//После того, как мы взяли Левые 10 бит, у нас остаются 7 бит Справа [0000101] - к ним нужно доавбить Слева нули до 10 бит в этом случае это 3 нуля --> [0000000101] и помещаем их во вторую пару за место X-ов: [110111_00 00000101].
		//Все UTF-16 символ готов.
		//---------------------------------------------------------------------------------------------------------------------------------------


		//-------------------------------------------------------------------------------------------------
		//pointer_to_begin_UTF8_symv - должен быть указатель на первый байт UTF-8 символа.
		//string_UTF8_size_byte      - размер в символах whar_t, в пределах которого нужно проверить Один символ на который указывает "pointer_to_begin_UTF16_symv", может быть по сути любым, ТО ЕСТЬ, функция проверит является ли Указатель "pointer_to_begin_UTF16_symv" указатель на начальый Байт UTF-16 символа, и если является, то првоерит корректность остальных байт этого UTF-8 символа, если размер указанынй в "string_UTF8_size_byte" будет больше, чем общий размер в байтах UTF-8 символа, то он просто не будет ни как испльзоватся, а, если размер указанынй в "string_UTF8_size_byte" будет меньше, чем размер в байтах UTF-8 символа на который указывает указатель "pointer_to_begin_UTF8_symv" - то такой символ будет признан Некорректным, так как функции просто тупо не хватило данных для окончания анализа.
		//-------------------------------------------------------------------------------------------------



		if ((pointer_to_begin_UTF16_symv[0] & 0b1111110000000000) == 0b1101100000000000)
		{
			//Значит первый Левый байт этой Левой пары содержит служебные биты "110110" харакерные для символа UTF-16 закодированного двумя парами.
			//Теперь проверим содержит ли вторая пара, которая Справа от этой служеюные биты "110111":
			//-Если содержит, то это корректный UTF-16 символ состоязий из двух пар.
			//-Если не содержит, то значит это не UTF-16 символ, так как во второй Правой части нет обязательного наличия служебных бит "110111"


			//------------------------------------------------------------------------------------------
			if (pointer_Pair_size >= 2)
			{
				if ((pointer_to_begin_UTF16_symv[1] & 0b1111110000000000) == 0b1101110000000000)
				{
					UTF16_PairSize = 2;
					return result_flag::OK;   //Возвращаем размер в символах wchar_t.
				}
				else
				{
					error = "UTF16_incorrect";
					return result_flag::UTF16_incorrect; //Значит вторая пара имеют Не корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-16 символ. Надо лучше размеры указывать.
			}
			//------------------------------------------------------------------------------------------


		}
		else
		{
			if (pointer_Pair_size >= 1)
			{
				UTF16_PairSize = 1;
				return result_flag::OK;  //Значит UTF16 символ состоит из одной пары wchar_t.
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-16 символ. Надо лучше размеры указывать.
			}

		}

	}

	template<typename T> inline const result_flag check_UTF16symvLE_and_return_PairSize_(const T* pointer_to_begin_UTF16_symv, const size_t pointer_Pair_size, const T** pointer_to_IncorrectPair, int& UTF16_PairSize)  noexcept
	{

		//---------------------------------------------------------------------------------------------------------------------------------------
		//Про кодировку UTF-8 читать в "check__UTF8symv_and_return_ByteSize".
		//Про перевод символа UTF-8 в Юникодное значение читать в "convert__UTF8Symv_to_UnicodePointCode".

		//Кодировка UTF-16 - это кодировка переменной длинны, в которой используется либо одна двухбайтная пара, либо две двухбайтные пары.
		//Все значения Юникод от 0 до 65535 (за исключение значений зарезервированных для "суррогатных пар"(об этом ниже) - это (десятичное: 55296 - 57343, шестнадацтиричное:0xD800 – 0xDFFF)) - кодируются ровно первой двухбайтной парой прямым способом, то есть значение Юникод просто помещается в два байта без всяких служебных битов, в UTF-8 - это сооветтвует кодированию о 1 до 3 байт.
		//Все значения Юникод от 65536 до максимального значения Юникод в 1114111 - кодируются в UTF-16 уже двумя двухбайтными парами, в UTF-8 это соответвует кодровки в 4 байта. И вот тут уже появляются служебные биты для распознования того, что UTF-16 символ закодирван двумя парами:
		//В первой Левой двухбайтовой паре Слева(в первом байте) идут сначала 6 служебных бит:110110_xx xxxxxxxx
		//Во второй Правой двухбайтовой паре Слева(в первом байте) идут сначала 6 служебных бит:110111_xx xxxxxxxx
		//Итого получается так: [110110_xx xxxxxxxx] [110111_xx xxxxxxxx]   - где x - Это "рабочие" биты, которые и составляют самое Юникодно значение, НО БЛЯТЬ ЕСТЬ НЮАНС НАХОЙ!!!! На сцену выходят "Суррогатные мать их пары".

		//"Суррогтаные пары"    - Предположим, что нам нужно закодировать Юникодный символ под значением "55296" в UTF-16, так как значение меньше "65535" - то кодируется одной парой - простым присваиванием значения. В двочином виде получаем: [11011000 00000000]
		//[11011000 00000000]   - как видно данный символ Юникодный символ под значнием "55296" очень странным образом первыми 6-ю своими битами напомниает - служебные биты при двух парной кодировке UTF-16, а если так, то, как тогда отличать, реально закодированный одной парой символ "55296" от двух парной кодировки - если первые 6 бит у них совпадают ?? 
		//А ни как не отличать, для этого в Юникоде есть костыль-геморрой, состоящий в том, что ЮНИКОДНЫЙ СИМВОЛОВ СО ЗНАЧНИЕМ В ДИАПАЗОНЕ "55296 - 57343" НЕ СУЩЕСТВУЕТ!!! НИ ОНДНОМУ ЗНАЧЕНИЮ ИЗ ЭТОГО ДИАПАЗОНА НЕТ СООВТЕТВУЮЩЕГО ЮНИКОДНОГО СИМВОЛА, СПЕЦИАЛЬНО ДЛЯ ТОГО, ЧТОБЫ НЕ БЫЛО ВОТ ТАКОЙ ПРОБЛЕМЫ С ПЕРСЕЧЕНИЕМ РЕАЛНЫХ И СЛУЖЕБНЫХ БИТ ПРИ UTF-16 КОДИРОВКЕ.
		//Возникает вопрос, а НА***УЯ это вообще придумали, почему нельзя было сделать так, как в UTF-8 кодировки, то есть если символ Юникод умещается в первую пару UTF-16, то он бы кодировался первым нулевым служебным битом(как в однобоайтовой кодировке UTF-8), а потом бы уже шли рабочие биты. И если бы символ Юникода не умещался бы в первую пару, то тогда, первая парабы начиналась со служеюных бит "110", а во творой паре было бы продолжение в виде двух служебных бит "10" (как при двухбайтовой кодировании UTF-8) - КОРОЧЕ если я праильно понял, то из за того, что нужно было сохранить совместимость с ранеее сущестовашей еще до UTF-16 кодировки UCS-2, которая так же была строго двух байтной и кодировалась прямым способо без всяких служебных бит.

		//ОДНАКО это не все: то есть еще раз, все значения Юникода от 0 до 65535 (исключая не существующий сурогаттный диапазон) просто заносятся в первую пару без изменений и получатся кодировка UTF-16.
		//Значение выше 65535 (исключая не существующий сурогаттный диапазон):  для кодировки таких символов уже нужно кол-во рабочих бит от 17 бит до 21 бита, ОДНАКО смотрим сколько реально рабочих бит доступно при двухпарном кодировании - [110110_xx xxxxxxxx] [110111_xx xxxxxxxx] = 10 + 10 = 20. Доступно 20 бит!!! А для максимальных значений Юникода нужен 21 бит!!!  Че за фигня ?
		//При кодировании символа Юникод выше значения 65535 - Всегда из значения этого символа вычитается значение "65536": то есть если взять максимальное значние Юникодного символа в "1114111" вычесть из него 65536, то получим "1048575", что в двоичном виде займет ровно 20 бит - "11111111111111111111", которые уже раваномрно распределятся по двум парам UTF-16.
		//РАССМОТРИМ ПРИМЕР: Возьмем к примеру иероглиф "𠀅" со значением в Юникоде "131077", теперь по правилу вычитаем из него "65536": 131077 - 65536 = 65541.  
		//Смотрим как выглядет "65541" в двоичном виде: 10000000000000101  - теперь первые Левые 10 бит [10000000000]  берем и помещаем в первую пару за место X-ов: [110110_10 000000000].
		//После того, как мы взяли Левые 10 бит, у нас остаются 7 бит Справа [0000101] - к ним нужно доавбить Слева нули до 10 бит в этом случае это 3 нуля --> [0000000101] и помещаем их во вторую пару за место X-ов: [110111_00 00000101].
		//Все UTF-16 символ готов.
		//---------------------------------------------------------------------------------------------------------------------------------------


		//-------------------------------------------------------------------------------------------------
		//pointer_to_begin_UTF8_symv - должен быть указатель на первый байт UTF-8 символа.
		//string_UTF8_size_byte      - размер в символах whar_t, в пределах которого нужно проверить Один символ на который указывает "pointer_to_begin_UTF16_symv", может быть по сути любым, ТО ЕСТЬ, функция проверит является ли Указатель "pointer_to_begin_UTF16_symv" указатель на начальый Байт UTF-16 символа, и если является, то првоерит корректность остальных байт этого UTF-8 символа, если размер указанынй в "string_UTF8_size_byte" будет больше, чем общий размер в байтах UTF-8 символа, то он просто не будет ни как испльзоватся, а, если размер указанынй в "string_UTF8_size_byte" будет меньше, чем размер в байтах UTF-8 символа на который указывает указатель "pointer_to_begin_UTF8_symv" - то такой символ будет признан Некорректным, так как функции просто тупо не хватило данных для окончания анализа.
		//-------------------------------------------------------------------------------------------------



		if ((pointer_to_begin_UTF16_symv[0] & 0b1111110000000000) == 0b1101100000000000)
		{
			//Значит первый Левый байт этой Левой пары содержит служебные биты "110110" харакерные для символа UTF-16 закодированного двумя парами.
			//Теперь проверим содержит ли вторая пара, которая Справа от этой служеюные биты "110111":
			//-Если содержит, то это корректный UTF-16 символ состоязий из двух пар.
			//-Если не содержит, то значит это не UTF-16 символ, так как во второй Правой части нет обязательного наличия служебных бит "110111"


			//------------------------------------------------------------------------------------------
			if (pointer_Pair_size >= 2)
			{
				if ((pointer_to_begin_UTF16_symv[1] & 0b1111110000000000) == 0b1101110000000000)
				{
					UTF16_PairSize = 2;
					return result_flag::OK;   //Возвращаем размер в символах wchar_t.
				}
				else
				{
					*pointer_to_IncorrectPair = &pointer_to_begin_UTF16_symv[1];
					error = "UTF16_incorrect";
					return result_flag::UTF16_incorrect; //Значит вторая пара имеют Не корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-16 символ. Надо лучше размеры указывать.
			}
			//------------------------------------------------------------------------------------------


		}
		else
		{
			if (pointer_Pair_size >= 1)
			{
				UTF16_PairSize = 1;
				return result_flag::OK;  //Значит UTF16 символ состоит из одной пары wchar_t.
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-16 символ. Надо лучше размеры указывать.
			}

		}

	}

	template<typename T> inline const result_flag check_UTF16symvLE__SurrogatePair_and_return_PairSize_(const T* pointer_to_begin_UTF16_symv, const size_t pointer_Pair_size, int& UTF16_PairSize)  noexcept
	{

		//---------------------------------------------------------------------------------------------------------------------------------------
		//Про кодировку UTF-8 читать в "check__UTF8symv_and_return_ByteSize".
		//Про перевод символа UTF-8 в Юникодное значение читать в "convert__UTF8Symv_to_UnicodePointCode".

		//Кодировка UTF-16 - это кодировка переменной длинны, в которой используется либо одна двухбайтная пара, либо две двухбайтные пары.
		//Все значения Юникод от 0 до 65535 (за исключение значений зарезервированных для "суррогатных пар"(об этом ниже) - это (десятичное: 55296 - 57343, шестнадацтиричное:0xD800 – 0xDFFF)) - кодируются ровно первой двухбайтной парой прямым способом, то есть значение Юникод просто помещается в два байта без всяких служебных битов, в UTF-8 - это сооветтвует кодированию о 1 до 3 байт.
		//Все значения Юникод от 65536 до максимального значения Юникод в 1114111 - кодируются в UTF-16 уже двумя двухбайтными парами, в UTF-8 это соответвует кодровки в 4 байта. И вот тут уже появляются служебные биты для распознования того, что UTF-16 символ закодирван двумя парами:
		//В первой Левой двухбайтовой паре Слева(в первом байте) идут сначала 6 служебных бит:110110_xx xxxxxxxx
		//Во второй Правой двухбайтовой паре Слева(в первом байте) идут сначала 6 служебных бит:110111_xx xxxxxxxx
		//Итого получается так: [110110_xx xxxxxxxx] [110111_xx xxxxxxxx]   - где x - Это "рабочие" биты, которые и составляют самое Юникодно значение, НО БЛЯТЬ ЕСТЬ НЮАНС НАХОЙ!!!! На сцену выходят "Суррогатные мать их пары".

		//"Суррогтаные пары"    - Предположим, что нам нужно закодировать Юникодный символ под значением "55296" в UTF-16, так как значение меньше "65535" - то кодируется одной парой - простым присваиванием значения. В двочином виде получаем: [11011000 00000000]
		//[11011000 00000000]   - как видно данный символ Юникодный символ под значнием "55296" очень странным образом первыми 6-ю своими битами напомниает - служебные биты при двух парной кодировке UTF-16, а если так, то, как тогда отличать, реально закодированный одной парой символ "55296" от двух парной кодировки - если первые 6 бит у них совпадают ?? 
		//А ни как не отличать, для этого в Юникоде есть костыль-геморрой, состоящий в том, что ЮНИКОДНЫЙ СИМВОЛОВ СО ЗНАЧНИЕМ В ДИАПАЗОНЕ "55296 - 57343" НЕ СУЩЕСТВУЕТ!!! НИ ОНДНОМУ ЗНАЧЕНИЮ ИЗ ЭТОГО ДИАПАЗОНА НЕТ СООВТЕТВУЮЩЕГО ЮНИКОДНОГО СИМВОЛА, СПЕЦИАЛЬНО ДЛЯ ТОГО, ЧТОБЫ НЕ БЫЛО ВОТ ТАКОЙ ПРОБЛЕМЫ С ПЕРСЕЧЕНИЕМ РЕАЛНЫХ И СЛУЖЕБНЫХ БИТ ПРИ UTF-16 КОДИРОВКЕ.
		//Возникает вопрос, а НА***УЯ это вообще придумали, почему нельзя было сделать так, как в UTF-8 кодировки, то есть если символ Юникод умещается в первую пару UTF-16, то он бы кодировался первым нулевым служебным битом(как в однобоайтовой кодировке UTF-8), а потом бы уже шли рабочие биты. И если бы символ Юникода не умещался бы в первую пару, то тогда, первая парабы начиналась со служеюных бит "110", а во творой паре было бы продолжение в виде двух служебных бит "10" (как при двухбайтовой кодировании UTF-8) - КОРОЧЕ если я праильно понял, то из за того, что нужно было сохранить совместимость с ранеее сущестовашей еще до UTF-16 кодировки UCS-2, которая так же была строго двух байтной и кодировалась прямым способо без всяких служебных бит.

		//ОДНАКО это не все: то есть еще раз, все значения Юникода от 0 до 65535 (исключая не существующий сурогаттный диапазон) просто заносятся в первую пару без изменений и получатся кодировка UTF-16.
		//Значение выше 65535 (исключая не существующий сурогаттный диапазон):  для кодировки таких символов уже нужно кол-во рабочих бит от 17 бит до 21 бита, ОДНАКО смотрим сколько реально рабочих бит доступно при двухпарном кодировании - [110110_xx xxxxxxxx] [110111_xx xxxxxxxx] = 10 + 10 = 20. Доступно 20 бит!!! А для максимальных значений Юникода нужен 21 бит!!!  Че за фигня ?
		//При кодировании символа Юникод выше значения 65535 - Всегда из значения этого символа вычитается значение "65536": то есть если взять максимальное значние Юникодного символа в "1114111" вычесть из него 65536, то получим "1048575", что в двоичном виде займет ровно 20 бит - "11111111111111111111", которые уже раваномрно распределятся по двум парам UTF-16.
		//РАССМОТРИМ ПРИМЕР: Возьмем к примеру иероглиф "𠀅" со значением в Юникоде "131077", теперь по правилу вычитаем из него "65536": 131077 - 65536 = 65541.  
		//Смотрим как выглядет "65541" в двоичном виде: 10000000000000101  - теперь первые Левые 10 бит [10000000000]  берем и помещаем в первую пару за место X-ов: [110110_10 000000000].
		//После того, как мы взяли Левые 10 бит, у нас остаются 7 бит Справа [0000101] - к ним нужно доавбить Слева нули до 10 бит в этом случае это 3 нуля --> [0000000101] и помещаем их во вторую пару за место X-ов: [110111_00 00000101].
		//Все UTF-16 символ готов.
		//---------------------------------------------------------------------------------------------------------------------------------------


		//-------------------------------------------------------------------------------------------------
		//pointer_to_begin_UTF8_symv - должен быть указатель на первый байт UTF-8 символа.
		//string_UTF8_size_byte      - размер в символах whar_t, в пределах которого нужно проверить Один символ на который указывает "pointer_to_begin_UTF16_symv", может быть по сути любым, ТО ЕСТЬ, функция проверит является ли Указатель "pointer_to_begin_UTF16_symv" указатель на начальый Байт UTF-16 символа, и если является, то првоерит корректность остальных байт этого UTF-8 символа, если размер указанынй в "string_UTF8_size_byte" будет больше, чем общий размер в байтах UTF-8 символа, то он просто не будет ни как испльзоватся, а, если размер указанынй в "string_UTF8_size_byte" будет меньше, чем размер в байтах UTF-8 символа на который указывает указатель "pointer_to_begin_UTF8_symv" - то такой символ будет признан Некорректным, так как функции просто тупо не хватило данных для окончания анализа.
		//-------------------------------------------------------------------------------------------------



		if ((pointer_to_begin_UTF16_symv[0] & 0b1111110000000000) == 0b1101100000000000)
		{
			//Значит первый Левый байт этой Левой пары содержит служебные биты "110110" харакерные для символа UTF-16 закодированного двумя парами.
			//Теперь проверим содержит ли вторая пара, которая Справа от этой служеюные биты "110111":
			//-Если содержит, то это корректный UTF-16 символ состоязий из двух пар.
			//-Если не содержит, то значит это не UTF-16 символ, так как во второй Правой части нет обязательного наличия служебных бит "110111"


			//------------------------------------------------------------------------------------------
			if (pointer_Pair_size >= 2)
			{
				if ((pointer_to_begin_UTF16_symv[1] & 0b1111110000000000) == 0b1101110000000000)
				{

					//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
					const int_least32_t UnicodePointCode = convert__UTF16SymvLE_to_UnicodePointCode<T>(pointer_to_begin_UTF16_symv, 2);

					if (UnicodePointCode > 1114111)     //Если больше максимального значения для Юникодного символа
					{
						return result_flag::Greater_than_MaxUnicodeValue;
					}
					//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


					UTF16_PairSize = 2;
					return result_flag::OK;   //Возвращаем размер в символах wchar_t.
				}
				else
				{
					error = "UTF16_incorrect";
					return result_flag::UTF16_incorrect; //Значит вторая пара имеют Не корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-16 символ. Надо лучше размеры указывать.
			}
			//------------------------------------------------------------------------------------------


		}
		else
		{
			if (pointer_Pair_size >= 1)
			{

				//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
				const int_least32_t UnicodePointCode = convert__UTF16SymvLE_to_UnicodePointCode<T>(pointer_to_begin_UTF16_symv, 1);

				if (UnicodePointCode >= 55296 && UnicodePointCode <= 57343)     //(0xD800 – 0xDFFF)
				{
					error = "IncludeSurrogatePairValue";
					return result_flag::IncludeSurrogatePairValue;    //Данный условно корреткный UTF-8 символ входит в Юникодное значение "суррогатной пары" - для которого в Юникодной таблицы - нет никаких символов. А значит такого Юникодного символа закодированного в UTF-8 или в какой либо другой кодировке быть не должно.
				}
				//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

				UTF16_PairSize = 1;
				return result_flag::OK;  //Значит UTF16 символ состоит из одной пары wchar_t.
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-16 символ. Надо лучше размеры указывать.
			}

		}

	}

	template<typename T> inline const result_flag check_UTF16symvLE__SurrogatePair_and_return_PairSize_(const T* pointer_to_begin_UTF16_symv, const size_t pointer_Pair_size, const T** pointer_to_IncorrectPair, int& UTF16_PairSize)  noexcept
	{

		//---------------------------------------------------------------------------------------------------------------------------------------
		//Про кодировку UTF-8 читать в "check__UTF8symv_and_return_ByteSize".
		//Про перевод символа UTF-8 в Юникодное значение читать в "convert__UTF8Symv_to_UnicodePointCode".

		//Кодировка UTF-16 - это кодировка переменной длинны, в которой используется либо одна двухбайтная пара, либо две двухбайтные пары.
		//Все значения Юникод от 0 до 65535 (за исключение значений зарезервированных для "суррогатных пар"(об этом ниже) - это (десятичное: 55296 - 57343, шестнадацтиричное:0xD800 – 0xDFFF)) - кодируются ровно первой двухбайтной парой прямым способом, то есть значение Юникод просто помещается в два байта без всяких служебных битов, в UTF-8 - это сооветтвует кодированию о 1 до 3 байт.
		//Все значения Юникод от 65536 до максимального значения Юникод в 1114111 - кодируются в UTF-16 уже двумя двухбайтными парами, в UTF-8 это соответвует кодровки в 4 байта. И вот тут уже появляются служебные биты для распознования того, что UTF-16 символ закодирван двумя парами:
		//В первой Левой двухбайтовой паре Слева(в первом байте) идут сначала 6 служебных бит:110110_xx xxxxxxxx
		//Во второй Правой двухбайтовой паре Слева(в первом байте) идут сначала 6 служебных бит:110111_xx xxxxxxxx
		//Итого получается так: [110110_xx xxxxxxxx] [110111_xx xxxxxxxx]   - где x - Это "рабочие" биты, которые и составляют самое Юникодно значение, НО БЛЯТЬ ЕСТЬ НЮАНС НАХОЙ!!!! На сцену выходят "Суррогатные мать их пары".

		//"Суррогтаные пары"    - Предположим, что нам нужно закодировать Юникодный символ под значением "55296" в UTF-16, так как значение меньше "65535" - то кодируется одной парой - простым присваиванием значения. В двочином виде получаем: [11011000 00000000]
		//[11011000 00000000]   - как видно данный символ Юникодный символ под значнием "55296" очень странным образом первыми 6-ю своими битами напомниает - служебные биты при двух парной кодировке UTF-16, а если так, то, как тогда отличать, реально закодированный одной парой символ "55296" от двух парной кодировки - если первые 6 бит у них совпадают ?? 
		//А ни как не отличать, для этого в Юникоде есть костыль-геморрой, состоящий в том, что ЮНИКОДНЫЙ СИМВОЛОВ СО ЗНАЧНИЕМ В ДИАПАЗОНЕ "55296 - 57343" НЕ СУЩЕСТВУЕТ!!! НИ ОНДНОМУ ЗНАЧЕНИЮ ИЗ ЭТОГО ДИАПАЗОНА НЕТ СООВТЕТВУЮЩЕГО ЮНИКОДНОГО СИМВОЛА, СПЕЦИАЛЬНО ДЛЯ ТОГО, ЧТОБЫ НЕ БЫЛО ВОТ ТАКОЙ ПРОБЛЕМЫ С ПЕРСЕЧЕНИЕМ РЕАЛНЫХ И СЛУЖЕБНЫХ БИТ ПРИ UTF-16 КОДИРОВКЕ.
		//Возникает вопрос, а НА***УЯ это вообще придумали, почему нельзя было сделать так, как в UTF-8 кодировки, то есть если символ Юникод умещается в первую пару UTF-16, то он бы кодировался первым нулевым служебным битом(как в однобоайтовой кодировке UTF-8), а потом бы уже шли рабочие биты. И если бы символ Юникода не умещался бы в первую пару, то тогда, первая парабы начиналась со служеюных бит "110", а во творой паре было бы продолжение в виде двух служебных бит "10" (как при двухбайтовой кодировании UTF-8) - КОРОЧЕ если я праильно понял, то из за того, что нужно было сохранить совместимость с ранеее сущестовашей еще до UTF-16 кодировки UCS-2, которая так же была строго двух байтной и кодировалась прямым способо без всяких служебных бит.

		//ОДНАКО это не все: то есть еще раз, все значения Юникода от 0 до 65535 (исключая не существующий сурогаттный диапазон) просто заносятся в первую пару без изменений и получатся кодировка UTF-16.
		//Значение выше 65535 (исключая не существующий сурогаттный диапазон):  для кодировки таких символов уже нужно кол-во рабочих бит от 17 бит до 21 бита, ОДНАКО смотрим сколько реально рабочих бит доступно при двухпарном кодировании - [110110_xx xxxxxxxx] [110111_xx xxxxxxxx] = 10 + 10 = 20. Доступно 20 бит!!! А для максимальных значений Юникода нужен 21 бит!!!  Че за фигня ?
		//При кодировании символа Юникод выше значения 65535 - Всегда из значения этого символа вычитается значение "65536": то есть если взять максимальное значние Юникодного символа в "1114111" вычесть из него 65536, то получим "1048575", что в двоичном виде займет ровно 20 бит - "11111111111111111111", которые уже раваномрно распределятся по двум парам UTF-16.
		//РАССМОТРИМ ПРИМЕР: Возьмем к примеру иероглиф "𠀅" со значением в Юникоде "131077", теперь по правилу вычитаем из него "65536": 131077 - 65536 = 65541.  
		//Смотрим как выглядет "65541" в двоичном виде: 10000000000000101  - теперь первые Левые 10 бит [10000000000]  берем и помещаем в первую пару за место X-ов: [110110_10 000000000].
		//После того, как мы взяли Левые 10 бит, у нас остаются 7 бит Справа [0000101] - к ним нужно доавбить Слева нули до 10 бит в этом случае это 3 нуля --> [0000000101] и помещаем их во вторую пару за место X-ов: [110111_00 00000101].
		//Все UTF-16 символ готов.
		//---------------------------------------------------------------------------------------------------------------------------------------


		//-------------------------------------------------------------------------------------------------
		//pointer_to_begin_UTF8_symv - должен быть указатель на первый байт UTF-8 символа.
		//string_UTF8_size_byte      - размер в символах whar_t, в пределах которого нужно проверить Один символ на который указывает "pointer_to_begin_UTF16_symv", может быть по сути любым, ТО ЕСТЬ, функция проверит является ли Указатель "pointer_to_begin_UTF16_symv" указатель на начальый Байт UTF-16 символа, и если является, то првоерит корректность остальных байт этого UTF-8 символа, если размер указанынй в "string_UTF8_size_byte" будет больше, чем общий размер в байтах UTF-8 символа, то он просто не будет ни как испльзоватся, а, если размер указанынй в "string_UTF8_size_byte" будет меньше, чем размер в байтах UTF-8 символа на который указывает указатель "pointer_to_begin_UTF8_symv" - то такой символ будет признан Некорректным, так как функции просто тупо не хватило данных для окончания анализа.
		//-------------------------------------------------------------------------------------------------



		if ((pointer_to_begin_UTF16_symv[0] & 0b1111110000000000) == 0b1101100000000000)
		{
			//Значит первый Левый байт этой Левой пары содержит служебные биты "110110" харакерные для символа UTF-16 закодированного двумя парами.
			//Теперь проверим содержит ли вторая пара, которая Справа от этой служеюные биты "110111":
			//-Если содержит, то это корректный UTF-16 символ состоязий из двух пар.
			//-Если не содержит, то значит это не UTF-16 символ, так как во второй Правой части нет обязательного наличия служебных бит "110111"


			//------------------------------------------------------------------------------------------
			if (pointer_Pair_size >= 2)
			{
				if ((pointer_to_begin_UTF16_symv[1] & 0b1111110000000000) == 0b1101110000000000)
				{

					//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
					const int_least32_t UnicodePointCode = convert__UTF16SymvLE_to_UnicodePointCode<T>(pointer_to_begin_UTF16_symv, 2);

					if (UnicodePointCode > 1114111)     //Если больше максимального значения для Юникодного символа
					{
						error = "Greater_than_MaxUnicodeValue";
						return result_flag::Greater_than_MaxUnicodeValue;
					}
					//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


					UTF16_PairSize = 2;
					return result_flag::OK;   //Возвращаем размер в символах wchar_t.
				}
				else
				{
					*pointer_to_IncorrectPair = &pointer_to_begin_UTF16_symv[1];
					error = "UTF16_incorrect";
					return result_flag::UTF16_incorrect; //Значит вторая пара имеют Не корреткные служебные биты.
				}
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-16 символ. Надо лучше размеры указывать.
			}
			//------------------------------------------------------------------------------------------


		}
		else
		{
			if (pointer_Pair_size >= 1)
			{

				//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
				const int_least32_t UnicodePointCode = convert__UTF16SymvLE_to_UnicodePointCode<T>(pointer_to_begin_UTF16_symv, 1);

				if (UnicodePointCode >= 55296 && UnicodePointCode <= 57343)     //(0xD800 – 0xDFFF)
				{
					error = "IncludeSurrogatePairValue";
					return result_flag::IncludeSurrogatePairValue;    //Данный условно корреткный UTF-8 символ входит в Юникодное значение "суррогатной пары" - для которого в Юникодной таблицы - нет никаких символов. А значит такого Юникодного символа закодированного в UTF-8 или в какой либо другой кодировке быть не должно.
				}
				//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

				UTF16_PairSize = 1;
				return result_flag::OK;  //Значит UTF16 символ состоит из одной пары wchar_t.
			}
			else
			{
				error = "size_not_enough_for_analysis";
				return result_flag::size_not_enough_for_analysis;  	//Значит, что размер по переданнмоу указателю указан недостаточный для анализа данного символа, а значит сделаем вывод, что это нифига не UTF-16 символ. Надо лучше размеры указывать.
			}

		}

	}


	template<typename T> const result_flag check_UTF16LE_string(const T* pointer_to_UTF16_string, const size_t string_UTF16_Pair_size)  noexcept
	{
		//pointer_to_UTF8_string - должен быть указатель на первый байт UTF-8 символа.
		//string_UTF8_size_byte  - размер в байтах всего массива на который указывает указатель "pointer_to_UTF8_string"


		result_flag result;
		int UTF16_PairSize;

		//-----------------------------------------------------------------------------------------------
		for (size_t i = 0; i < string_UTF16_Pair_size; i++)
		{
			result = check_UTF16symvLE_and_return_PairSize_<T>(&pointer_to_UTF16_string[i], string_UTF16_Pair_size - i, UTF16_PairSize);

			if (result != result_flag::OK)
			{
				//Значит "&pointer_to_UTF8_string[i]" - наткнулся на некорректное начало очередного UTF-8 символа

				return result;
			}

			i = i + UTF16_PairSize - 1;          //Прибавляем размер байт найденого UTF8-символа, чтобы со следующей итерацией начать с теоертически нового UTF-8 символа. "-1" - нужен для компенсации того, что со следующей итерацией цикла "for" сам еще прибавит "+1". 
		}
		//-----------------------------------------------------------------------------------------------

		return result_flag::OK;
	}

	template<typename T> const result_flag check_UTF16LE_string(const T* pointer_to_UTF16_string, const size_t string_UTF16_Pair_size, const T** pointer_to_IncorrectByte)  noexcept
	{
		//pointer_to_UTF8_string - должен быть указатель на первый байт UTF-8 символа.
		//string_UTF8_size_byte  - размер в байтах всего массива на который указывает указатель "pointer_to_UTF8_string"


		result_flag result;
		int UTF16_PairSize;

		//-----------------------------------------------------------------------------------------------
		for (size_t i = 0; i < string_UTF16_Pair_size; i++)
		{
			result = check_UTF16symvLE_and_return_PairSize_<T>(&pointer_to_UTF16_string[i], string_UTF16_Pair_size - i, pointer_to_IncorrectByte, UTF16_PairSize);          //Проверяем сооветтвует ли байт на который указывает указатель - началу UTF-8 символа.

			if (result != result_flag::OK)
			{
				//Значит "&pointer_to_UTF8_string[i]" - наткнулся на некорректное начало очередного UTF-8 символа

				return result;
			}

			i = i + UTF16_PairSize - 1;          //Прибавляем размер байт найденого UTF8-символа, чтобы со следующей итерацией начать с теоертически нового UTF-8 символа. "-1" - нужен для компенсации того, что со следующей итерацией цикла "for" сам еще прибавит "+1". 
		}
		//-----------------------------------------------------------------------------------------------


		return result_flag::OK;
	}

	template<typename T> const result_flag check_UTF16LE_string__SurrogatePair(const T* pointer_to_UTF16_string, const size_t string_UTF16_Pair_size)  noexcept
	{
		//pointer_to_UTF8_string - должен быть указатель на первый байт UTF-8 символа.
		//string_UTF8_size_byte  - размер в байтах всего массива на который указывает указатель "pointer_to_UTF8_string"


		result_flag result;
		int UTF16_PairSize;

		//-----------------------------------------------------------------------------------------------
		for (size_t i = 0; i < string_UTF16_Pair_size; i++)
		{
			result = check_UTF16symvLE__SurrogatePair_and_return_PairSize_<T>(&pointer_to_UTF16_string[i], string_UTF16_Pair_size - i, UTF16_PairSize);

			if (result != result_flag::OK)
			{
				//Значит "&pointer_to_UTF8_string[i]" - наткнулся на некорректное значение UTF-8 символа.

				return result;
			}

			i = i + UTF16_PairSize - 1;          //Прибавляем размер байт найденого UTF8-символа, чтобы со следующей итерацией начать с теоертически нового UTF-8 символа. "-1" - нужен для компенсации того, что со следующей итерацией цикла "for" сам еще прибавит "+1". 
		}
		//-----------------------------------------------------------------------------------------------

		return result_flag::OK;
	}

	template<typename T> const result_flag check_UTF16LE_string__SurrogatePair(const T* pointer_to_UTF16_string, const size_t string_UTF16_Pair_size, const T** pointer_to_IncorrectByte)  noexcept
	{
		//pointer_to_UTF8_string - должен быть указатель на первый байт UTF-8 символа.
		//string_UTF8_size_byte  - размер в байтах всего массива на который указывает указатель "pointer_to_UTF8_string"


		result_flag result;
		int UTF16_PairSize;

		//-----------------------------------------------------------------------------------------------
		for (size_t i = 0; i < string_UTF16_Pair_size; i++)
		{
			result = check_UTF16symvLE__SurrogatePair_and_return_PairSize_<T>(&pointer_to_UTF16_string[i], string_UTF16_Pair_size - i, pointer_to_IncorrectByte, UTF16_PairSize);          //Проверяем сооветтвует ли байт на который указывает указатель - началу UTF-8 символа.

			if (result != result_flag::OK)
			{
				//Значит "&pointer_to_UTF8_string[i]" - наткнулся на некорректное значение UTF-8 символа.

				return result;
			}

			i = i + UTF16_PairSize - 1;          //Прибавляем размер байт найденого UTF8-символа, чтобы со следующей итерацией начать с теоертически нового UTF-8 символа. "-1" - нужен для компенсации того, что со следующей итерацией цикла "for" сам еще прибавит "+1". 
		}
		//-----------------------------------------------------------------------------------------------


		return result_flag::OK;
	}
	//****************************************************************UTF-16:Начало***********************************************************************************


	//****************************************************************UTF-8:Начало***********************************************************************************
	inline const int get__UTF8symv_ByteSize(const char* pointer_to_begin_UTF8_symv) noexcept
	{


		//ВСЕ ПЕРЕДАВАЕМЫЕ ЮНИКОДНЫЕ СИМВОЛЫ ДОЛЖНЫ БЫТЬ ГАРАНТИРОВАНО КОРРЕКТНЫМИ. ФУНКЦИЯ НЕ ЗАНИМАЕТСЯ ПРОВЕРКОЙ СИМВОЛОВ НА КОРРЕКТНОСТЬ СООТВЕТВУЮЩЕЙ КОДИРОВКЕ. 
		//ЕСЛИ ЕСТЬ СОМНЕНИЯ В КОРРЕКТНОСТИ КОДИРОВК - ЭТО ДОЛЖНО БЫТЬ ПРОВЕРЕНО ЗАРАНЕЕ ПРОВЕРЕНО С ПОМОЩЬЮ СОООВЕТСТВУЮЩЕЙ ФУНКЦИИ  "check_xxxxx"




			//-------------------------------------------------------------------------------------------------------
			//Описание, как кодируется символ UTF8: в трех примерах:
			//Кодировка UTF8 - это кодировка переменной длинны от 1 до 4 байт:
			//Условимся, что мы рассматриваем байты и биты слева направо, то есть первый бит или байт всегда будет слева, осталные правее.

			//Пример1: Закодируем символ ASCII к примеру "Q" кодировкой UTF8:
			//Первый БИТ начального байта будет содержать 0 - он будет указывать на то, что это первый БАЙТ - это единсвенный БАЙТ, в котором содержится кодировка символа "Q".
			//Остальные БИТы БАЙТа - будут содержать само закодированное значение и это значение 81 из таблицы ASII закодированое в бинарное - 1010001 и с учетом первого нуля - это будет: 01010001 - то есть значение кодировки UTF8 для символов ASCII будет ПОЛНОСТЬЮ совпадать.



			//Пример2: Закодируем символ к примеру "Ǎ" "Латинская заглавная буква «A» с гачеком Ǎ" кодировкой UTF8:
			//В таблице Юникода буква "Ǎ" - имеет десятичное значение - 461.
			//Переведем десятичное значения 461 - в двоичное число: 111001101 - то есть для кодирования значения 461 или буквы "Ǎ" нужно 9 БИТ. 9 рабочих БИТ, не учитывая служебные БИТы, которые говрят о кол-ве БАЙТ, которым закодирован символ.
			//Теперь закодируем значение 111001101 по правилам кодировки UTF8:
			//То есть в первом БАЙТЕ в первых двух БИТАХ указывается - 11, что будет говорит о том, что кодирование значения "111001101" будет занимать два БАЙТА.
			//В первом байте после указания "11" ДОЛЖЕН ВСЕГДА ОБЯЗАТЕЛЬНО ИДТИ БИТ с числом "0": 110 ... ТАК ПРЕДВАРИТЕЛЬНО Первый БАЙТ Готов. ТЕПЕРЬ ВАЖНО: теперь заполнение эти Двух БАЙТ нужно ввести с конца, то есть в данном случае со второго БАЙТА:
			//Теперь закодируем Второй БАЙТ : ЛЮБЫЕ ПОСЛЕДУЮЩИЕ БАЙТЫ закодированного символа после Первого байта ДОЛЖНЫ начинатся с служебных двух БИТОВ : 10 ..и после них уже должны идти следующие рабочие БИТЫ.
			//То есть во Втором БАЙТЕ у нас с учетом первых двух служебных БИТОВ 10 - остается 6 БИТ: заполним их 6-Ю БИТАМИ из нашего символа "Ǎ" справа: "111_001101" - то есть мы берем правую часть БИТОВ, который помещаются во второй БАЙТ и помещаем их во Второй БАЙТ: 10_001101
			//Теперь переходим к Первому БАЙТУ и оставшимся БИТАМ из "Ǎ"  "111_001101" - то есть остались "111" - их нужно добавить в Первый БАЙТ...
			//Напоминаю, что Первый БАЙТ у нас заполнен на данный момент следующими служебными БИТАМИ: 110... - то есть у нас в Первом БАЙТЕ свободно еще 5 БИТ, но ДОБАВИТЬ нужно всего 3 БИТА "111"...
			//ДОБАВЛЯТЬ эти три БИТА "111" в Первый БАЙТ нужно также справа и те БИТЫ в первом БАЙТЕ, который не "заполнились" - заполняем нулем: итого: "110_00_111"


			//ВСЕ остальные символы кодируются по этим двум правилам: итого:
			//Кодировка UTF8 может занимать от 1 до 4 байт.
			//Символ закодированный 1 байтом - всегда начинается с 0 бита слева. И все остальные 7 бит - это самое значение символа:
			//0.....

			//Символ закодированный от 2 до 4 байт ВСЕГДА начинается в Первом байте с:
			//110...     - закодирован двумя байтами
			//1110...    - закодирован тремя байтами
			//11110...   - закодирован четрьмя байтами

			//Вторые и поcледующие байты символ закодированных более, чем одним байтом ВСЕГДА начинаются с двух первых битов "10" и остальные 6 бит - заполянются уже рабочми битами самого символа.
			//Заполнение рабочих бит символа - идет с последнего байта к первому. И там где в Первом байте остались "незаполенные биты" - они заполняются нулями.
			//-------------------------------------------------------------------------------------------------------


			//pointer_to_begin_UTF8_symv - должен быть указатель на первый байт UTF-8 символа.


			//А теперь применем побитовое "И" для того, чтобы определить есть ли в  проверяемом байте сооветствие 4-ем видам битов UTF-8 говорящее о кол-ве задействоаных байт для кодировки символа:

		if (0b11110000 == (pointer_to_begin_UTF8_symv[0] & 0b11110000))
		{
			return 4;
		}
		else if (0b11100000 == (pointer_to_begin_UTF8_symv[0] & 0b11100000))
		{
			return 3;
		}
		else if (0b11000000 == (pointer_to_begin_UTF8_symv[0] & 0b11000000))
		{
			return 2;
		}
		else if (0b00000000 == (pointer_to_begin_UTF8_symv[0] & 0b10000000))     //То есть если проверяемый байт имеет первый Бит 1_1010101 - то после применения "И" - будет 1_0010101 - что не соответвует одному из 4-ех видов начальных бит. Если же первый бит 0_0010101 - то после "И" соответсвенно будет 0_0010101 - что показывает, что первый Бит нулевой, что соответвует одному их 4 видов.
		{
			return 1;
		}

	}

	const size_t get__UTF8SymvsNumbers(const char* pointer_to_UTF8_string, const size_t string_UTF8_size_byte) noexcept
	{

		//ВСЕ ПЕРЕДАВАЕМЫЕ ЮНИКОДНЫЕ СИМВОЛЫ ДОЛЖНЫ БЫТЬ ГАРАНТИРОВАНО КОРРЕКТНЫМИ. ФУНКЦИЯ НЕ ЗАНИМАЕТСЯ ПРОВЕРКОЙ СИМВОЛОВ НА КОРРЕКТНОСТЬ СООТВЕТВУЮЩЕЙ КОДИРОВКЕ. 
		//ЕСЛИ ЕСТЬ СОМНЕНИЯ В КОРРЕКТНОСТИ КОДИРОВК - ЭТО ДОЛЖНО БЫТЬ ПРОВЕРЕНО ЗАРАНЕЕ ПРОВЕРЕНО С ПОМОЩЬЮ СОООВЕТСТВУЮЩЕЙ ФУНКЦИИ  "check_xxxxx"


		//pointer_to_UTF8_string - должен быть указатель на первый байт UTF-8 символа.
		//string_UTF8_size_byte  - размер в байтах всего массива на который указывает указатель "pointer_to_UTF8_string"


		//------------------------------
		size_t UTF8_numbers = 0;

		int UTF8_symv__ByteSize;
		//------------------------------


		//-----------------------------------------------------------------------------------------------
		for (size_t i = 0; i < string_UTF8_size_byte; i++)
		{
			UTF8_symv__ByteSize = get__UTF8symv_ByteSize(&pointer_to_UTF8_string[i]);          //Проверяем сооветтвует ли байт на который указывает указатель - началу UTF-8 символа(не проверяем в данном случае) и если соовтетвует, то получаем размер в байтах этого симола.

			UTF8_numbers++;                           //прибавляем счетчик найденых символов UTF8.

			i = i + UTF8_symv__ByteSize - 1;          //Прибавляем размер байт найденого UTF8-символа, чтобы со следующей итерацией начать с теоертически нового UTF-8 символа. "-1" - нужен для компенсации того, что со следующей итерацией цикла "for" сам еще прибавит "+1". 
		}
		//-----------------------------------------------------------------------------------------------

		return UTF8_numbers;

	}
	//****************************************************************UTF-8:Конец***********************************************************************************



	//****************************************************************UTF-16:Начало***********************************************************************************
	template<typename T> inline const int get__UTF16symvLE_PairSize(const T* pointer_to_begin_UTF16_symv) noexcept
	{

		//ВСЕ ПЕРЕДАВАЕМЫЕ ЮНИКОДНЫЕ СИМВОЛЫ ДОЛЖНЫ БЫТЬ ГАРАНТИРОВАНО КОРРЕКТНЫМИ. ФУНКЦИЯ НЕ ЗАНИМАЕТСЯ ПРОВЕРКОЙ СИМВОЛОВ НА КОРРЕКТНОСТЬ СООТВЕТВУЮЩЕЙ КОДИРОВКЕ. 
		//ЕСЛИ ЕСТЬ СОМНЕНИЯ В КОРРЕКТНОСТИ КОДИРОВК - ЭТО ДОЛЖНО БЫТЬ ПРОВЕРЕНО ЗАРАНЕЕ ПРОВЕРЕНО С ПОМОЩЬЮ СОООВЕТСТВУЮЩЕЙ ФУНКЦИИ  "check_xxxxx"



		//pointer_to_begin_UTF16_symv - должен быть указатель на первую пару UTF-16 символа.


		//[110110_xx xxxxxxxx][110111_xx xxxxxxxx]

		//А теперь применем побитовое "И" для того, чтобы определить есть ли в  проверяемой паре сооветствующий служебный бит характерный для первой "суррогатной пары":


		if ((pointer_to_begin_UTF16_symv[0] & 0b1111110000000000) == 0b1101100000000000)
		{
			//Значит первый Левый байт этой пары содержит служебные биты "110110" харакерные для символа UTF-16 закодированного двумя парами.
			//Значит этот символ UTF-16, который Заведомо должен быть корректным, имеет размер в две пары.

			return 2;
		}
		else
		{

			//Значит служеюных битов характерных для первой "суррогтаной пары" не найдено, значит это однопарный сиимвол UTF-16.
			return 1;
		}

	}

	template<typename T> const size_t get__UTF16SymvsLENumbers(const T* pointer_to_UTF16_string, const size_t string_UTF16_Pair_size) noexcept
	{

		//ВСЕ ПЕРЕДАВАЕМЫЕ ЮНИКОДНЫЕ СИМВОЛЫ ДОЛЖНЫ БЫТЬ ГАРАНТИРОВАНО КОРРЕКТНЫМИ. ФУНКЦИЯ НЕ ЗАНИМАЕТСЯ ПРОВЕРКОЙ СИМВОЛОВ НА КОРРЕКТНОСТЬ СООТВЕТВУЮЩЕЙ КОДИРОВКЕ. 
		//ЕСЛИ ЕСТЬ СОМНЕНИЯ В КОРРЕКТНОСТИ КОДИРОВК - ЭТО ДОЛЖНО БЫТЬ ПРОВЕРЕНО ЗАРАНЕЕ ПРОВЕРЕНО С ПОМОЩЬЮ СОООВЕТСТВУЮЩЕЙ ФУНКЦИИ  "check_xxxxx"




		//pointer_to_UTF16_string - должен быть указатель на первую пару UTF-16 символа.
		//string_UTF16_Pair_size  - размер в парах всего массива на который указывает указатель "pointer_to_UTF16_string"


		//------------------------------
		size_t UTF16_numbers = 0;

		int UTF16_symv__PairSize;
		//------------------------------


		//-----------------------------------------------------------------------------------------------
		for (size_t i = 0; i < string_UTF16_Pair_size; i++)
		{
			UTF16_symv__PairSize = get__UTF16symvLE_PairSize(&pointer_to_UTF16_string[i]);          //получаем размер в парах этого симола.

			UTF16_numbers++;                           //прибавляем счетчик найденых символов UTF16.

			i = i + UTF16_symv__PairSize - 1;          //Прибавляем размер байт найденого UTF8-символа, чтобы со следующей итерацией начать с теоертически нового UTF-8 символа. "-1" - нужен для компенсации того, что со следующей итерацией цикла "for" сам еще прибавит "+1". 
		}
		//-----------------------------------------------------------------------------------------------

		return UTF16_numbers;

	}
	//****************************************************************UTF-16:Конец***********************************************************************************






	//****************************************************************UTF-8:Начало***********************************************************************************
	
	inline const int_least32_t convert__UTF8Symv_to_UnicodePointCode(const char* pointer_to_beg_UTF8_symv) noexcept
	{
		
		//ВСЕ ПЕРЕДАВАЕМЫЕ ЮНИКОДНЫЕ СИМВОЛЫ ДОЛЖНЫ БЫТЬ ГАРАНТИРОВАНО КОРРЕКТНЫМИ. ФУНКЦИЯ НЕ ЗАНИМАЕТСЯ ПРОВЕРКОЙ СИМВОЛОВ НА КОРРЕКТНОСТЬ СООТВЕТВУЮЩЕЙ КОДИРОВКЕ. 
		//ЕСЛИ ЕСТЬ СОМНЕНИЯ В КОРРЕКТНОСТИ КОДИРОВК - ЭТО ДОЛЖНО БЫТЬ ПРОВЕРЕНО ЗАРАНЕЕ ПРОВЕРЕНО С ПОМОЩЬЮ СОООВЕТСТВУЮЩЕЙ ФУНКЦИИ  "check_xxxxx"



		//Про кодировку UTF-8 читать в функции "check_UTF8_symv_and_return_ByteSize".


		//pointer_to_beg_UTF8_symv - должен быть указатель на первый байт UTF-8 символа.


		//-----------------------------------------------Получим размер в байтах UTF8 символа:Начало-----------------------------------------

		const int UTF8symv_ByteSize = get__UTF8symv_ByteSize(pointer_to_beg_UTF8_symv);   //ПРОВЕРКУ НА НЕКОРРЕКТНОСТЬ UTF-8 кодировки не проверяется, так как считается, что Указатель должен быть заведомо на Корректный символ. Если нужно првоерить на Коррекстность кодировки UTF-8, то есть функция "check_UTF8_string"

		//-----------------------------------------------Получим размер в байтах UTF8 символа:Конец-----------------------------------------





		//----------------------------------------------------------------------------------------------------------------------------------------
		if (UTF8symv_ByteSize == 1)
		{
			//То есть наш UTF-8 символ занимает 1 байт: этот байт и будет значением кодовой точки Юникод, который совпадает с ASCII. 
			//0_xxxxxxx

			return *pointer_to_beg_UTF8_symv;      //То есть просто передаем сам первый char, значение которого потом автоматически приводится к int - это и будет значение Юникодного символа, который совпадает с ASCII.
		}
		else if (UTF8symv_ByteSize == 2)
		{
			//То есть наш UTF-8 символ занимает 2 байт: x - это как раз рабочие Биты из которых нужно сложить значение, которое и будет значит Кодовую Точку Юникод. (читать описание в "get__UTF8symv_ByteSize")
			//110_xxxxx
			//10_xxxxxx

			//Пример, условно выдуманный UTF-8 символ подходящий по всем правилам кодировки UTF-8, который занимает два байта:

			//110_01010
			//10_011101

			//ТО ЕСТЬ теперь нужно составить из "01010" и "011101" новое значение: "01010011101" и привести его к примеру к int. Шаги, которые нужно для этого препринять следующие:

			//110_01010  - нам условно нужно "избавится" от первых трех служебных бит, ну или занулить их, для это применем масочку 000_11111 и операцию "И", на что получим: 000_01010
			//10_011101  - нам условно нужно "избавится" от первых трех служебных бит, ну или занулить их, для это применем масочку 00_111111 и операцию "И", на что получим: 00_011101

			//ТЕПЕРЬ будем "помещать" эти биты в итоговый int:

			//00000000 00000000 00000000 00000000  - это наш 4-байтный int, куда мы поместим результат.

			//Берем "000_01010" и сдвигаем на 6 бит влево:

			//00000000 00000000 00000000 00000000
			//00000000 00000000 00000010 10[000000] - [6 бит теперь свободны для 6 бит "011101" из второго байта], которые мы "помещаем" в эти крайние 6 бит слева Операцией "ИЛИ" для "00_011101"   

			//Применяем к сдвинутому на 6 бит влево int`y "ИЛИ" по второму байту "00_011101":
			//00000000 00000000 00000010 10000000
			//                           00011101
			//00000000 00000000 00000010 10011101   - ИТОГОВЫЙ РЕЗУЛЬТАТ. То есть "01010" и "011101" идут подряд составляя новое значение.

			//Теперь сделаем это:

			return ((0b00011111 & pointer_to_beg_UTF8_symv[0]) << 6 | (0b00111111 & pointer_to_beg_UTF8_symv[1]) << 0);
		}
		else if (UTF8symv_ByteSize == 3)
		{
			//То есть наш UTF-8 символ занимает 3 байт: x - это как раз рабочие Биты из которых нужно сложить значение, которое и будет значит Кодовую Точку Юникод. (читать описание в "get__UTF8symv_ByteSize")
			//1110_xxxx
			//10_xxxxxx
			//10_xxxxxx

			//ВСЕ ТОЖЕ САМОЕ - КАК ДЛЯ ДВУХ БАЙТ, НО ТОЛЬКО ДЛЯ ТРЕХ БАЙТ.


			return ((0b00001111 & pointer_to_beg_UTF8_symv[0]) << 12 | (0b00111111 & pointer_to_beg_UTF8_symv[1]) << 6 | (0b00111111 & pointer_to_beg_UTF8_symv[2]) << 0);

		}
		else if (UTF8symv_ByteSize == 4)
		{
			//То есть наш UTF-8 символ занимает 4 байт: x - это как раз рабочие Биты из которых нужно сложить значение, которое и будет значит Кодовую Точку Юникод. (читать описание в "get__UTF8symv_ByteSize")
			//11110_xxx
			//10_xxxxxx
			//10_xxxxxx
			//10_xxxxxx


			//ВСЕ ТОЖЕ САМОЕ - КАК ДЛЯ ДВУХ БАЙТ, НО ТОЛЬКО ДЛЯ ЧЕТЫРЕХ БАЙТ.


			return ((0b00000111 & pointer_to_beg_UTF8_symv[0]) << 18 | (0b00111111 & pointer_to_beg_UTF8_symv[1]) << 12 | (0b00111111 & pointer_to_beg_UTF8_symv[2]) << 6 | (0b00111111 & pointer_to_beg_UTF8_symv[3]) << 0);

		}
		//----------------------------------------------------------------------------------------------------------------------------------------

	}
	
	inline const int_least32_t convert__UTF8Symv_to_UnicodePointCode(const char* pointer_to_beg_UTF8_symv, const int UTF8symv_ByteSize) noexcept
	{

		//ВСЕ ПЕРЕДАВАЕМЫЕ СИМВОЛЫ ДОЛЖНЫ БЫТЬ ГАРАНТИРОВАНО КОРРЕКТНЫМИ. ФУНКЦИЯ НЕ ЗАНИМАЕТСЯ ПРОВЕРКОЙ СИМВОЛОВ НА КОРРЕКТНОСТЬ СООТВЕТВУЮЩЕЙ КОДИРОВКЕ. 
		//ЕСЛИ ЕСТЬ СОМНЕНИЯ В КОРРЕКТНОСТИ КОДИРОВК - ЭТО ДОЛЖНО БЫТЬ ПРОВЕРЕНО ЗАРАНЕЕ ПРОВЕРЕНО С ПОМОЩЬЮ СОООВЕТСТВУЮЩЕЙ ФУНКЦИИ  "check_xxxxx"



		//Про кодировку UTF-8 читать в функции "check_UTF8_symv_and_return_ByteSize".


		//pointer_to_beg_UTF8_symv - должен быть указатель на первый байт UTF-8 символа.
		//symv__Byte_Size          - размер символа UTF-8 в байтах.



		//----------------------------------------------------------------------------------------------------------------------------------------
		if (UTF8symv_ByteSize == 1)
		{
			//То есть наш UTF-8 символ занимает 1 байт: этот байт и будет значением кодовой точки Юникод, который совпадает с ASCII. 
			//0_xxxxxxx

			return *pointer_to_beg_UTF8_symv;      //То есть просто передаем сам первый char, значение которого потом автоматически приводится к int - это и будет значение Юникодного символа, который совпадает с ASCII.
		}
		else if (UTF8symv_ByteSize == 2)
		{
			//То есть наш UTF-8 символ занимает 2 байт: x - это как раз рабочие Биты из которых нужно сложить значение, которое и будет значит Кодовую Точку Юникод. (читать описание в "get__UTF8symv_ByteSize")
			//110_xxxxx
			//10_xxxxxx

			//Пример, условно выдуманный UTF-8 символ подходящий по всем правилам кодировки UTF-8, который занимает два байта:

			//110_01010
			//10_011101

			//ТО ЕСТЬ теперь нужно составить из "01010" и "011101" новое значение: "01010011101" и привести его к примеру к int. Шаги, которые нужно для этого препринять следующие:

			//110_01010  - нам условно нужно "избавится" от первых трех служебных бит, ну или занулить их, для это применем масочку 000_11111 и операцию "И", на что получим: 000_01010
			//10_011101  - нам условно нужно "избавится" от первых трех служебных бит, ну или занулить их, для это применем масочку 00_111111 и операцию "И", на что получим: 00_011101

			//ТЕПЕРЬ будем "помещать" эти биты в итоговый int:

			//00000000 00000000 00000000 00000000  - это наш 4-байтный int, куда мы поместим результат.

			//Берем "000_01010" и сдвигаем на 6 бит влево:

			//00000000 00000000 00000000 00000000
			//00000000 00000000 00000010 10[000000] - [6 бит теперь свободны для 6 бит "011101" из второго байта], которые мы "помещаем" в эти крайние 6 бит слева Операцией "ИЛИ" для "00_011101"   

			//Применяем к сдвинутому на 6 бит влево int`y "ИЛИ" по второму байту "00_011101":
			//00000000 00000000 00000010 10000000
			//                           00011101
			//00000000 00000000 00000010 10011101   - ИТОГОВЫЙ РЕЗУЛЬТАТ. То есть "01010" и "011101" идут подряд составляя новое значение.

			//Теперь сделаем это:

			return ((0b00011111 & pointer_to_beg_UTF8_symv[0]) << 6 | (0b00111111 & pointer_to_beg_UTF8_symv[1]) << 0);
		}
		else if (UTF8symv_ByteSize == 3)
		{
			//То есть наш UTF-8 символ занимает 3 байт: x - это как раз рабочие Биты из которых нужно сложить значение, которое и будет значит Кодовую Точку Юникод. (читать описание в "get__UTF8symv_ByteSize")
			//1110_xxxx
			//10_xxxxxx
			//10_xxxxxx

			//ВСЕ ТОЖЕ САМОЕ - КАК ДЛЯ ДВУХ БАЙТ, НО ТОЛЬКО ДЛЯ ТРЕХ БАЙТ.


			return ((0b00001111 & pointer_to_beg_UTF8_symv[0]) << 12 | (0b00111111 & pointer_to_beg_UTF8_symv[1]) << 6 | (0b00111111 & pointer_to_beg_UTF8_symv[2]) << 0);

		}
		else if (UTF8symv_ByteSize == 4)
		{
			//То есть наш UTF-8 символ занимает 4 байт: x - это как раз рабочие Биты из которых нужно сложить значение, которое и будет значит Кодовую Точку Юникод. (читать описание в "get__UTF8symv_ByteSize")
			//11110_xxx
			//10_xxxxxx
			//10_xxxxxx
			//10_xxxxxx


			//ВСЕ ТОЖЕ САМОЕ - КАК ДЛЯ ДВУХ БАЙТ, НО ТОЛЬКО ДЛЯ ЧЕТЫРЕХ БАЙТ.


			return ((0b00000111 & pointer_to_beg_UTF8_symv[0]) << 18 | (0b00111111 & pointer_to_beg_UTF8_symv[1]) << 12 | (0b00111111 & pointer_to_beg_UTF8_symv[2]) << 6 | (0b00111111 & pointer_to_beg_UTF8_symv[3]) << 0);

		}
		//----------------------------------------------------------------------------------------------------------------------------------------


	}


	void convert__UTF8_String_to_Vec_UnicodePointCode(const char* pointer_to_UTF8_string, const size_t string_UTF8_size_byte, std::vector<int_least32_t>& vec_UnicodePointCode)
	{

		//ВСЕ ПЕРЕДАВАЕМЫЕ ЮНИКОДНЫЕ СИМВОЛЫ ДОЛЖНЫ БЫТЬ ГАРАНТИРОВАНО КОРРЕКТНЫМИ. ФУНКЦИЯ НЕ ЗАНИМАЕТСЯ ПРОВЕРКОЙ СИМВОЛОВ НА КОРРЕКТНОСТЬ СООТВЕТВУЮЩЕЙ КОДИРОВКЕ. 
		//ЕСЛИ ЕСТЬ СОМНЕНИЯ В КОРРЕКТНОСТИ КОДИРОВК - ЭТО ДОЛЖНО БЫТЬ ПРОВЕРЕНО ЗАРАНЕЕ ПРОВЕРЕНО С ПОМОЩЬЮ СОООВЕТСТВУЮЩЕЙ ФУНКЦИИ  "check_xxxxx"


		//pointer_to_UTF8_string - должен быть указатель на первый байт UTF-8 символа.
		//string_UTF8_size_byte  - размер в байтах всего массива на который указывает указатель "pointer_to_UTF8_string"


		//-----------------------------------------------------------------------------------------------
		for (size_t i = 0; i < string_UTF8_size_byte; i++)
		{

			//---------------------------------------------------------------------
			const int ByteSize = get__UTF8symv_ByteSize(&pointer_to_UTF8_string[i]);
			const int_least32_t UnicodePointCode = convert__UTF8Symv_to_UnicodePointCode(&pointer_to_UTF8_string[i], ByteSize);

			vec_UnicodePointCode.emplace_back(UnicodePointCode);
			//---------------------------------------------------------------------


			i = i + ByteSize - 1;          //Прибавляем размер байт найденого UTF8-символа, чтобы со следующей итерацией начать с теоертически нового UTF-8 символа. "-1" - нужен для компенсации того, что со следующей итерацией цикла "for" сам еще прибавит "+1". 
		}
		//-----------------------------------------------------------------------------------------------


	}
	
	void convert__UTF8_String_to_Vec_UnicodePointCode__Info(const char* pointer_to_UTF8_string, const size_t string_UTF8_size_byte, std::vector<UTF8_SymvInfo_struct>& vec_info)
	{

		//ВСЕ ПЕРЕДАВАЕМЫЕ ЮНИКОДНЫЕ СИМВОЛЫ ДОЛЖНЫ БЫТЬ ГАРАНТИРОВАНО КОРРЕКТНЫМИ. ФУНКЦИЯ НЕ ЗАНИМАЕТСЯ ПРОВЕРКОЙ СИМВОЛОВ НА КОРРЕКТНОСТЬ СООТВЕТВУЮЩЕЙ КОДИРОВКЕ. 
		//ЕСЛИ ЕСТЬ СОМНЕНИЯ В КОРРЕКТНОСТИ КОДИРОВК - ЭТО ДОЛЖНО БЫТЬ ПРОВЕРЕНО ЗАРАНЕЕ ПРОВЕРЕНО С ПОМОЩЬЮ СОООВЕТСТВУЮЩЕЙ ФУНКЦИИ  "check_xxxxx"


		//pointer_to_UTF8_string - должен быть указатель на первый байт UTF-8 символа.
		//string_UTF8_size_byte  - размер в байтах всего массива на который указывает указатель "pointer_to_UTF8_string"



		//-----------------------------------------------------------------------------------------------
		for (size_t i = 0; i < string_UTF8_size_byte; i++)
		{

			//---------------------------------------------------------------------
			const int ByteSize = get__UTF8symv_ByteSize(&pointer_to_UTF8_string[i]);
			const int_least32_t UnicodePointCode = convert__UTF8Symv_to_UnicodePointCode(&pointer_to_UTF8_string[i], ByteSize);
			const size_t NumPos = i;
			const char* pointer_to_UTF8_Symv = &pointer_to_UTF8_string[i];

			vec_info.emplace_back(UnicodePointCode, NumPos, pointer_to_UTF8_Symv, ByteSize);
			//---------------------------------------------------------------------


			i = i + ByteSize - 1;          //Прибавляем размер байт найденого UTF8-символа, чтобы со следующей итерацией начать с теоертически нового UTF-8 символа. "-1" - нужен для компенсации того, что со следующей итерацией цикла "for" сам еще прибавит "+1". 
		}
		//-----------------------------------------------------------------------------------------------


	}
	
	const result_flag convert__SpecificNumPos_UTF8Symv_in_String_to_UnicodePointCode__Info(const char* pointer_to_UTF8_string, const size_t string_UTF8_size_byte, const size_t UTF8Symv_NumPos, UTF8_SymvInfo_struct& info)
	{

		//ВСЕ ПЕРЕДАВАЕМЫЕ ЮНИКОДНЫЕ СИМВОЛЫ ДОЛЖНЫ БЫТЬ ГАРАНТИРОВАНО КОРРЕКТНЫМИ. ФУНКЦИЯ НЕ ЗАНИМАЕТСЯ ПРОВЕРКОЙ СИМВОЛОВ НА КОРРЕКТНОСТЬ СООТВЕТВУЮЩЕЙ КОДИРОВКЕ. 
		//ЕСЛИ ЕСТЬ СОМНЕНИЯ В КОРРЕКТНОСТИ КОДИРОВК - ЭТО ДОЛЖНО БЫТЬ ПРОВЕРЕНО ЗАРАНЕЕ ПРОВЕРЕНО С ПОМОЩЬЮ СОООВЕТСТВУЮЩЕЙ ФУНКЦИИ  "check_xxxxx"



		//pointer_to_UTF8_string - должен быть указатель на первый байт UTF-8 символа.
		//string_UTF8_size_byte  - размер в байтах всего массива на который указывает указатель "pointer_to_UTF8_string"



		size_t cntr = 0;


		//-----------------------------------------------------------------------------------------------
		for (size_t i = 0; i < string_UTF8_size_byte; i++)
		{

			//---------------------------------------------------------------------
			const int ByteSize = get__UTF8symv_ByteSize(&pointer_to_UTF8_string[i]);
			//---------------------------------------------------------------------


			if (cntr == UTF8Symv_NumPos)
			{

				//------------------------------------------------------
				const int_least32_t UnicodePointCode = convert__UTF8Symv_to_UnicodePointCode(&pointer_to_UTF8_string[i], ByteSize);
				const size_t NumPos = i;
				const char* pointer_to_UTF8_Symv = &pointer_to_UTF8_string[i];

				info = { UnicodePointCode, NumPos, pointer_to_UTF8_Symv, ByteSize };

				return result_flag::OK;
				//------------------------------------------------------

			}


			i = i + ByteSize - 1;          //Прибавляем размер байт найденого UTF8-символа, чтобы со следующей итерацией начать с теоертически нового UTF-8 символа. "-1" - нужен для компенсации того, что со следующей итерацией цикла "for" сам еще прибавит "+1". 
			cntr++;

		}
		//-----------------------------------------------------------------------------------------------

		error = "NumPos_NotExist";
		return result_flag::NumPos_NotExist;  //UTF8Symv_NumPos нет такого порядкого номера UTF-8 символа - по указателю "pointer_to_UTF8_string" в пределах указанного "string_UTF8_size_byte" размера.

	}
	
	void convert__UTF8_String_to_Vec_UnicodePointCode__NotDuplicated(const char* pointer_to_UTF8_string, const size_t string_UTF8_size_byte, std::vector<int_least32_t>& vec_UnicodePointCode)
	{

		//ВСЕ ПЕРЕДАВАЕМЫЕ ЮНИКОДНЫЕ СИМВОЛЫ ДОЛЖНЫ БЫТЬ ГАРАНТИРОВАНО КОРРЕКТНЫМИ. ФУНКЦИЯ НЕ ЗАНИМАЕТСЯ ПРОВЕРКОЙ СИМВОЛОВ НА КОРРЕКТНОСТЬ СООТВЕТВУЮЩЕЙ КОДИРОВКЕ. 
		//ЕСЛИ ЕСТЬ СОМНЕНИЯ В КОРРЕКТНОСТИ КОДИРОВК - ЭТО ДОЛЖНО БЫТЬ ПРОВЕРЕНО ЗАРАНЕЕ ПРОВЕРЕНО С ПОМОЩЬЮ СОООВЕТСТВУЮЩЕЙ ФУНКЦИИ  "check_xxxxx"



		//pointer_to_UTF8_string - должен быть указатель на первый байт UTF-8 символа.
		//string_UTF8_size_byte  - размер в байтах всего массива на который указывает указатель "pointer_to_UTF8_string"


		std::unordered_set<int_least32_t> unordered_set_;


		//-----------------------------------------------------------------------------------------------
		for (size_t i = 0; i < string_UTF8_size_byte; i++)
		{

			//---------------------------------------------------------------------
			const int ByteSize = get__UTF8symv_ByteSize(&pointer_to_UTF8_string[i]);
			const int_least32_t UnicodePointCode = convert__UTF8Symv_to_UnicodePointCode(&pointer_to_UTF8_string[i], ByteSize);


			if (unordered_set_.find(UnicodePointCode) == unordered_set_.end())
			{
				//Значит такого значения еше нет, добавим:

				vec_UnicodePointCode.emplace_back(UnicodePointCode);

				unordered_set_.insert(UnicodePointCode);
			}
			//---------------------------------------------------------------------


			i = i + ByteSize - 1;          //Прибавляем размер байт найденого UTF8-символа, чтобы со следующей итерацией начать с теоертически нового UTF-8 символа. "-1" - нужен для компенсации того, что со следующей итерацией цикла "for" сам еще прибавит "+1". 
		}
		//-----------------------------------------------------------------------------------------------


	}
	
	//*****************************************************************UTF-8:Конец**********************************************************************************
	


	//****************************************************************UTF-16:Начало***********************************************************************************
	template<typename T> inline const int_least32_t convert__UTF16SymvLE_to_UnicodePointCode(const T* pointer_to_beg_UTF16_symv) noexcept
	{

		//ВСЕ ПЕРЕДАВАЕМЫЕ ЮНИКОДНЫЕ СИМВОЛЫ ДОЛЖНЫ БЫТЬ ГАРАНТИРОВАНО КОРРЕКТНЫМИ. ФУНКЦИЯ НЕ ЗАНИМАЕТСЯ ПРОВЕРКОЙ СИМВОЛОВ НА КОРРЕКТНОСТЬ СООТВЕТВУЮЩЕЙ КОДИРОВКЕ. 
		//ЕСЛИ ЕСТЬ СОМНЕНИЯ В КОРРЕКТНОСТИ КОДИРОВК - ЭТО ДОЛЖНО БЫТЬ ПРОВЕРЕНО ЗАРАНЕЕ ПРОВЕРЕНО С ПОМОЩЬЮ СОООВЕТСТВУЮЩЕЙ ФУНКЦИИ  "check_xxxxx"



		//pointer_to_beg_UTF16_symv - должен быть указатель на первую пару UTF-16 символа.


		//-----------------------------------------------Получим размер в парах UTF16 символа:Начало-----------------------------------------

		const int UTF16symv_PairSize = get__UTF16symvLE_PairSize(pointer_to_beg_UTF16_symv);   //Получаем кол-во пар задейсвованных в UTF-16 символе на который указывает "pointer_to_beg_UTF16_symv"

		//-----------------------------------------------Получим размер в парах UTF16 символа:Конец-----------------------------------------





		//----------------------------------------------------------------------------------------------------------------------------------------
		if (UTF16symv_PairSize == 1)
		{
			return *pointer_to_beg_UTF16_symv;      //То есть просто передаем саму первую пару, значение которого потом автоматически приводится к int - это и будет значение Юникодного символа.
		}
		else if (UTF16symv_PairSize == 2)
		{
			//То есть наш UTF-16 символ занимает 2 пары: [110110_xx xxxxxxxx] [110111_xx xxxxxxxx]
			//[110110_xx xxxxxxxx]
			//[110111_xx xxxxxxxx]

			//Вычленим рабочие "x" биты:

			//[110110_xx xxxxxxxx]
			// 000000_11 11111111       Применяю маску "000000_11 11111111" к первой Левой паре через операцию "И" зануляя служебные биты и оставляя только рабочие.
			// 000000_xx xxxxxxxx       [результат применения маски 1]

			//Потом полученный реультат свдигаю на "16 минус 6", то есть на 10 бит влево - создавая пространство только для рабочих бит второй правой пары:
			//000000_xx xxxxxxxx << 10
			//000000_xx xxxxxxxx 00 00000000  [результат свдига на 10 влево]

			//[110111_xx xxxxxxxx]
			// 000000_11 11111111       //Применяю маску "000000_11 11111111" ко второй Правой паре через операцию "И" зануляя служебные биты и оставляя только рабочие.
			// 000000_xx xxxxxxxx       [результат применения маски 2]

			//И теперь  результат свдига через операцию "ИЛИ" обьеденяю с "результатом маски - 2":

			//000000_xx xxxxxxxx 00 00000000             [результат свдига на 10 влево]
			//            000000 xx xxxxxxxx             [результат применения маски 2]
			//Применяем операцию "ИЛИ"
			//000000_xx xxxxxxxx xx xxxxxxxx             [Все, можно привести к примеру к int и получим значение Юникодной кодовой точки]
			//И заключительный шаг к получившемуся занчения прибавить значение "65536".


			return ((((pointer_to_beg_UTF16_symv[0] & 0b0000001111111111)) << 10) | (pointer_to_beg_UTF16_symv[1] & 0b0000001111111111)) + 65536;
		}
		//----------------------------------------------------------------------------------------------------------------------------------------


	}

	template<typename T> inline const int_least32_t convert__UTF16SymvLE_to_UnicodePointCode(const T* pointer_to_beg_UTF16_symv, const int UTF16symv_PairSize)
	{


		//pointer_to_beg_UTF16_symv - должен быть указатель на первую пару UTF-16 символа.
		//UTF16symv_PairSize        - кол-во пар из который состоит UTF-16 символ.



		//----------------------------------------------------------------------------------------------------------------------------------------
		if (UTF16symv_PairSize == 1)
		{
			return *pointer_to_beg_UTF16_symv;      //То есть просто передаем саму первую пару, значение которого потом автоматически приводится к int - это и будет значение Юникодного символа.
		}
		else if (UTF16symv_PairSize == 2)
		{
			//То есть наш UTF-16 символ занимает 2 пары: [110110_xx xxxxxxxx] [110111_xx xxxxxxxx]
			//[110110_xx xxxxxxxx]
			//[110111_xx xxxxxxxx]

			//Вычленим рабочие "x" биты:

			//[110110_xx xxxxxxxx]
			// 000000_11 11111111       Применяю маску "000000_11 11111111" к первой Левой паре через операцию "И" зануляя служебные биты и оставляя только рабочие.
			// 000000_xx xxxxxxxx       [результат применения маски 1]

			//Потом полученный реультат свдигаю на "16 минус 6", то есть на 10 бит влево - создавая пространство только для рабочих бит второй правой пары:
			//000000_xx xxxxxxxx << 10
			//000000_xx xxxxxxxx 00 00000000  [результат свдига на 10 влево]

			//[110111_xx xxxxxxxx]
			// 000000_11 11111111       //Применяю маску "000000_11 11111111" ко второй Правой паре через операцию "И" зануляя служебные биты и оставляя только рабочие.
			// 000000_xx xxxxxxxx       [результат применения маски 2]

			//И теперь  результат свдига через операцию "ИЛИ" обьеденяю с "результатом маски - 2":

			//000000_xx xxxxxxxx 00 00000000             [результат свдига на 10 влево]
			//            000000 xx xxxxxxxx             [результат применения маски 2]
			//Применяем операцию "ИЛИ"
			//000000_xx xxxxxxxx xx xxxxxxxx             [Все, можно привести к примеру к int и получим значение Юникодной кодовой точки]
			//И заключительный шаг к получившемуся занчения прибавить значение "65536".


			return ((((pointer_to_beg_UTF16_symv[0] & 0b0000001111111111)) << 10) | (pointer_to_beg_UTF16_symv[1] & 0b0000001111111111)) + 65536;
		}
		//----------------------------------------------------------------------------------------------------------------------------------------


	}


	template<typename T> void convert__UTF16LE_String_to_Vec_UnicodePointCode(const T* pointer_to_UTF16_string, const size_t string_UTF16_Pair_size, std::vector<int_least32_t>& vec_UnicodePointCode)
	{

		//ВСЕ ПЕРЕДАВАЕМЫЕ ЮНИКОДНЫЕ СИМВОЛЫ ДОЛЖНЫ БЫТЬ ГАРАНТИРОВАНО КОРРЕКТНЫМИ. ФУНКЦИЯ НЕ ЗАНИМАЕТСЯ ПРОВЕРКОЙ СИМВОЛОВ НА КОРРЕКТНОСТЬ СООТВЕТВУЮЩЕЙ КОДИРОВКЕ. 
		//ЕСЛИ ЕСТЬ СОМНЕНИЯ В КОРРЕКТНОСТИ КОДИРОВК - ЭТО ДОЛЖНО БЫТЬ ПРОВЕРЕНО ЗАРАНЕЕ ПРОВЕРЕНО С ПОМОЩЬЮ СОООВЕТСТВУЮЩЕЙ ФУНКЦИИ  "check_xxxxx"



		//pointer_to_UTF16_string - должен быть указатель на первый байт UTF-16 символа.
		//string_UTF16_Pair_size  - размер в парах всего массива на который указывает указатель "pointer_to_UTF16_string"


		//-----------------------------------------------------------------------------------------------
		for (size_t i = 0; i < string_UTF16_Pair_size; i++)
		{

			//---------------------------------------------------------------------
			const int PairSize = get__UTF16symvLE_PairSize(&pointer_to_UTF16_string[i]);
			const int_least32_t UnicodePointCode = convert__UTF16SymvLE_to_UnicodePointCode<T>(&pointer_to_UTF16_string[i], PairSize);

			vec_UnicodePointCode.emplace_back(UnicodePointCode);
			//---------------------------------------------------------------------


			i = i + PairSize - 1;          //Прибавляем размер пары найденого UTF16-символа, чтобы со следующей итерацией начать с теоертически нового UTF-16 символа. "-1" - нужен для компенсации того, что со следующей итерацией цикла "for" сам еще прибавит "+1". 
		}
		//-----------------------------------------------------------------------------------------------

	}

	template<typename T> void convert__UTF16LE_String_to_Vec_UnicodePointCode__Info(const T* pointer_to_UTF16_string, const size_t string_UTF16_Pair_size, std::vector<UTF16_SymvInfo_struct>& vec_info)
	{

		//ВСЕ ПЕРЕДАВАЕМЫЕ ЮНИКОДНЫЕ СИМВОЛЫ ДОЛЖНЫ БЫТЬ ГАРАНТИРОВАНО КОРРЕКТНЫМИ. ФУНКЦИЯ НЕ ЗАНИМАЕТСЯ ПРОВЕРКОЙ СИМВОЛОВ НА КОРРЕКТНОСТЬ СООТВЕТВУЮЩЕЙ КОДИРОВКЕ. 
		//ЕСЛИ ЕСТЬ СОМНЕНИЯ В КОРРЕКТНОСТИ КОДИРОВК - ЭТО ДОЛЖНО БЫТЬ ПРОВЕРЕНО ЗАРАНЕЕ ПРОВЕРЕНО С ПОМОЩЬЮ СОООВЕТСТВУЮЩЕЙ ФУНКЦИИ  "check_xxxxx"




		//pointer_to_UTF16_string - должен быть указатель на первый байт UTF-16 символа.
		//string_UTF16_Pair_size  - размер в парах всего массива на который указывает указатель "pointer_to_UTF16_string"



		//-----------------------------------------------------------------------------------------------
		for (size_t i = 0; i < string_UTF16_Pair_size; i++)
		{

			//---------------------------------------------------------------------
			const int PairSize = get__UTF16symvLE_PairSize(&pointer_to_UTF16_string[i]);
			const int_least32_t UnicodePointCode = convert__UTF16SymvLE_to_UnicodePointCode<T>(&pointer_to_UTF16_string[i], PairSize);
			const size_t NumPos = i;
			const wchar_t* pointer_to_UTF16_Symv = &pointer_to_UTF16_string[i];

			vec_info.emplace_back(UnicodePointCode, NumPos, pointer_to_UTF16_string, PairSize);
			//---------------------------------------------------------------------


			i = i + PairSize - 1;          //Прибавляем размер байт найденого UTF16-символа, чтобы со следующей итерацией начать с теоертически нового UTF-16 символа. "-1" - нужен для компенсации того, что со следующей итерацией цикла "for" сам еще прибавит "+1". 
		}
		//-----------------------------------------------------------------------------------------------


	}

	template<typename T> const result_flag convert__SpecificNumPos_UTF16SymvLE_in_String_to_UnicodePointCode__Info(const T* pointer_to_UTF16_string, const size_t string_UTF16_Pair_size, const size_t UTF16Symv_NumPos, UTF16_SymvInfo_struct& info)
	{

		//ВСЕ ПЕРЕДАВАЕМЫЕ ЮНИКОДНЫЕ СИМВОЛЫ ДОЛЖНЫ БЫТЬ ГАРАНТИРОВАНО КОРРЕКТНЫМИ. ФУНКЦИЯ НЕ ЗАНИМАЕТСЯ ПРОВЕРКОЙ СИМВОЛОВ НА КОРРЕКТНОСТЬ СООТВЕТВУЮЩЕЙ КОДИРОВКЕ. 
		//ЕСЛИ ЕСТЬ СОМНЕНИЯ В КОРРЕКТНОСТИ КОДИРОВК - ЭТО ДОЛЖНО БЫТЬ ПРОВЕРЕНО ЗАРАНЕЕ ПРОВЕРЕНО С ПОМОЩЬЮ СОООВЕТСТВУЮЩЕЙ ФУНКЦИИ  "check_xxxxx"




		//pointer_to_UTF16_string - должен быть указатель на первый байт UTF-16 символа.
		//string_UTF16_Pair_size  - размер в парах всего массива на который указывает указатель "pointer_to_UTF16_string"



		size_t cntr = 0;


		//-----------------------------------------------------------------------------------------------
		for (size_t i = 0; i < string_UTF16_Pair_size; i++)
		{

			//---------------------------------------------------------------------
			const int PairSize = get__UTF16symvLE_PairSize(&pointer_to_UTF16_string[i]);
			//---------------------------------------------------------------------


			if (cntr == UTF16Symv_NumPos)
			{

				//------------------------------------------------------
				const int_least32_t UnicodePointCode = convert__UTF16SymvLE_to_UnicodePointCode<T>(&pointer_to_UTF16_string[i], PairSize);
				const size_t NumPos = i;
				const wchar_t* pointer_to_UTF16_Symv = &pointer_to_UTF16_string[i];

				info = { UnicodePointCode, NumPos, pointer_to_UTF16_Symv, PairSize };

				return result_flag::OK;
				//------------------------------------------------------

			}


			i = i + PairSize - 1;          //Прибавляем размер байт найденого UTF8-символа, чтобы со следующей итерацией начать с теоертически нового UTF-8 символа. "-1" - нужен для компенсации того, что со следующей итерацией цикла "for" сам еще прибавит "+1". 
			cntr++;

		}
		//-----------------------------------------------------------------------------------------------


		error = "NumPos_NotExist";
		return result_flag::NumPos_NotExist;         //UTF8Symv_NumPos нет такого порядкого номера UTF-8 символа - по указателю "pointer_to_UTF8_string" в пределах указанного "string_UTF8_size_byte" размера.

	}

	template<typename T> void convert__UTF16LE_String_to_Vec_UnicodePointCode__NotDuplicated(const T* pointer_to_UTF16_string, const size_t string_UTF16_Pair_size, std::vector<int_least32_t>& vec_UnicodePointCode)
	{

		//ВСЕ ПЕРЕДАВАЕМЫЕ ЮНИКОДНЫЕ СИМВОЛЫ ДОЛЖНЫ БЫТЬ ГАРАНТИРОВАНО КОРРЕКТНЫМИ. ФУНКЦИЯ НЕ ЗАНИМАЕТСЯ ПРОВЕРКОЙ СИМВОЛОВ НА КОРРЕКТНОСТЬ СООТВЕТВУЮЩЕЙ КОДИРОВКЕ. 
		//ЕСЛИ ЕСТЬ СОМНЕНИЯ В КОРРЕКТНОСТИ КОДИРОВК - ЭТО ДОЛЖНО БЫТЬ ПРОВЕРЕНО ЗАРАНЕЕ ПРОВЕРЕНО С ПОМОЩЬЮ СОООВЕТСТВУЮЩЕЙ ФУНКЦИИ  "check_xxxxx"




		//pointer_to_UTF16_string - должен быть указатель на первый байт UTF-16 символа.
		//string_UTF16_Pair_size  - размер в парах всего массива на который указывает указатель "pointer_to_UTF16_string"


		std::unordered_set<int> unordered_set_;


		//-----------------------------------------------------------------------------------------------
		for (size_t i = 0; i < string_UTF16_Pair_size; i++)
		{

			//---------------------------------------------------------------------
			const int PairSize = get__UTF16symvLE_PairSize(&pointer_to_UTF16_string[i]);
			const int_least32_t UnicodePointCode = convert__UTF16SymvLE_to_UnicodePointCode<T>(&pointer_to_UTF16_string[i], PairSize);


			if (unordered_set_.find(UnicodePointCode) == unordered_set_.end())
			{
				//Значит такого значения еше нет, добавим:

				vec_UnicodePointCode.emplace_back(UnicodePointCode);

				unordered_set_.insert(UnicodePointCode);
			}
			//---------------------------------------------------------------------


			i = i + PairSize - 1;          //Прибавляем размер байт найденого UTF16-символа, чтобы со следующей итерацией начать с теоертически нового UTF-16 символа. "-1" - нужен для компенсации того, что со следующей итерацией цикла "for" сам еще прибавит "+1". 
		}
		//-----------------------------------------------------------------------------------------------


	}
	//****************************************************************UTF-16:Конец***********************************************************************************



	//****************************************************************UTF-8:Начало***********************************************************************************
	inline void convert__UnicodePointCode_to_UTF8Symv(const int_least32_t UnicodePointCode, std::string& result)
	{

		//ВСЕ ПЕРЕДАВАЕМЫЕ ЮНИКОДНЫЕ ЗНАЧЕНИЯ ДОЛЖНЫ БЫТЬ ГАРАНТИРОВАНО КОРРЕКТНЫМИ. ФУНКЦИЯ НЕ ЗАНИМАЕТСЯ ПРОВЕРКОЙ СИМВОЛОВ НА КОРРЕКТНОСТЬ СООТВЕТВУЮЩЕЙ КОДИРОВКЕ. 
		//ЕСЛИ ЕСТЬ СОМНЕНИЯ В КОРРЕКТНОСТИ ЮНИКОДНЫХ ЗНАЧЕНИЙ - ЭТО ДОЛЖНО БЫТЬ ПРОВЕРЕНО ЗАРАНЕЕ.С ПОМОЩЬЮ СОООВЕТСТВУЮЩЕЙ ФУНКЦИИ  "check__UnicodePointCode"



		//Про кодировку UTF-8 читать в функции "check_UTF8_symv_and_return_ByteSize".

		//---------------------------------------------------------------------------------------------------------------------------------------

		//СУТЬ вообщем такая, вкратце: в зависимости от ткого сколькими байтами закодирован UTF-8 символ - мы имеем следующую ситуацию:

		//Символ закодированный 1 байтом - всегда начинается с 0 бита слева.И все остальные 7 бит - это самое значение символа :
		//0_xxxxxxx 

		//Символ закодированный от 2 до 4 байт ВСЕГДА начинается в Первом байте с:
		//110_xxxxx     - закодирован двумя байтами
		//1110_xxxx     - закодирован тремя байтами
		//11110_xxx     - закодирован четрьмя байтами
		//И в последующих байтах ЭТОГО ЖЕ символа, байты начинаются всегда с 10_xxxxxx и потом следуют 6 бит самого значенния.

		//Значит для Однобайтового    символа UTF-8 кол-во полезных бит, которые составляют самого значение Юникод: это 7 бит             - это максимальное значение: 127.
		//Значит для Двухбайтового    символа UTF-8 кол-во полезных бит, которые составляют самого значение Юникод: это 5 + 6 бит         - это максимальное значение: 2047.
		//Значит для Трехбайтового    символа UTF-8 кол-во полезных бит, которые составляют самого значение Юникод: это 4 + 6 + 6 бит     - это максимальное значение: 65535.
		//Значит для Четырехбайтового символа UTF-8 кол-во полезных бит, которые составляют самого значение Юникод: это 3 + 6 + 6 + 6 бит - это максимальное значение: 2097151 (а вот иииии нет!!). Максимально значение Юникода ограничено не самой кодировкой, а самой спецификацией и оно сосставляет значение "1114111", что в двоичном значении будет "100010000000000000000", НО в любом случае - это Максимально 21 бит.

		//ТЕПЕРЬ для примера возьмем юникодный китайский иероглиф "东": ее значение равно 19996: https://symbl.cc/ru/4E1C/     
		//В кодироке UTF-8 он выглядет так: 11100100 10111000 10011100
		//И теперь посмотрим, как конвертирувать Юникодное значение "19996" ее в UTF-8 кодировку "11100100 10111000 10011100".

		//ЗНАЧИТ Юникодное значение находится в промежутке больше "2047" и меньше "65535", то есть на кодирвку UTF-8 для "19996" потрубуется три байта формата:
		//1110_xxxx     -  Первый байт
		//10_xxxxxx     -  Второй байт
		//10_xxxxxx     -  Третий байт

		//Само значение "19996" находится условно в "третьей группе" - то есть для этой группы максимальное значение Бит, которое может быть это "4 + 6 + 6", то есть 16 Бит, то есть я имею ввиду, что для значения "65535" - это было бы значение в двоичном формате "1111111111111111", для "19996" - это "100111000011100" или если слева "добрать" один ноль для 21 бита, то "0100111000011100".
		//Теперь из этих бит "0100111000011100" нужно сформировать UTF-8 кодировку:
		//1110_xxxx == 1110_0100  -  "[0100]111000011100"
		//10_xxxxxx == 10_111000  -  "0100[111000]011100"
		//10_xxxxxx == 10_011100  -  "0100111000[011100]"


		//1- Для формирование первого байта нужно взять из "[0100]111000011100" только первые 4 бита, и для этого сдвинем все биты Влево на значение равное  16-4 = 12 бит, то есть максимальное значение Бит для "третьей группы" минус то кол-во рабочих Бит, которое требуется для формирование Первого байта, ТО ЕСТЬ сдвигае все биты Справа "за пределы" кроме 4 нужных битов с крайнего левого ряда.
		//1- "[0100]111000011100" >> 12  == 0000000000000100 - то есть получаем чистые 4 бита слева "0100" для которых осталось доабвить только 4 служебных бита "1110_". Сделаем это через маску "11100000" и операцию "ИЛИ" и получим готовы Первый байт: 
		//1- 0000000000000100    [Сдвинутые биты]
		//1-         11100000    [Маска для установления двух служебных битов 1110_xxxxxx" 1110_0000"] и операция "ИЛИ"
		//1-         11100100    [Готовый первый байт UTF-8]


		//2- Для формирование второго байта нужно взять из "0100[111000]011100" уже 6 бит, после первых взятых 4ех и сдигаем их ровно на то значение, которое есть перед этими 6-битами, котоыре нужно свдинуть так, чтобы все биты, которые срава сместились "за предел", то есть в даннос случае это также 6 бит.
		//2- "0100[111000]011100" >> 6  == 000000[0100][111000]  - то есть получаем 6 бит слева, которые нужно поместить во второй байт UTF-8 символа, НО, как видно нам могу помешать 4 бита Слева от этих 6 бит, и их нужно, как то выпилить, ПОЭТОМУ просто применим операцию "И" с маской "00111111".
		//2- 00000001 00111000    [Сдвинутые биты]
		//2- 00000000 00111111    [Маска "00111111"] и операция "И"
		//2- 00000000 00111000    [Резльтат маскирования]
		//2- Так мы установили рабочие 6 Бит, но теперь осталось доабвитть 2 служебных бита 10_xxxxxx, делаем так же, как и в первом Байте, применяем маску со служебными "10_111111" битами и операцию "ИЛИ":
		//2- 00111000    [Готовые рабочие биты]
		//2- 10111111    [Маска для установления двух служебных битов 10_xxxxxx] и операция "ИЛИ"
		//2- 10111000    [Второй байт сформирован]


		//3- Для формирование третьего байта нужно взять из "0100111000[011100]" уже последние 6 бит, их сдвигать уже никуда не надо, они и так уже находятся у "Правого края". Только нужно занулить все остальые биты слева, чтобы потом к первым двум битам перед этии 6 битами - применить маску служебных битов "10_xxxxxx"
		//3- "0100111000[011100]" применяем маску "00111111" и операцию "И" и зануляем все биты Слева от нашиз 6 бит.
		//3-  00000001 00111111   [Маска "00111111"] и операция "И"
		//3-  00000000 00011100   [Резльтат маскирования]
		//3-           10111111   [Маска для установления двух служебных битов 10_xxxxxx] и операция "ИЛИ"
		//3-           10011100   [Третий байт сформирован]

		//Собсвенно все, сформированы три байта, теперь их можно просто добавить в строку и все. Остальные Юникодные значения из тругих дмапазонов значений - фрмируются по такому же принципе.

		//---------------------------------------------------------------------------------------------------------------------------------------


		if (UnicodePointCode <= 127)
		{
			result.push_back((UnicodePointCode));

			return;
		}
		else if (UnicodePointCode <= 2047)
		{
			result.push_back((0b11000000 | (UnicodePointCode >> 6)));
			result.push_back((0b10000000 | (UnicodePointCode & 0b00111111)));

			return;
		}
		else if (UnicodePointCode <= 65535)
		{
			result.push_back((0b11100000 | (UnicodePointCode >> 12)));
			result.push_back((0b10000000 | ((UnicodePointCode >> 6) & 0b00111111)));
			result.push_back((0b10000000 | (UnicodePointCode & 0b00111111)));

			return;
		}
		else if (UnicodePointCode <= 1114111)
		{
			result.push_back((0b11110000 | (UnicodePointCode >> 18)));
			result.push_back((0b10000000 | ((UnicodePointCode >> 12) & 0b00111111)));
			result.push_back((0b10000000 | ((UnicodePointCode >> 6) & 0b00111111)));
			result.push_back((0b10000000 | (UnicodePointCode & 0b00111111)));

			return;
		}

	}
	
	void convert__Vec_UnicodePointCode_to_Vec_UTF8(const std::vector<int_least32_t>& vec_UnicodePointCode, std::vector<std::string>& vec_result)
	{

		//ВСЕ ПЕРЕДАВАЕМЫЕ ЮНИКОДНЫЕ ЗНАЧЕНИЯ ДОЛЖНЫ БЫТЬ ГАРАНТИРОВАНО КОРРЕКТНЫМИ. ФУНКЦИЯ НЕ ЗАНИМАЕТСЯ ПРОВЕРКОЙ СИМВОЛОВ НА КОРРЕКТНОСТЬ СООТВЕТВУЮЩЕЙ КОДИРОВКЕ. 
		//ЕСЛИ ЕСТЬ СОМНЕНИЯ В КОРРЕКТНОСТИ ЮНИКОДНЫХ ЗНАЧЕНИЙ - ЭТО ДОЛЖНО БЫТЬ ПРОВЕРЕНО ЗАРАНЕЕ.С ПОМОЩЬЮ СОООВЕТСТВУЮЩЕЙ ФУНКЦИИ  "check__UnicodePointCode"


		//Описание читать в "convert__UnicodePointCode_to_UTF8Symv".


		const size_t size_before = vec_result.size();

		//-------------------------------------------------------
		vec_result.resize(size_before + vec_UnicodePointCode.size());
		//-------------------------------------------------------

		//-----------------------------------------------------------------
		for (size_t i = 0; i < vec_UnicodePointCode.size(); i++)
		{
			convert__UnicodePointCode_to_UTF8Symv(vec_UnicodePointCode[i], vec_result[size_before + i]);
		}
		//-----------------------------------------------------------------


	}
	
	void convert__Vec_UnicodePointCode_to_String_UTF8(const std::vector<int_least32_t>& vec_UnicodePointCode, std::string& string_result)
	{

		//ВСЕ ПЕРЕДАВАЕМЫЕ ЮНИКОДНЫЕ ЗНАЧЕНИЯ ДОЛЖНЫ БЫТЬ ГАРАНТИРОВАНО КОРРЕКТНЫМИ. ФУНКЦИЯ НЕ ЗАНИМАЕТСЯ ПРОВЕРКОЙ СИМВОЛОВ НА КОРРЕКТНОСТЬ СООТВЕТВУЮЩЕЙ КОДИРОВКЕ. 
		//ЕСЛИ ЕСТЬ СОМНЕНИЯ В КОРРЕКТНОСТИ ЮНИКОДНЫХ ЗНАЧЕНИЙ - ЭТО ДОЛЖНО БЫТЬ ПРОВЕРЕНО ЗАРАНЕЕ.С ПОМОЩЬЮ СОООВЕТСТВУЮЩЕЙ ФУНКЦИИ  "check__UnicodePointCode"


		//Описание читать в "convert__UnicodePointCode_to_UTF8Symv".




		//-------------------------------------------------------
		string_result.reserve(string_result.size() + vec_UnicodePointCode.size());
		//-------------------------------------------------------

		//-----------------------------------------------------------------
		for (size_t i = 0; i < vec_UnicodePointCode.size(); i++)
		{
			convert__UnicodePointCode_to_UTF8Symv(vec_UnicodePointCode[i], string_result);
		}
		//-----------------------------------------------------------------


	}
	//*****************************************************************UTF-8:Конец**********************************************************************************




	//****************************************************************UTF-16:Начало***********************************************************************************
	
	template<typename T> inline void convert__UnicodePointCode_to_UTF16SymvLE(const int_least32_t UnicodePointCode, T& result)
	{

		//ВСЕ ПЕРЕДАВАЕМЫЕ ЮНИКОДНЫЕ СИМВОЛЫ ДОЛЖНЫ БЫТЬ ГАРАНТИРОВАНО КОРРЕКТНЫМИ. ФУНКЦИЯ НЕ ЗАНИМАЕТСЯ ПРОВЕРКОЙ СИМВОЛОВ НА КОРРЕКТНОСТЬ СООТВЕТВУЮЩЕЙ КОДИРОВКЕ. 
		//ЕСЛИ ЕСТЬ СОМНЕНИЯ В КОРРЕКТНОСТИ КОДИРОВК - ЭТО ДОЛЖНО БЫТЬ ПРОВЕРЕНО ЗАРАНЕЕ ПРОВЕРЕНО С ПОМОЩЬЮ СОООВЕТСТВУЮЩЕЙ ФУНКЦИИ  "check_xxxxx"



		//[110110_xx xxxxxxxx]
		//[110111_xx xxxxxxxx]



		if (UnicodePointCode <= 65535)
		{
			result.push_back((UnicodePointCode));

			return;
		}
		else
		{

			//Вычитаем из "UnicodePointCode" значение "65536". Теперь в "UnicodePointCode" максимальное значение Юникод которое может быть составляет максимум 20 бит. 
			//Теперь эти 20 бит по ровну нужно раскидать по двух парам UTF-16 символа: [110110_xx xxxxxxxx] [110111_xx xxxxxxxx]

			//Сдвигаем крайние 10 бит Справа в "UnicodePointCode" на 10 бит Вправо, чтобы вывести их за пределы, и чтобы остались только 10 бит, которые были Слева и потом применяем маску для установки служеюных бит "110110"
			//000000xx xxxxxxxx           [Результат сдвига на 16 бит Вправо, то есть остаются только 10 бит, а остальные биты в перменной что были левее этих 10 бит нулевые, так как максимальные значение занимает только 20 бит из условных 32 бит, если бы это был 32-битный int]
			//11011000 00000000           [маска которую применяем через операцию "ИЛИ"]
			//110110xx xxxxxxxx           [Сформировали первую Правую Пару UTF-16 символа]


			//Применяем маску "000000_11 11111111" к крайним Справа 10 битам "UnicodePointCode" через операцию "И" оставляя только Првые 10 бит и устанльные зануляя и потом к этому результату применяем маску для установки вторых служебных бит ""110111"
			//00000000 0000 xxxx xxxxxxxx xxxxxxxx        [это крайние Левые и Крайние правые 16 бит в "UnicodePointCode"]
			//00000000 0000 0000 00000011 11111111        [Маска для занеудения всех бит Левее 10 Крайних Правых бит через операцию "И"]
			
			//00000000 0000 0000 11011100 00000000        [Маска для Установки служебныx бит "110111" через операцию "ИЛИ"]
			//00000000 0000 0000 110111xx xxxxxxxx        [сформировали Правую вторую пару UTF-16 символа]



			const int UnicodePointCode_ = UnicodePointCode - 65536;

			result.push_back((UnicodePointCode_ >> 10) | 0b1101100000000000);
			result.push_back((UnicodePointCode_ & 0b1111111111) | 0b1101110000000000);

			return;
		}


	}
	
	template<typename T> void convert__Vec_UnicodePointCode_to_Vec_UTF16LE(const std::vector<int_least32_t>& vec_UnicodePointCode, std::vector<T>& vec_result)
	{

		//Описание читать в "convert__UnicodePointCode_to_UTF16SymvLE".


		const size_t size_before = vec_result.size();


		//-------------------------------------------------------
		vec_result.resize(size_before + vec_UnicodePointCode.size());
		//-------------------------------------------------------

		//-----------------------------------------------------------
		for (size_t i = 0; i < vec_UnicodePointCode.size(); i++)
		{
			convert__UnicodePointCode_to_UTF16SymvLE(vec_UnicodePointCode[i], vec_result[size_before + i]);
		}
		//-----------------------------------------------------------


	}
	
	template<typename T> void convert__Vec_UnicodePointCode_to_String_UTF16LE(const std::vector<int_least32_t>& vec_UnicodePointCode, T& wstring_result)
	{

		//Описание читать в "convert__UnicodePointCode_to_UTF16SymvLE".




		//-------------------------------------------------------
		wstring_result.reserve(wstring_result.size() + vec_UnicodePointCode.size());
		//-------------------------------------------------------

		//-----------------------------------------------------------
		for (size_t i = 0; i < vec_UnicodePointCode.size(); i++)
		{
			convert__UnicodePointCode_to_UTF16SymvLE(vec_UnicodePointCode[i], wstring_result);
		}
		//-----------------------------------------------------------


	}

	//****************************************************************UTF-16:Конец***********************************************************************************




	//***************************************************************************************************************************************************
	template<typename T> inline void Convert_from_UTF8_to_UTF16LE_OneSymv(const char* pointer_to_beg_UTF8_symv, T& result_wstring_UTF16)
	{

		//ВСЕ ПЕРЕДАВАЕМЫЕ ЮНИКОДНЫЕ СИМВОЛЫ ДОЛЖНЫ БЫТЬ ГАРАНТИРОВАНО КОРРЕКТНЫМИ. ФУНКЦИЯ НЕ ЗАНИМАЕТСЯ ПРОВЕРКОЙ СИМВОЛОВ НА КОРРЕКТНОСТЬ СООТВЕТВУЮЩЕЙ КОДИРОВКЕ. 
	    //ЕСЛИ ЕСТЬ СОМНЕНИЯ В КОРРЕКТНОСТИ КОДИРОВК - ЭТО ДОЛЖНО БЫТЬ ПРОВЕРЕНО ЗАРАНЕЕ ПРОВЕРЕНО С ПОМОЩЬЮ СОООВЕТСТВУЮЩЕЙ ФУНКЦИИ  "check_xxxxx"

		//pointer_to_UTF8_string - должен быть указатель на первый байт UTF-8 символа.


		//Делаем легко, просто преобразовываем UTF-8 символ на который указывает "pointer_to_beg_UTF8_symv" - преобразовываем его в значение Юникод, и потом это значение Юникод конверти в UTF-16 Символ.


		const int_least32_t UnicodeCodePont = convert__UTF8Symv_to_UnicodePointCode(pointer_to_beg_UTF8_symv);

		convert__UnicodePointCode_to_UTF16SymvLE(UnicodeCodePont, result_wstring_UTF16);

	}

	template<typename T> inline void Convert_from_UTF16LE_to_UTF8_OneSymv(const T* pointer_to_beg_UTF16_symv, std::string& result_string_UTF8)
	{

		//ВСЕ ПЕРЕДАВАЕМЫЕ ЮНИКОДНЫЕ СИМВОЛЫ ДОЛЖНЫ БЫТЬ ГАРАНТИРОВАНО КОРРЕКТНЫМИ. ФУНКЦИЯ НЕ ЗАНИМАЕТСЯ ПРОВЕРКОЙ СИМВОЛОВ НА КОРРЕКТНОСТЬ СООТВЕТВУЮЩЕЙ КОДИРОВКЕ. 
		//ЕСЛИ ЕСТЬ СОМНЕНИЯ В КОРРЕКТНОСТИ КОДИРОВК - ЭТО ДОЛЖНО БЫТЬ ПРОВЕРЕНО ЗАРАНЕЕ ПРОВЕРЕНО С ПОМОЩЬЮ СОООВЕТСТВУЮЩЕЙ ФУНКЦИИ  "check_xxxxx"

		//pointer_to_UTF8_string - должен быть указатель на первый байт UTF-8 символа.

		//Делаем легко, просто преобразовываем UTF-16 символ на который указывает "pointer_to_beg_UTF16_symv" - преобразовываем его в значение Юникод, и потом это значение Юникод конверти в UTF-8 Символ.


		const int_least32_t UnicodeCodePont = convert__UTF16SymvLE_to_UnicodePointCode(pointer_to_beg_UTF16_symv);

		convert__UnicodePointCode_to_UTF8Symv(UnicodeCodePont, result_string_UTF8);
	}


	template<typename T> void Convert_from_UTF8_to_UTF16LE(const char* pointer_to_beg_UTF8_symv, const size_t pointer_to_beg_UTF8_Size, T& result_wstring_UTF16)
	{

		//ВСЕ ПЕРЕДАВАЕМЫЕ ЮНИКОДНЫЕ СИМВОЛЫ ДОЛЖНЫ БЫТЬ ГАРАНТИРОВАНО КОРРЕКТНЫМИ. ФУНКЦИЯ НЕ ЗАНИМАЕТСЯ ПРОВЕРКОЙ СИМВОЛОВ НА КОРРЕКТНОСТЬ СООТВЕТВУЮЩЕЙ КОДИРОВКЕ. 
		//ЕСЛИ ЕСТЬ СОМНЕНИЯ В КОРРЕКТНОСТИ КОДИРОВК - ЭТО ДОЛЖНО БЫТЬ ПРОВЕРЕНО ЗАРАНЕЕ ПРОВЕРЕНО С ПОМОЩЬЮ СОООВЕТСТВУЮЩЕЙ ФУНКЦИИ  "check_xxxxx"

		//pointer_to_UTF8_string - должен быть указатель на первый байт UTF-8 символа.


		std::vector<int_least32_t>vec_UnicodeCodePoint;

		convert__UTF8_String_to_Vec_UnicodePointCode(pointer_to_beg_UTF8_symv, pointer_to_beg_UTF8_Size, vec_UnicodeCodePoint);

		convert__Vec_UnicodePointCode_to_String_UTF16LE(vec_UnicodeCodePoint, result_wstring_UTF16);
	
	}

	template<typename T> void Convert_from_UTF16LE_to_UTF8(const T* pointer_to_beg_UTF16_symv, const size_t pointer_to_beg_UTF16_Size, std::string& result_string_UTF8)
	{

		//ВСЕ ПЕРЕДАВАЕМЫЕ ЮНИКОДНЫЕ СИМВОЛЫ ДОЛЖНЫ БЫТЬ ГАРАНТИРОВАНО КОРРЕКТНЫМИ. ФУНКЦИЯ НЕ ЗАНИМАЕТСЯ ПРОВЕРКОЙ СИМВОЛОВ НА КОРРЕКТНОСТЬ СООТВЕТВУЮЩЕЙ КОДИРОВКЕ. 
		//ЕСЛИ ЕСТЬ СОМНЕНИЯ В КОРРЕКТНОСТИ КОДИРОВК - ЭТО ДОЛЖНО БЫТЬ ПРОВЕРЕНО ЗАРАНЕЕ ПРОВЕРЕНО С ПОМОЩЬЮ СОООВЕТСТВУЮЩЕЙ ФУНКЦИИ  "check_xxxxx"

		//pointer_to_UTF8_string - должен быть указатель на первую пару UTF-16 символа.


		std::vector<int_least32_t>vec_UnicodeCodePoint;

		convert__UTF16LE_String_to_Vec_UnicodePointCode<T>(pointer_to_beg_UTF16_symv, pointer_to_beg_UTF16_Size, vec_UnicodeCodePoint);

		convert__Vec_UnicodePointCode_to_String_UTF8(vec_UnicodeCodePoint, result_string_UTF8);
	}
	//***************************************************************************************************************************************************




	const std::string& get__LastResultErrorName()
	{
		return error;
	}


private:


	std::string error;


};
